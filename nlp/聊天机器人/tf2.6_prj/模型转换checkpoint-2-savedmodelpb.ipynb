{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import io\n",
    "from configparser import SafeConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_file = os.getcwd() + \"/seq2seq.ini\"\n",
    "if not os.path.exists(conf_file):\n",
    "    conf_file = os.path.dirname(os.getcwd() + \"/seq2seq.ini\")\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    print(conf_file)\n",
    "    parser = SafeConfigParser()\n",
    "    parser.read(conf_file, encoding='utf-8')\n",
    "    _conf_ints = [(key, int(value)) for key, value in parser.items('ints')]\n",
    "    _conf_floats = [(key, float(value)) for key, value in parser.items('floats')]\n",
    "    _conf_strings = [(key, str(value)) for key, value in parser.items('strings')]\n",
    "    return dict(_conf_ints + _conf_floats + _conf_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gaozhiyong/Documents/GitHub/ML/nlp/聊天机器人/tf2.6_prj/seq2seq.ini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaozhiyong/Documents/pyenv/py3.6_tf2.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: The SafeConfigParser class has been renamed to ConfigParser in Python 3.2. This alias will be removed in future versions. Use ConfigParser directly instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 初始化超参字典\n",
    "gConf = {}\n",
    "gConf = get_config()\n",
    "\n",
    "# 通过超参字典为vocab_in_size,vocab_tar_size,embedding_dim,units等赋值\n",
    "vocab_inp_size = gConf[\"vocab_inp_size\"]\n",
    "vocab_tar_size = gConf[\"vocab_tar_size\"]\n",
    "embedding_dim = gConf[\"embedding_dim\"]\n",
    "units = gConf[\"layer_size\"]\n",
    "BATCH_SIZE = gConf[\"batch_size\"]\n",
    "\n",
    "max_length_inp = gConf[\"max_length\"]\n",
    "max_length_tar = gConf[\"max_length\"]\n",
    "log_dir = gConf[\"log_dir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义encoder类\n",
    "class Encoder(tf.keras.Model):\n",
    "    # 初始化参数，对默认参数进行初始化\n",
    "\n",
    "    def __init__(self, vocab_size, emdedding_dim, encode_units, batch_size):\n",
    "        \"\"\"\n",
    "        @param vocab_size:\n",
    "        @param emdedding_dim:\n",
    "        @param encode_units:\n",
    "        @param batch_size:\n",
    "\n",
    "        :param vocab_size: 非重复的词汇总数\n",
    "        :param embedding_dim: 词嵌入的维度\n",
    "        :enc_units: 编码器中GRU层的隐含节点数\n",
    "        :batch_sz: 数据批次大小(每次参数更新用到的数据量)\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encode_units = encode_units\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \"\"\"\n",
    "               return_sequences：\n",
    "                       布尔值。是返回输出序列中的最后一个输出还是完整序列。 默认值：False。\n",
    "                       True代表返回GRU序列模型的每个时间步的输出(每个输出做连接操作)\n",
    "               return_state：\n",
    "                       布尔值。 除输出外，是否返回最后一个状态。 默认值：False。\n",
    "                       True代表除了返回输出外，还需要返回最后一个隐层状态。\n",
    "               recurrent_initializer：\n",
    "                       recurrent_kernel权重矩阵的初始化程序，用于对递归状态进行线性转换。 默认值：正交。\n",
    "                       'glorot_uniform'即循环状态张量的初始化方式为均匀分布。\n",
    "        \"\"\"\n",
    "        # 实例化gru层\n",
    "        # return_sequences=True代表返回GRU序列模型的每个时间步的输出(每个输出做连接操作)\n",
    "        # return_state=True代表除了返回输出外，还需要返回最后一个隐层状态\n",
    "        # recurrent_initializer='glorot_uniform'即循环状态张量的初始化方式为均匀分布\n",
    "        self.gru = tf.keras.layers.GRU(self.encode_units, return_sequences=True, return_state=True,\n",
    "                                       recurrent_initializer=\"glorot_uniform\")\n",
    "\n",
    "    # 定义调用函数\n",
    "    def call(self, x, hidden):\n",
    "        \"\"\"\n",
    "        @param x:\n",
    "        @param hidden:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        # 对输入进行embedding操作\n",
    "        x_embedding = self.embedding(x)\n",
    "        \"\"\"initial_state：要传递给单元格的第一个调用的初始状态张量的列表（可选，默认为None，这将导致创建零填充的初始状态张量）。\"\"\"\n",
    "        # 通过gru层获得最后一个时间步的输出和隐含状态\n",
    "        output, state = self.gru(x_embedding, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        \"\"\" (BATCH_SIZE, 隐藏层中的隐藏神经元数量) \"\"\"\n",
    "        # gru层的隐含节点对应的参数张量以零张量初始化\n",
    "        return tf.zeros((self.batch_size, self.encode_units))\n",
    "\n",
    "\n",
    "# 定义bahdanauAttention类，bahdanauAttention是常用的attention实现方法之一\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        # 注意力网络的初始化\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    \"\"\"\n",
    "    传入值：\n",
    "        features：编码器的输出，(64, 16, 1024) 即 (BATCH_SIZE, 输入序列最大长度句子的长度, 隐藏层中的隐藏神经元数量)\n",
    "        hidden：解码器的隐层输出状态，(64, 1024) 即 (batch_size, hidden_size) (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "    返回值：\n",
    "        attention_result：(64, 1024) 即 (batch size, units) (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "        attention_weights：(64, 16, 1) 即 (batch_size, sequence_length, 1) (BATCH_SIZE, 输入序列最大长度句子的长度, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \"\"\"\n",
    "        description: 具体计算函数\n",
    "        :param features: 编码器的输出\n",
    "        :param hidden: 解码器的隐层输出状态\n",
    "        return: 通过注意力机制处理后的结果和注意力权重attention_weights\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        1.hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "                解码器的隐层输出状态hidden，(64, 1024) 即 (batch_size, hidden_size) (BATCH_SIZE, 隐藏层中的隐藏神经元数量)。\n",
    "                hidden扩展一个维度从(64, 1024)变成(64, 1,1024)。\n",
    "        2.score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "                计算注意力得分score。\n",
    "                features：编码器的输出，(64, 16, 1024)。\n",
    "                hidden_with_time_axis：解码器的隐层输出状态，(64, 1,1024)\n",
    "                W1和W2：Dense(隐藏层中的隐藏神经元数量1024)\n",
    "                tanh(W1(features) + W2(hidden_with_time_axis))：\n",
    "                ---> tanh(W1((64, 16, 1024)) + W2((64, 1,1024)))\n",
    "                ---> tanh((64, 16, 1024))\n",
    "                ---> (64, 16, 1024) 即 (BATCH_SIZE, 输入序列最大长度句子的长度, 隐藏层中的隐藏神经元数量)\n",
    "        3.attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "                计算注意力权重attention_weights。\n",
    "                V：Dense(隐藏层中的隐藏神经元数量1)\n",
    "                softmax(V(score), axis=1)\n",
    "                ---> softmax(V((64, 16, 1024)), axis=1)\n",
    "                ---> softmax((64, 16, 1), axis=1)\n",
    "                ---> (64, 16, 1) 即 (BATCH_SIZE, 输入序列最大长度句子的长度, 1)\n",
    "                因为注意力得分score的形状是(BATCH_SIZE, 输入序列最大长度句子的长度, 隐藏层中的隐藏神经元数量)，\n",
    "                输入序列最大长度句子的长度(max_length)是输入的长度。\n",
    "                因为我们想为每个输入长度分配一个权重，所以softmax应该用在第一个轴(max_length)上axis=1，\n",
    "                而softmax默认被应用于最后一个轴axis=-1。\n",
    "        4.context_vector = tf.reduce_sum(attention_weights * features, axis=1)\n",
    "                获得注意力机制处理后的结果context_vector。\n",
    "                reduce_sum(attention_weights * features, axis=1)\n",
    "                ---> reduce_sum((64, 16, 1) * (64, 16, 1024), axis=1)\n",
    "                ---> reduce_sum((64, 16, 1024), axis=1)\n",
    "                ---> (64, 1024) 即 (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "        \"\"\"\n",
    "\n",
    "        # 将hidden增加一个维度,(batch_size, hidden_size) --> (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # 根据公式计算注意力得分, 输出score的形状为: (batch_size, 16, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        # 根据公式计算注意力权重, 输出attention_weights形状为: (batch_size, 16, 1)\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        # 最后根据公式获得注意力机制处理后的结果context_vector\n",
    "        # context_vector的形状为: (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        # 将乘机后的context_vector按行相加，进行压缩得到最终的context_vector\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "构建RNN解码器：这里RNN是指GRU, 同时在解码器中使用注意力机制.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, decode_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        # 初始化batch_size、decode_units、embedding 、gru 、fc、attention\n",
    "        self.batch_size = batch_size\n",
    "        self.decode_units = decode_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.decode_units, return_sequences=True, return_state=True,\n",
    "                                       recurrent_initializer=\"glorot_uniform\")\n",
    "\n",
    "        # 实例化一个Dense层作为输出层\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        # 在解码器阶段我们将使用注意力机制，这里实例化注意力的类\n",
    "        self.attention = BahdanauAttention(self.decode_units)\n",
    "\n",
    "    \"\"\"\n",
    "    1.x = self.embedding(x)\n",
    "            输入：(64, 1) 64行1列，批量大小句子数为64，1列为该行句子的第N列的单词\n",
    "            输出：(64, 1, 256) (BATCH_SIZE, 输入序列最大长度句子的长度, 嵌入维度)\n",
    "    2.context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "            attention_weights注意力权重：(64, 16, 1) 即 (BATCH_SIZE, 输入序列最大长度句子的长度, 1)\n",
    "            context_vector注意力机制处理后的结果：(64, 1024) 即 (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "    3.x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "            tf.expand_dims(context_vector, 1)：(64, 1, 1024) 即 (BATCH_SIZE, 1, 隐藏层中的隐藏神经元数量)\n",
    "            concat([(64, 1, 1024),(64, 1, 256)], axis=-1)：1024+256=1280，最终输出 (64, 1, 1280)\n",
    "    4.GRU\n",
    "        1.tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "            return_sequences：\n",
    "                    布尔值。是返回输出序列中的最后一个输出还是完整序列。 默认值：False。\n",
    "                    True代表返回GRU序列模型的每个时间步的输出(每个输出做连接操作)\n",
    "            return_state：\n",
    "                    布尔值。 除输出外，是否返回最后一个状态。 默认值：False。\n",
    "                    True代表除了返回输出外，还需要返回最后一个隐层状态。\n",
    "            recurrent_initializer：\n",
    "                    recurrent_kernel权重矩阵的初始化程序，用于对递归状态进行线性转换。 默认值：正交。\n",
    "                    'glorot_uniform'即循环状态张量的初始化方式为均匀分布。\n",
    "        2.output, state = gru(x)      \n",
    "            output：\n",
    "                    (64, 1, 1024) 即 (BATCH_SIZE, 1, 隐藏层中的隐藏神经元数量)\n",
    "                    (当前批次的样本个数, 当前样本的序列长度(单词个数), 隐藏层中神经元数量 * 1)\n",
    "            state：\n",
    "                    (64, 1024) 即 (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "    5.output = tf.reshape(output, (-1, output.shape[2]))\n",
    "             (-1, output.shape[2])：表示把(64, 1, 1024)转换为(64, 1024) 即 (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "    6.x = self.fc(output)\n",
    "            x：(64, 4935) 即 (BATCH_SIZE, 目标序列的不重复单词的总数作为目标序列的字典大小)\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, x, hidden, encode_ouput):\n",
    "        # print(\"x.shape\",x.shape) #(64, 1)。64行1列，批量大小句子数为64，1列为该行句子的第N列的单词\n",
    "\n",
    "        # 对decoder的输入通过embedding层\n",
    "        x = self.embedding(x)\n",
    "        # print(\"x1.shape\",x.shape) #(64, 1, 256)。(BATCH_SIZE, 输入序列最大长度句子的长度, 嵌入维度)\n",
    "\n",
    "        # 使用注意力规则计算hidden与enc_output的'相互影响程度(计算attention，输出上下文语境向量)\n",
    "        context_vector, attention_weights = self.attention(encode_ouput, hidden)\n",
    "        # print(\"tf.expand_dims(context_vector, 1).shape\",tf.expand_dims(context_vector, 1).shape) #(64, 1, 1024)\n",
    "\n",
    "        # 将这种'影响程度'与输入x拼接(这个操作也是注意力计算规则的一部分)（拼接上下文语境与decoder的输入embedding，并送入gru中）\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        # print(\"x2.shape\",x.shape) #(64, 1, 1280)\n",
    "\n",
    "        # 将新的x输入到gru层中得到输出\n",
    "        output, state = self.gru(x)\n",
    "        # print(\"output1.shape\",output.shape) #(64, 1, 1024) 即 (BATCH_SIZE, 1, 隐藏层中的隐藏神经元数量)\n",
    "        # print(\"state.shape\",state.shape) #(64, 1024) 即 (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "\n",
    "        # 改变输出形状使其适应全连接层的输入形式\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        # print(\"output2.shape\",output.shape) #(64, 1024) 即 (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "\n",
    "        # 使用全连接层作为输出层\n",
    "        # 输出的形状 == （批大小，vocab）\n",
    "        x = self.fc(output)\n",
    "        # print(\"x3.shape\",x.shape) #(64, 4935) 即 (BATCH_SIZE, 目标序列的不重复单词的总数作为目标序列的字典大小)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros(self.batch_size, self.decode_units)\n",
    "\n",
    "# 对训练语料进行处理，上下文分别加上start end表示\n",
    "def preprocess_sentence(w):\n",
    "    w = 'start ' + w + ' end'\n",
    "    return w\n",
    "\n",
    "# 定义word2number函数，通过对语料的处理提取词典，并进行word2number处理以及padding补全\n",
    "def tokenize(vocab_file):\n",
    "    # 从词典中读取预先生成tokenizer的config，构建词典矩阵\n",
    "    with open(vocab_file, 'r', encoding=\"utf-8\") as f:\n",
    "        tokenize_config = json.dumps(json.load(f), ensure_ascii=False)\n",
    "        lang_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenize_config)\n",
    "\n",
    "    # 利用词典进行word2number的转换以及padding处理\n",
    "    return lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化encoder、decoder、optimizer、checkpoint等\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义预测函数，根据上下文预测下文对话\n",
    "def predict(sentence):\n",
    "    # 从词典中读取预先 生成的tokenizer的config， 构建词典矩阵\n",
    "    input_tokenizer = tokenize(gConf[\"vocab_inp_path\"])\n",
    "    target_tokenizer = tokenize(gConf[\"vocab_tar_path\"])\n",
    "\n",
    "    # 加载预训练模型\n",
    "    checkpoint_dir = gConf[\"model_data\"]\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    # 对输入字句进行处理，加上 start end标示\n",
    "    # sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 进行word2number的转换\n",
    "    inputs = input_tokenizer.texts_to_sequences(tuple([sentence]))\n",
    "    # 进行padding补全\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "\n",
    "    # 初始化一个中间状态\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    # 对输入上文进行encoder编码，提取特征\n",
    "    encoder_out, encoder_hidden = encoder(inputs, hidden)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    # decoder的输入从 start的对应id 开始正向输入\n",
    "    decoder_input = tf.expand_dims([target_tokenizer.word_index['start']], 0)\n",
    "    # 在最大的语句长度范围内，使用模型中的decoder进行循环解码\n",
    "    for t in range(max_length_tar):\n",
    "        # 获得解码结果，并使用argmax确定概率最大的id\n",
    "        predictions, decoder_hidden, attention_weights = decoder(decoder_input, decoder_hidden,encoder_out)\n",
    "        predicted_id = tf.argmax(predictions[0].numpy())\n",
    "        # 判断当前id是否为 语句结束表示， 如果是则停止循环解码， 否则进行number2word的转换，并进行语句的拼接\n",
    "        if target_tokenizer.index_word[predicted_id.numpy()] == 'end':\n",
    "            break\n",
    "\n",
    "        result += str(target_tokenizer.index_word[predicted_id.numpy()]) + ' '\n",
    "        # 将预测得到的id作为下一个时刻的decoder输入\n",
    "        decoder_input = tf.expand_dims([predicted_id.numpy()], 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\" \".join(jieba.cut(\"你是谁\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f91dfcebda0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # 加载预训练模型\n",
    "checkpoint_dir = gConf[\"model_data\"]\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tmp/chatbot/0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tmp/chatbot/0001/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(checkpoint, './tmp/chatbot/0001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.saved_model.contains_saved_model(\n",
    "    './tmp/chatbot/0001'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6_tf2.6",
   "language": "python",
   "name": "py3.6_tf2.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2ba7f227dd2f82ec0cdc722a377d43f03afd14732d34352df098cc3be507649"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
