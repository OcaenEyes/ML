{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf165f41-09bb-4787-8dbf-5b50e6ab38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from configparser import SafeConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b92047e-9feb-4d84-926b-b3d20a280570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaozhiyong/Documents/pyenv/py3.6_tf2.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: The SafeConfigParser class has been renamed to ConfigParser in Python 3.2. This alias will be removed in future versions. Use ConfigParser directly instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./seq2seq.ini']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = SafeConfigParser()\n",
    "parser.read(\"./seq2seq.ini\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11a019c-15ec-4f45-9d34-c25caf9d6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "_conf_ints = [ (key, int(value)) for key,value in parser.items('ints')]\n",
    "_conf_floats = [ (key, float(value)) for key,value in parser.items('floats') ]\n",
    "_conf_strings = [ (key, str(value)) for key,value in parser.items('strings') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65468e5b-6b79-42c7-8b3c-e3a1d9b33dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_inp_size': 20000,\n",
       " 'vocab_tar_size': 20000,\n",
       " 'embedding_dim': 128,\n",
       " 'train_epoch': 10,\n",
       " 'layer_size': 512,\n",
       " 'batch_size': 64,\n",
       " 'max_length': 20,\n",
       " 'number_work': 2,\n",
       " 'min_loss': 0.2,\n",
       " 'mode': 'train',\n",
       " 'train_data': 'datasets',\n",
       " 'seq_data': 'datasets/seq.data',\n",
       " 'vocab_inp_path': 'datasets/inp.vocab',\n",
       " 'vocab_tar_path': 'datasets/tar.vocab',\n",
       " 'resource_data': 'datasets/xiaohuangji50w_nofenci.conv',\n",
       " 'split_train_data': 'datasets/seq_data_',\n",
       " 'e': 'E',\n",
       " 'm': 'M',\n",
       " 'model_data': 'model_data',\n",
       " 'log_dir': 'log_dir'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(_conf_ints+_conf_floats+_conf_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f4d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import jieba\n",
    "from zhon.hanzi import punctuation\n",
    "from get_config import get_config\n",
    "import io\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85229adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gaozhiyong/Documents/GitHub/ML/nlp/聊天机器人/tf2.6_prj/seq2seq.ini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaozhiyong/Documents/pyenv/py3.6_tf2.6/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The SafeConfigParser class has been renamed to ConfigParser in Python 3.2. This alias will be removed in future versions. Use ConfigParser directly instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "conf_file = os.getcwd() + \"/seq2seq.ini\"\n",
    "print(conf_file)\n",
    "parser = SafeConfigParser()\n",
    "parser.read(conf_file,encoding=\"utf-8\")\n",
    "_conf_ints = [(key, int(value)) for key, value in parser.items('ints')]\n",
    "_conf_floats = [(key, float(value)) for key, value in parser.items('floats')]\n",
    "_conf_strings = [(key, str(value)) for key, value in parser.items('strings')]\n",
    "gConf= dict(_conf_ints+_conf_floats+_conf_strings)\n",
    "\n",
    "conv_path = gConf[\"resource_data\"]\n",
    "vocab_inp_path = gConf[\"vocab_inp_path\"]\n",
    "vocab_tar_path = gConf[\"vocab_tar_path\"]\n",
    "vocab_inp_size = gConf[\"vocab_inp_size\"]\n",
    "vocab_tar_size = gConf[\"vocab_tar_size\"]\n",
    "seq_train = gConf[\"seq_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fd10ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predata_util():\n",
    "    # 判断训语料文件是否存在，如果不存在则提醒\n",
    "    if not os.path.exists(conv_path):\n",
    "        print(\"找不到conv文件\")\n",
    "        exit()\n",
    "\n",
    "    # 新建一个文件，用于存放处理后的对话语料\n",
    "    seq_train_file = open(seq_train, \"w\",encoding=\"utf-8\")\n",
    "    # 打开要处理的语料，逐条读取并进行数据处理\n",
    "    with open(conv_path, encoding=\"utf-8\") as f:\n",
    "        one_conv = \"\"  # 存储一次完整的对话\n",
    "        i = 0\n",
    "        # 开始循环语料\n",
    "        for line in f:\n",
    "            line = line.strip(\"\\n\")\n",
    "            line = re.sub(r\"[%s]+\" % punctuation, \"\", line)  # 去除标点符号\n",
    "            if line == \"\":\n",
    "                continue\n",
    "            # 判断是否为一段对话的开始，如果是，则把刚处理过的语料保存下来\n",
    "            if line[0] == gConf[\"e\"]:\n",
    "                if one_conv:\n",
    "                    seq_train_file.write(one_conv[:-1] + \"\\n\")\n",
    "                    i = i + 1\n",
    "                    if i % 1000 == 0:\n",
    "                        print(\"处理进度：\", i)\n",
    "                one_conv = \"\"\n",
    "\n",
    "            # 判断是否正在处理对华语剧，如果是则进行语料的拼接处理， 以及分词\n",
    "            elif line[0] == gConf['m']:\n",
    "                one_conv = one_conv + str(\" \".join(jieba.cut(line.split(\" \")[1]))) + \"\\t\"  # 存储一次问答\n",
    "\n",
    "    # 处理完成，关闭文件\n",
    "    seq_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "441e65b4-2035-4994-a669-75644a175b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(lang, vocab_path, vocab_size):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=3)\n",
    "    tokenizer.fit_on_texts(lang)\n",
    "    vocab = json.loads(tokenizer.to_json(ensure_ascii=False))\n",
    "    vocab[\"index_word\"] = tokenizer.index_word\n",
    "    vocab[\"word_index\"] = tokenizer.word_index\n",
    "    vocab[\"document_count\"] = tokenizer.document_count\n",
    "    vocab = json.dumps(vocab,ensure_ascii=False)\n",
    "    with open(vocab_path,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(vocab)\n",
    "    f.close()\n",
    "    print(\"字典存在:{}\".format(vocab_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc11d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = 'start'+w +'end'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13f6460a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理进度： 1000\n",
      "处理进度： 2000\n",
      "处理进度： 3000\n",
      "处理进度： 4000\n",
      "处理进度： 5000\n",
      "处理进度： 6000\n",
      "处理进度： 7000\n",
      "处理进度： 8000\n",
      "处理进度： 9000\n",
      "处理进度： 10000\n",
      "处理进度： 11000\n",
      "处理进度： 12000\n",
      "处理进度： 13000\n",
      "处理进度： 14000\n",
      "处理进度： 15000\n",
      "处理进度： 16000\n",
      "处理进度： 17000\n",
      "处理进度： 18000\n",
      "处理进度： 19000\n",
      "处理进度： 20000\n",
      "处理进度： 21000\n",
      "处理进度： 22000\n",
      "处理进度： 23000\n",
      "处理进度： 24000\n",
      "处理进度： 25000\n",
      "处理进度： 26000\n",
      "处理进度： 27000\n",
      "处理进度： 28000\n",
      "处理进度： 29000\n",
      "处理进度： 30000\n",
      "处理进度： 31000\n",
      "处理进度： 32000\n",
      "处理进度： 33000\n",
      "处理进度： 34000\n",
      "处理进度： 35000\n",
      "处理进度： 36000\n",
      "处理进度： 37000\n",
      "处理进度： 38000\n",
      "处理进度： 39000\n",
      "处理进度： 40000\n",
      "处理进度： 41000\n",
      "处理进度： 42000\n",
      "处理进度： 43000\n",
      "处理进度： 44000\n",
      "处理进度： 45000\n",
      "处理进度： 46000\n",
      "处理进度： 47000\n",
      "处理进度： 48000\n",
      "处理进度： 49000\n",
      "处理进度： 50000\n",
      "处理进度： 51000\n",
      "处理进度： 52000\n",
      "处理进度： 53000\n",
      "处理进度： 54000\n",
      "处理进度： 55000\n",
      "处理进度： 56000\n",
      "处理进度： 57000\n",
      "处理进度： 58000\n",
      "处理进度： 59000\n",
      "处理进度： 60000\n",
      "处理进度： 61000\n",
      "处理进度： 62000\n",
      "处理进度： 63000\n",
      "处理进度： 64000\n",
      "处理进度： 65000\n",
      "处理进度： 66000\n",
      "处理进度： 67000\n",
      "处理进度： 68000\n",
      "处理进度： 69000\n",
      "处理进度： 70000\n",
      "处理进度： 71000\n",
      "处理进度： 72000\n",
      "处理进度： 73000\n",
      "处理进度： 74000\n",
      "处理进度： 75000\n",
      "处理进度： 76000\n",
      "处理进度： 77000\n",
      "处理进度： 78000\n",
      "处理进度： 79000\n",
      "处理进度： 80000\n",
      "处理进度： 81000\n",
      "处理进度： 82000\n",
      "处理进度： 83000\n",
      "处理进度： 84000\n",
      "处理进度： 85000\n",
      "处理进度： 86000\n",
      "处理进度： 87000\n",
      "处理进度： 88000\n",
      "处理进度： 89000\n",
      "处理进度： 90000\n",
      "处理进度： 91000\n",
      "处理进度： 92000\n",
      "处理进度： 93000\n",
      "处理进度： 94000\n",
      "处理进度： 95000\n",
      "处理进度： 96000\n",
      "处理进度： 97000\n",
      "处理进度： 98000\n",
      "处理进度： 99000\n",
      "处理进度： 100000\n",
      "处理进度： 101000\n",
      "处理进度： 102000\n",
      "处理进度： 103000\n",
      "处理进度： 104000\n",
      "处理进度： 105000\n",
      "处理进度： 106000\n",
      "处理进度： 107000\n",
      "处理进度： 108000\n",
      "处理进度： 109000\n",
      "处理进度： 110000\n",
      "处理进度： 111000\n",
      "处理进度： 112000\n",
      "处理进度： 113000\n",
      "处理进度： 114000\n",
      "处理进度： 115000\n",
      "处理进度： 116000\n",
      "处理进度： 117000\n",
      "处理进度： 118000\n",
      "处理进度： 119000\n",
      "处理进度： 120000\n",
      "处理进度： 121000\n",
      "处理进度： 122000\n",
      "处理进度： 123000\n",
      "处理进度： 124000\n",
      "处理进度： 125000\n",
      "处理进度： 126000\n",
      "处理进度： 127000\n",
      "处理进度： 128000\n",
      "处理进度： 129000\n",
      "处理进度： 130000\n",
      "处理进度： 131000\n",
      "处理进度： 132000\n",
      "处理进度： 133000\n",
      "处理进度： 134000\n",
      "处理进度： 135000\n",
      "处理进度： 136000\n",
      "处理进度： 137000\n",
      "处理进度： 138000\n",
      "处理进度： 139000\n",
      "处理进度： 140000\n",
      "处理进度： 141000\n",
      "处理进度： 142000\n",
      "处理进度： 143000\n",
      "处理进度： 144000\n",
      "处理进度： 145000\n",
      "处理进度： 146000\n",
      "处理进度： 147000\n",
      "处理进度： 148000\n",
      "处理进度： 149000\n",
      "处理进度： 150000\n",
      "处理进度： 151000\n",
      "处理进度： 152000\n",
      "处理进度： 153000\n",
      "处理进度： 154000\n",
      "处理进度： 155000\n",
      "处理进度： 156000\n",
      "处理进度： 157000\n",
      "处理进度： 158000\n",
      "处理进度： 159000\n",
      "处理进度： 160000\n",
      "处理进度： 161000\n",
      "处理进度： 162000\n",
      "处理进度： 163000\n",
      "处理进度： 164000\n",
      "处理进度： 165000\n",
      "处理进度： 166000\n",
      "处理进度： 167000\n",
      "处理进度： 168000\n",
      "处理进度： 169000\n",
      "处理进度： 170000\n",
      "处理进度： 171000\n",
      "处理进度： 172000\n",
      "处理进度： 173000\n",
      "处理进度： 174000\n",
      "处理进度： 175000\n",
      "处理进度： 176000\n",
      "处理进度： 177000\n",
      "处理进度： 178000\n",
      "处理进度： 179000\n",
      "处理进度： 180000\n",
      "处理进度： 181000\n",
      "处理进度： 182000\n",
      "处理进度： 183000\n",
      "处理进度： 184000\n",
      "处理进度： 185000\n",
      "处理进度： 186000\n",
      "处理进度： 187000\n",
      "处理进度： 188000\n",
      "处理进度： 189000\n",
      "处理进度： 190000\n",
      "处理进度： 191000\n",
      "处理进度： 192000\n",
      "处理进度： 193000\n",
      "处理进度： 194000\n",
      "处理进度： 195000\n",
      "处理进度： 196000\n",
      "处理进度： 197000\n",
      "处理进度： 198000\n",
      "处理进度： 199000\n",
      "处理进度： 200000\n",
      "处理进度： 201000\n",
      "处理进度： 202000\n",
      "处理进度： 203000\n",
      "处理进度： 204000\n",
      "处理进度： 205000\n",
      "处理进度： 206000\n",
      "处理进度： 207000\n",
      "处理进度： 208000\n",
      "处理进度： 209000\n",
      "处理进度： 210000\n",
      "处理进度： 211000\n",
      "处理进度： 212000\n",
      "处理进度： 213000\n",
      "处理进度： 214000\n",
      "处理进度： 215000\n",
      "处理进度： 216000\n",
      "处理进度： 217000\n",
      "处理进度： 218000\n",
      "处理进度： 219000\n",
      "处理进度： 220000\n",
      "处理进度： 221000\n",
      "处理进度： 222000\n",
      "处理进度： 223000\n",
      "处理进度： 224000\n",
      "处理进度： 225000\n",
      "处理进度： 226000\n",
      "处理进度： 227000\n",
      "处理进度： 228000\n",
      "处理进度： 229000\n",
      "处理进度： 230000\n",
      "处理进度： 231000\n",
      "处理进度： 232000\n",
      "处理进度： 233000\n",
      "处理进度： 234000\n",
      "处理进度： 235000\n",
      "处理进度： 236000\n",
      "处理进度： 237000\n",
      "处理进度： 238000\n",
      "处理进度： 239000\n",
      "处理进度： 240000\n",
      "处理进度： 241000\n",
      "处理进度： 242000\n",
      "处理进度： 243000\n",
      "处理进度： 244000\n",
      "处理进度： 245000\n",
      "处理进度： 246000\n",
      "处理进度： 247000\n",
      "处理进度： 248000\n",
      "处理进度： 249000\n",
      "处理进度： 250000\n",
      "处理进度： 251000\n",
      "处理进度： 252000\n",
      "处理进度： 253000\n",
      "处理进度： 254000\n",
      "处理进度： 255000\n",
      "处理进度： 256000\n",
      "处理进度： 257000\n",
      "处理进度： 258000\n",
      "处理进度： 259000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-e1039cacb852>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredata_util\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mword_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpreprocess_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0minput_lang\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_lang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mword_pairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-83e9471c9c29>\u001b[0m in \u001b[0;36mpredata_util\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mgConf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"e\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mone_conv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                     \u001b[0mseq_train_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_conv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predata_util()\n",
    "\n",
    "lines = io.open(seq_train,encoding=\"utf-8\").readlines()\n",
    "word_pairs = [[preprocess_sentence(w) for w in l.split(\"\\t\")] for l in lines]\n",
    "input_lang,target_lang = zip(*word_pairs)\n",
    "\n",
    "create_vocab(input_lang,vocab_inp_path,vocab_inp_size)\n",
    "create_vocab(target_lang,vocab_tar_path,vocab_tar_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20012fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过超参字典为vocab_inp_size、vocab_tar_size、embedding_dim、units等赋值\n",
    "vocab_inp_size = gConf[\"vocab_inp_size\"]\n",
    "vocab_tar_size = gConf[\"vocab_tar_size\"]\n",
    "embedding_dim = gConf[\"embedding_dim\"]\n",
    "units = gConf[\"layer_size\"]\n",
    "BATCH_SIZE = gConf[\"batch_size\"]\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "255cbf9e-bf51-474c-b89d-7a403a6509cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义encoder类\n",
    "class Encoder(tf.keras.Model):\n",
    "    # 初始化参数，对默认参数进行初始化\n",
    "    def __init__(self,vocab_size,emdedding_dim,encode_units,batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encode_units=encode_units\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.encode_units,return_sequences=True,return_state=True,recurrent_initializer=\"glorot_uniform\")\n",
    "        \n",
    "    # 定义调用函数    \n",
    "    def call(self,x,hidden):\n",
    "        x_embedding = self.embedding(x)\n",
    "        output,state = self.gru(x_embedding,initial_state=hidden)\n",
    "        return output,state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encode_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "229bbf72-c358-4382-a1da-4a5efc7f60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义bahdanauAttention类，bahdanauAttention是常用的attention实现方法之一\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self,units):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "        # 注意力网络的初始化\n",
    "        self.W1= tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \"\"\"\n",
    "    传入值：\n",
    "        features：编码器的输出，(64, 16, 1024) 即 (BATCH_SIZE, 输入序列最大长度句子的长度, 隐藏层中的隐藏神经元数量)\n",
    "        hidden：解码器的隐层输出状态，(64, 1024) 即 (batch_size, hidden_size) (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "    返回值：\n",
    "        attention_result：(64, 1024) 即 (batch size, units) (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "        attention_weights：(64, 16, 1) 即 (batch_size, sequence_length, 1) (BATCH_SIZE, 输入序列最大长度句子的长度, 1)\n",
    "    \"\"\"\n",
    "    def call(self,features,hidden):\n",
    "        \n",
    "        \"\"\"\n",
    "        description: 具体计算函数\n",
    "        :param features: 编码器的输出\n",
    "        :param hidden: 解码器的隐层输出状态\n",
    "        return: 通过注意力机制处理后的结果和注意力权重attention_weights\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        1.hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "                解码器的隐层输出状态hidden，(64, 1024) 即 (batch_size, hidden_size) (BATCH_SIZE, 隐藏层中的隐藏神经元数量)。\n",
    "                hidden扩展一个维度从(64, 1024)变成(64, 1,1024)。\n",
    "        2.score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "                计算注意力得分score。\n",
    "                features：编码器的输出，(64, 16, 1024)。\n",
    "                hidden_with_time_axis：解码器的隐层输出状态，(64, 1,1024)\n",
    "                W1和W2：Dense(隐藏层中的隐藏神经元数量1024)\n",
    "                tanh(W1(features) + W2(hidden_with_time_axis))：\n",
    "                ---> tanh(W1((64, 16, 1024)) + W2((64, 1,1024)))\n",
    "                ---> tanh((64, 16, 1024))\n",
    "                ---> (64, 16, 1024) 即 (BATCH_SIZE, 输入序列最大长度句子的长度, 隐藏层中的隐藏神经元数量)\n",
    "        3.attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "                计算注意力权重attention_weights。\n",
    "                V：Dense(隐藏层中的隐藏神经元数量1)\n",
    "                softmax(V(score), axis=1)\n",
    "                ---> softmax(V((64, 16, 1024)), axis=1)\n",
    "                ---> softmax((64, 16, 1), axis=1)\n",
    "                ---> (64, 16, 1) 即 (BATCH_SIZE, 输入序列最大长度句子的长度, 1)\n",
    "                因为注意力得分score的形状是(BATCH_SIZE, 输入序列最大长度句子的长度, 隐藏层中的隐藏神经元数量)，\n",
    "                输入序列最大长度句子的长度(max_length)是输入的长度。\n",
    "                因为我们想为每个输入长度分配一个权重，所以softmax应该用在第一个轴(max_length)上axis=1，\n",
    "                而softmax默认被应用于最后一个轴axis=-1。\n",
    "        4.context_vector = tf.reduce_sum(attention_weights * features, axis=1)\n",
    "                获得注意力机制处理后的结果context_vector。\n",
    "                reduce_sum(attention_weights * features, axis=1)\n",
    "                ---> reduce_sum((64, 16, 1) * (64, 16, 1024), axis=1)\n",
    "                ---> reduce_sum((64, 16, 1024), axis=1)\n",
    "                ---> (64, 1024) 即 (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "        \"\"\"\n",
    "        \n",
    "        # 将hidden增加一个维度,(batch_size, hidden_size) --> (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden,1)\n",
    "        # 根据公式计算注意力得分, 输出score的形状为: (batch_size, 16, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features)+self.W2(hidden_with_time_axis))\n",
    "        # 根据公式计算注意力权重, 输出attention_weights形状为: (batch_size, 16, 1)\n",
    "        attention_weights = tf.nn.softmax(self.V(score),axis=1)\n",
    "        # 最后根据公式获得注意力机制处理后的结果context_vector\n",
    "        # context_vector的形状为: (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        # 将乘机后的context_vector按行相加，进行压缩得到最终的context_vector\n",
    "        context_vector = tf.reduce_sum(context_vector)\n",
    "        return context_vector,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "857b0c1a-2d8a-4c6f-8ccd-e8c3367215f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "构建RNN解码器：这里RNN是指GRU, 同时在解码器中使用注意力机制.\n",
    "\"\"\"\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_dim,decode_units,batch_size):\n",
    "        super(Decoder,self).__init__()\n",
    "        # 初始化batch_size、decode_units、embedding 、gru 、fc、attention\n",
    "        self.batch_size = batch_size\n",
    "        self.decode_units = decode_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.decode_units, return_sequences=True,return_state=True, recurrent_initializer=\"glorot_uniform\")\n",
    "        \n",
    "        # 实例化一个Dense层作为输出层\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        # 在解码器阶段我们将使用注意力机制，这里实例化注意力的类\n",
    "        self.attention = BahdanauAttention(self.decode_units)\n",
    "        \n",
    "    \"\"\"\n",
    "    1.x = self.embedding(x)\n",
    "            输入：(64, 1) 64行1列，批量大小句子数为64，1列为该行句子的第N列的单词\n",
    "            输出：(64, 1, 256) (BATCH_SIZE, 输入序列最大长度句子的长度, 嵌入维度)\n",
    "    2.context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "            attention_weights注意力权重：(64, 16, 1) 即 (BATCH_SIZE, 输入序列最大长度句子的长度, 1)\n",
    "            context_vector注意力机制处理后的结果：(64, 1024) 即 (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "    3.x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "            tf.expand_dims(context_vector, 1)：(64, 1, 1024) 即 (BATCH_SIZE, 1, 隐藏层中的隐藏神经元数量)\n",
    "            concat([(64, 1, 1024),(64, 1, 256)], axis=-1)：1024+256=1280，最终输出 (64, 1, 1280)\n",
    "    4.GRU\n",
    "        1.tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "            return_sequences：\n",
    "                    布尔值。是返回输出序列中的最后一个输出还是完整序列。 默认值：False。\n",
    "                    True代表返回GRU序列模型的每个时间步的输出(每个输出做连接操作)\n",
    "            return_state：\n",
    "                    布尔值。 除输出外，是否返回最后一个状态。 默认值：False。\n",
    "                    True代表除了返回输出外，还需要返回最后一个隐层状态。\n",
    "            recurrent_initializer：\n",
    "                    recurrent_kernel权重矩阵的初始化程序，用于对递归状态进行线性转换。 默认值：正交。\n",
    "                    'glorot_uniform'即循环状态张量的初始化方式为均匀分布。\n",
    "        2.output, state = gru(x)      \n",
    "            output：\n",
    "                    (64, 1, 1024) 即 (BATCH_SIZE, 1, 隐藏层中的隐藏神经元数量)\n",
    "                    (当前批次的样本个数, 当前样本的序列长度(单词个数), 隐藏层中神经元数量 * 1)\n",
    "            state：\n",
    "                    (64, 1024) 即 (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "    5.output = tf.reshape(output, (-1, output.shape[2]))\n",
    "             (-1, output.shape[2])：表示把(64, 1, 1024)转换为(64, 1024) 即 (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "    6.x = self.fc(output)\n",
    "            x：(64, 4935) 即 (BATCH_SIZE, 目标序列的不重复单词的总数作为目标序列的字典大小)\n",
    "    \"\"\"\n",
    "    \n",
    "    def call(self,x,hidden,encode_ouput):\n",
    "        # print(\"x.shape\",x.shape) #(64, 1)。64行1列，批量大小句子数为64，1列为该行句子的第N列的单词\n",
    "        \n",
    "        # 对decoder的输入通过embedding层\n",
    "        x = self.embedding(x)\n",
    "        # print(\"x1.shape\",x.shape) #(64, 1, 256)。(BATCH_SIZE, 输入序列最大长度句子的长度, 嵌入维度)\n",
    "        \n",
    "        # 使用注意力规则计算hidden与enc_output的'相互影响程度(计算attention，输出上下文语境向量)\n",
    "        context_vector,attention_weights = self.attention(encode_ouput,hidden)\n",
    "        # print(\"tf.expand_dims(context_vector, 1).shape\",tf.expand_dims(context_vector, 1).shape) #(64, 1, 1024)\n",
    "        \n",
    "        # 将这种'影响程度'与输入x拼接(这个操作也是注意力计算规则的一部分)（拼接上下文语境与decoder的输入embedding，并送入gru中）\n",
    "        x = tf.concat([tf.expand_dims(context_vector,1),x])\n",
    "        # print(\"x2.shape\",x.shape) #(64, 1, 1280)\n",
    "        \n",
    "        # 将新的x输入到gru层中得到输出\n",
    "        output,state = self.gru(x)\n",
    "        # print(\"output1.shape\",output.shape) #(64, 1, 1024) 即 (BATCH_SIZE, 1, 隐藏层中的隐藏神经元数量)\n",
    "        # print(\"state.shape\",state.shape) #(64, 1024) 即 (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "        \n",
    "        # 改变输出形状使其适应全连接层的输入形式\n",
    "        output = tf.reshape(output,(-1,output.shape[2]))\n",
    "        # print(\"output2.shape\",output.shape) #(64, 1024) 即 (BATCH_SIZE, 隐藏层中的隐藏神经元数量)\n",
    "        \n",
    "        # 使用全连接层作为输出层\n",
    "        # 输出的形状 == （批大小，vocab）\n",
    "        x = self.fc(output)\n",
    "        # print(\"x3.shape\",x.shape) #(64, 4935) 即 (BATCH_SIZE, 目标序列的不重复单词的总数作为目标序列的字典大小)\n",
    "        \n",
    "        return x,state,attention_weights\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros(self.batch_size,self.decode_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b71a907-a7a2-483a-a2d6-580f7913ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "def loss_function(real,pred):\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    # mask掉start,去除start对于loss的干扰\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    loss_ = loss_object(real,pred)\n",
    "    mask = tf.cast(mask,dtype=loss_.dtype)\n",
    "    loss_ *=mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0705456-333b-4944-8ef3-e7fc8d9c607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化encoder、decoder、optimizer、checkpoint等\n",
    "encoder = Encoder(vocab_inp_size,embedding_dim,units,BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size,embedding_dim,units,BATCH_SIZE)\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9996670d-82e1-4925-8d60-d393fe4e08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(inp,targ,target_lang,encode_hidden):\n",
    "    loss=0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encode_output,encode_hidden = encoder(inp,encode_hidden)\n",
    "        decode_hidden = encode_hidden\n",
    "        \n",
    "        decode_input = tf.expand_dims([target_lang.word_index[\"start\"]] * BATCH_SIZE ,1)\n",
    "        for t in range(1,targ.shape[1]):\n",
    "            predictions,decode_hidden,_ = decoder(decode_input,decode_hidden,encode_output)\n",
    "            \n",
    "            loss += loss_function(targ[:,t],predictions)\n",
    "            decode_input = tf.expand_dims(target_lang[:,t],1)\n",
    "            \n",
    "    step_loss = (loss / int(targ.shape[1]))\n",
    "    \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradients(loss,variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients,variables))\n",
    "    \n",
    "    return step_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc0ddf-2d49-4923-9f4a-480ea2045bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6_tf2.6",
   "language": "python",
   "name": "py3.6_tf2.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
