{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.keras.layers.Embedding(\n",
    "    input_dim, output_dim, embeddings_initializer='uniform',\n",
    "    embeddings_regularizer=None, activity_regularizer=None,\n",
    "    embeddings_constraint=None, mask_zero=False, input_length=None, **kwargs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=10\n",
    "out_dim=10\n",
    "batch_size =32\n",
    "units = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x1bcfff30320>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 其中1000表示input_dimension,也就是词汇表的大小，size of vocabulary\n",
    "embed = keras.layers.Embedding(input_dim=vocab_size,output_dim=out_dim)\n",
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7, 8],\n",
       "       [7, 1],\n",
       "       [0, 0],\n",
       "       [8, 8],\n",
       "       [9, 9],\n",
       "       [6, 4],\n",
       "       [8, 1],\n",
       "       [9, 7],\n",
       "       [8, 5],\n",
       "       [8, 9],\n",
       "       [7, 5],\n",
       "       [2, 2],\n",
       "       [4, 9],\n",
       "       [9, 8],\n",
       "       [3, 2],\n",
       "       [2, 4],\n",
       "       [3, 6],\n",
       "       [0, 1],\n",
       "       [8, 2],\n",
       "       [2, 9],\n",
       "       [4, 0],\n",
       "       [3, 9],\n",
       "       [5, 1],\n",
       "       [0, 5],\n",
       "       [9, 9],\n",
       "       [4, 2],\n",
       "       [9, 8],\n",
       "       [4, 9],\n",
       "       [3, 9],\n",
       "       [7, 9],\n",
       "       [4, 3],\n",
       "       [6, 4]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x=np.random.randint(10,size=(batch_size,2))\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2, 10), dtype=float32, numpy=\n",
       "array([[[-0.04455454,  0.04463908,  0.04506776,  0.02344776,\n",
       "         -0.04919156, -0.04341774, -0.0441567 ,  0.01678076,\n",
       "         -0.01249163,  0.0438653 ],\n",
       "        [-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007]],\n",
       "\n",
       "       [[-0.04455454,  0.04463908,  0.04506776,  0.02344776,\n",
       "         -0.04919156, -0.04341774, -0.0441567 ,  0.01678076,\n",
       "         -0.01249163,  0.0438653 ],\n",
       "        [ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "         -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "         -0.01525488, -0.04575637]],\n",
       "\n",
       "       [[-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "          0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "          0.01806841,  0.03489599],\n",
       "        [-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "          0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "          0.01806841,  0.03489599]],\n",
       "\n",
       "       [[-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007],\n",
       "        [-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007]],\n",
       "\n",
       "       [[ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "         -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "         -0.03513166,  0.03322264],\n",
       "        [ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461]],\n",
       "\n",
       "       [[-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007],\n",
       "        [ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "         -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "         -0.01525488, -0.04575637]],\n",
       "\n",
       "       [[ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803],\n",
       "        [-0.04455454,  0.04463908,  0.04506776,  0.02344776,\n",
       "         -0.04919156, -0.04341774, -0.0441567 ,  0.01678076,\n",
       "         -0.01249163,  0.0438653 ]],\n",
       "\n",
       "       [[-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007],\n",
       "        [-0.02563901, -0.01560853,  0.012044  ,  0.01931993,\n",
       "         -0.03744175,  0.03179225, -0.04118588, -0.01800214,\n",
       "          0.01870343,  0.01538343]],\n",
       "\n",
       "       [[-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[-0.04455454,  0.04463908,  0.04506776,  0.02344776,\n",
       "         -0.04919156, -0.04341774, -0.0441567 ,  0.01678076,\n",
       "         -0.01249163,  0.0438653 ],\n",
       "        [-0.02563901, -0.01560853,  0.012044  ,  0.01931993,\n",
       "         -0.03744175,  0.03179225, -0.04118588, -0.01800214,\n",
       "          0.01870343,  0.01538343]],\n",
       "\n",
       "       [[-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "         -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "          0.01271724, -0.02674476],\n",
       "        [-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "         -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "          0.01271724, -0.02674476]],\n",
       "\n",
       "       [[ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803],\n",
       "        [-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007]],\n",
       "\n",
       "       [[-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "         -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "         -0.01439886, -0.01674361],\n",
       "        [-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "         -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "          0.01271724, -0.02674476]],\n",
       "\n",
       "       [[-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "         -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "          0.01271724, -0.02674476],\n",
       "        [ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461]],\n",
       "\n",
       "       [[-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "         -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "         -0.01439886, -0.01674361],\n",
       "        [-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "         -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "         -0.03513166,  0.03322264]],\n",
       "\n",
       "       [[-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "          0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "          0.01806841,  0.03489599],\n",
       "        [ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "         -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "         -0.01525488, -0.04575637]],\n",
       "\n",
       "       [[-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007],\n",
       "        [-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "         -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "          0.01271724, -0.02674476]],\n",
       "\n",
       "       [[-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "         -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "          0.01271724, -0.02674476],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461],\n",
       "        [-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "          0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "          0.01806841,  0.03489599]],\n",
       "\n",
       "       [[-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "         -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "         -0.01439886, -0.01674361],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[-0.02563901, -0.01560853,  0.012044  ,  0.01931993,\n",
       "         -0.03744175,  0.03179225, -0.04118588, -0.01800214,\n",
       "          0.01870343,  0.01538343],\n",
       "        [ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "         -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "         -0.01525488, -0.04575637]],\n",
       "\n",
       "       [[-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "          0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "          0.01806841,  0.03489599],\n",
       "        [-0.02563901, -0.01560853,  0.012044  ,  0.01931993,\n",
       "         -0.03744175,  0.03179225, -0.04118588, -0.01800214,\n",
       "          0.01870343,  0.01538343]],\n",
       "\n",
       "       [[ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461],\n",
       "        [-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "         -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "          0.01271724, -0.02674476]],\n",
       "\n",
       "       [[ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803],\n",
       "        [-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007]],\n",
       "\n",
       "       [[ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "         -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "         -0.01439886, -0.01674361],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[-0.04455454,  0.04463908,  0.04506776,  0.02344776,\n",
       "         -0.04919156, -0.04341774, -0.0441567 ,  0.01678076,\n",
       "         -0.01249163,  0.0438653 ],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461],\n",
       "        [-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "         -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "         -0.01439886, -0.01674361]],\n",
       "\n",
       "       [[-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "         -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "         -0.03513166,  0.03322264],\n",
       "        [ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461]]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[0,1],[1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 10), dtype=float32, numpy=\n",
       "array([[[-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "          0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "          0.01806841,  0.03489599],\n",
       "        [ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "         -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "         -0.01525488, -0.04575637]],\n",
       "\n",
       "       [[ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "         -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "         -0.01525488, -0.04575637],\n",
       "        [-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "         -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "          0.01271724, -0.02674476]]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(np.array([[0,1],[1,2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.keras.layers.GRU(\n",
    "    units, activation='tanh', recurrent_activation='sigmoid',\n",
    "    use_bias=True, kernel_initializer='glorot_uniform',\n",
    "    recurrent_initializer='orthogonal',\n",
    "    bias_initializer='zeros', kernel_regularizer=None,\n",
    "    recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "    kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,\n",
    "    dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False,\n",
    "    go_backwards=False, stateful=False, unroll=False, time_major=False,\n",
    "    reset_after=True, **kwargs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 参数\n",
    "\n",
    "```\n",
    "units 正整数，输出空间的维度。\n",
    "activation 要使用的激活函数。默认值：双曲正切(tanh)。如果您通过 None ，则不会应用激活(即 \"linear\" 激活：a(x) = x )。\n",
    "recurrent_activation 用于循环步骤的激活函数。默认值：sigmoid (sigmoid)。如果您通过 None ，则不会应用激活(即 \"linear\" 激活：a(x) = x )。\n",
    "use_bias 布尔值，(默认 True )，图层是否使用偏置向量。\n",
    "kernel_initializer kernel 权重矩阵的初始化器，用于输入的线性变换。默认值：glorot_uniform。\n",
    "recurrent_initializer recurrent_kernel 权重矩阵的初始化器，用于循环状态的线性变换。默认值：orthogonal。\n",
    "bias_initializer 偏置向量的初始化器。默认值：zeros。\n",
    "kernel_regularizer 应用于kernel 权重矩阵的正则化函数。默认值：None。\n",
    "recurrent_regularizer 应用于recurrent_kernel 权重矩阵的正则化函数。默认值：None。\n",
    "bias_regularizer 应用于偏置向量的正则化函数。默认值：None。\n",
    "activity_regularizer 应用于层输出的正则化函数(\"activation\")。默认值：None。\n",
    "kernel_constraint 应用于kernel 权重矩阵的约束函数。默认值：None。\n",
    "recurrent_constraint 应用于recurrent_kernel 权重矩阵的约束函数。默认值：None。\n",
    "bias_constraint 应用于偏置向量的约束函数。默认值：None。\n",
    "dropout 在 0 和 1 之间浮点数。为输入的线性变换而下降的单位分数。默认值：0。\n",
    "recurrent_dropout 在 0 和 1 之间浮点数。用于循环状态的线性变换的单位的分数。默认值：0。\n",
    "return_sequences 布尔值。是返回输出序列中的最后一个输出，还是返回完整序列。默认值：False。\n",
    "return_state 布尔值。是否返回除了输出之外的最后一个状态。默认值：False。\n",
    "go_backwards 布尔值(默认 False )。如果为 True，则反向处理输入序列并返回反向序列。\n",
    "stateful 布尔值(默认为 False)。如果为 True，则批次中索引 i 处每个样本的最后状态将用作下一批中索引 i 的样本的初始状态。\n",
    "unroll 布尔值(默认为 False)。如果为 True，则网络将展开，否则将使用符号循环。展开可以speed-up一个RNN，虽然它往往更多memory-intensive。展开仅适用于短序列。\n",
    "time_major inputs 和 outputs 张量的形状格式。如果为 True，输入和输出的形状将是 [timesteps, batch, feature] ，而在 False 情况下，它将是 [batch, timesteps, feature] 。使用time_major = True 效率更高一些，因为它避免了 RNN 计算开始和结束时的转置。但是，大多数 TensorFlow 数据是 batch-major，因此默认情况下，此函数接受输入并以 batch-major 形式发出输出。\n",
    "reset_after GRU 约定(是否在矩阵乘法之后或之前应用重置门)。 False = \"before\"，True = \"after\"(默认和 cuDNN 兼容)。\n",
    "```\n",
    "\n",
    "- 属性\n",
    "\n",
    "```\n",
    "activation\n",
    "bias_constraint\n",
    "bias_initializer\n",
    "bias_regularizer\n",
    "dropout\n",
    "implementation\n",
    "kernel_constraint\n",
    "kernel_initializer\n",
    "kernel_regularizer\n",
    "recurrent_activation\n",
    "recurrent_constraint\n",
    "recurrent_dropout\n",
    "recurrent_initializer\n",
    "recurrent_regularizer\n",
    "reset_after\n",
    "states\n",
    "units\n",
    "use_bias\n",
    "```\n",
    "\n",
    "有关 RNN API 使用的详细信息，请参阅 Keras RNN API 指南。\n",
    "\n",
    "根据可用的运行时硬件和约束，该层将选择不同的实现(基于 cuDNN 或 pure-TensorFlow)以最大化性能。如果 GPU 可用并且该层的所有参数都满足 cuDNN 内核的要求(详见下文)，则该层将使用快速 cuDNN 实现。\n",
    "\n",
    "使用 cuDNN 实现的要求是：\n",
    "```\n",
    "activation == tanh\n",
    "recurrent_activation == sigmoid\n",
    "recurrent_dropout == 0\n",
    "unroll 是 False\n",
    "use_bias 是 True\n",
    "reset_after 是 True\n",
    "```\n",
    "输入，如果使用掩码，严格来说是right-padded。\n",
    "在最外层的上下文中启用了即刻执行。\n",
    "GRU 实现有两种变体。默认的基于 v3，并在矩阵乘法之前将重置门应用于隐藏状态。另一种是根据原件，顺序颠倒。\n",
    "\n",
    "第二个变体与 CuDNNGRU (GPU-only) 兼容，并允许在 CPU 上进行推理。因此，它对 kernel 和 recurrent_kernel 有不同的偏见。要使用此变体，请设置 reset_after=True 和 recurrent_activation='sigmoid' 。\n",
    "\n",
    "例如：\n",
    "```\n",
    "inputs = tf.random.normal([32, 10, 8])\n",
    "gru = tf.keras.layers.GRU(4)\n",
    "output = gru(inputs)\n",
    "print(output.shape)\n",
    "(32, 4)\n",
    "gru = tf.keras.layers.GRU(4, return_sequences=True, return_state=True)\n",
    "whole_sequence_output, final_state = gru(inputs)\n",
    "print(whole_sequence_output.shape)\n",
    "(32, 10, 4)\n",
    "print(final_state.shape)\n",
    "(32, 4)\n",
    "```\n",
    "\n",
    "调用参数：\n",
    "inputs:一个 3D 张量，有形状[batch, timesteps, feature].\n",
    "mask: 形状的二进制张量[samples, timesteps]指示是否应屏蔽给定时间步(可选，默认为None)。个人True条目指示应使用相应的时间步长，而Falseentry 表示应该忽略相应的时间步长。\n",
    "training:Python 布尔值，指示层应该在训练模式还是推理模式下运行。此参数在调用时传递给单元格。这仅在以下情况下才相关dropout或者recurrent_dropout使用(可选，默认为None)。\n",
    "initial_state：要传递给单元格第一次调用的初始状态张量列表(可选，默认为None这会导致创建zero-filled 初始状态张量)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = keras.layers.GRU(units, return_sequences=True, return_state=True,\n",
    "                                       recurrent_initializer=\"glorot_uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on GRU in module keras.layers.recurrent_v2 object:\n",
      "\n",
      "class GRU(keras.layers.recurrent.DropoutRNNCellMixin, keras.layers.recurrent.GRU)\n",
      " |  Gated Recurrent Unit - Cho et al. 2014.\n",
      " |  \n",
      " |  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n",
      " |  for details about the usage of RNN API.\n",
      " |  \n",
      " |  Based on available runtime hardware and constraints, this layer\n",
      " |  will choose different implementations (cuDNN-based or pure-TensorFlow)\n",
      " |  to maximize the performance. If a GPU is available and all\n",
      " |  the arguments to the layer meet the requirement of the CuDNN kernel\n",
      " |  (see below for details), the layer will use a fast cuDNN implementation.\n",
      " |  \n",
      " |  The requirements to use the cuDNN implementation are:\n",
      " |  \n",
      " |  1. `activation` == `tanh`\n",
      " |  2. `recurrent_activation` == `sigmoid`\n",
      " |  3. `recurrent_dropout` == 0\n",
      " |  4. `unroll` is `False`\n",
      " |  5. `use_bias` is `True`\n",
      " |  6. `reset_after` is `True`\n",
      " |  7. Inputs, if use masking, are strictly right-padded.\n",
      " |  8. Eager execution is enabled in the outermost context.\n",
      " |  \n",
      " |  There are two variants of the GRU implementation. The default one is based on\n",
      " |  [v3](https://arxiv.org/abs/1406.1078v3) and has reset gate applied to hidden\n",
      " |  state before matrix multiplication. The other one is based on\n",
      " |  [original](https://arxiv.org/abs/1406.1078v1) and has the order reversed.\n",
      " |  \n",
      " |  The second variant is compatible with CuDNNGRU (GPU-only) and allows\n",
      " |  inference on CPU. Thus it has separate biases for `kernel` and\n",
      " |  `recurrent_kernel`. To use this variant, set `'reset_after'=True` and\n",
      " |  `recurrent_activation='sigmoid'`.\n",
      " |  \n",
      " |  For example:\n",
      " |  \n",
      " |  >>> inputs = tf.random.normal([32, 10, 8])\n",
      " |  >>> gru = tf.keras.layers.GRU(4)\n",
      " |  >>> output = gru(inputs)\n",
      " |  >>> print(output.shape)\n",
      " |  (32, 4)\n",
      " |  >>> gru = tf.keras.layers.GRU(4, return_sequences=True, return_state=True)\n",
      " |  >>> whole_sequence_output, final_state = gru(inputs)\n",
      " |  >>> print(whole_sequence_output.shape)\n",
      " |  (32, 10, 4)\n",
      " |  >>> print(final_state.shape)\n",
      " |  (32, 4)\n",
      " |  \n",
      " |  Args:\n",
      " |    units: Positive integer, dimensionality of the output space.\n",
      " |    activation: Activation function to use.\n",
      " |      Default: hyperbolic tangent (`tanh`).\n",
      " |      If you pass `None`, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    recurrent_activation: Activation function to use\n",
      " |      for the recurrent step.\n",
      " |      Default: sigmoid (`sigmoid`).\n",
      " |      If you pass `None`, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix,\n",
      " |      used for the linear transformation of the inputs. Default:\n",
      " |      `glorot_uniform`.\n",
      " |    recurrent_initializer: Initializer for the `recurrent_kernel`\n",
      " |       weights matrix, used for the linear transformation of the recurrent\n",
      " |       state. Default: `orthogonal`.\n",
      " |    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n",
      " |    kernel_regularizer: Regularizer function applied to the `kernel` weights\n",
      " |      matrix. Default: `None`.\n",
      " |    recurrent_regularizer: Regularizer function applied to the\n",
      " |      `recurrent_kernel` weights matrix. Default: `None`.\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector. Default:\n",
      " |      `None`.\n",
      " |    activity_regularizer: Regularizer function applied to the output of the\n",
      " |      layer (its \"activation\"). Default: `None`.\n",
      " |    kernel_constraint: Constraint function applied to the `kernel` weights\n",
      " |      matrix. Default: `None`.\n",
      " |    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n",
      " |      weights matrix. Default: `None`.\n",
      " |    bias_constraint: Constraint function applied to the bias vector. Default:\n",
      " |      `None`.\n",
      " |    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n",
      " |      transformation of the inputs. Default: 0.\n",
      " |    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n",
      " |      the linear transformation of the recurrent state. Default: 0.\n",
      " |    return_sequences: Boolean. Whether to return the last output\n",
      " |      in the output sequence, or the full sequence. Default: `False`.\n",
      " |    return_state: Boolean. Whether to return the last state in addition to the\n",
      " |      output. Default: `False`.\n",
      " |    go_backwards: Boolean (default `False`).\n",
      " |      If True, process the input sequence backwards and return the\n",
      " |      reversed sequence.\n",
      " |    stateful: Boolean (default False). If True, the last state\n",
      " |      for each sample at index i in a batch will be used as initial\n",
      " |      state for the sample of index i in the following batch.\n",
      " |    unroll: Boolean (default False).\n",
      " |      If True, the network will be unrolled,\n",
      " |      else a symbolic loop will be used.\n",
      " |      Unrolling can speed-up a RNN,\n",
      " |      although it tends to be more memory-intensive.\n",
      " |      Unrolling is only suitable for short sequences.\n",
      " |    time_major: The shape format of the `inputs` and `outputs` tensors.\n",
      " |      If True, the inputs and outputs will be in shape\n",
      " |      `[timesteps, batch, feature]`, whereas in the False case, it will be\n",
      " |      `[batch, timesteps, feature]`. Using `time_major = True` is a bit more\n",
      " |      efficient because it avoids transposes at the beginning and end of the\n",
      " |      RNN calculation. However, most TensorFlow data is batch-major, so by\n",
      " |      default this function accepts input and emits output in batch-major\n",
      " |      form.\n",
      " |    reset_after: GRU convention (whether to apply reset gate after or\n",
      " |      before matrix multiplication). False = \"before\",\n",
      " |      True = \"after\" (default and CuDNN compatible).\n",
      " |  \n",
      " |  Call arguments:\n",
      " |    inputs: A 3D tensor, with shape `[batch, timesteps, feature]`.\n",
      " |    mask: Binary tensor of shape `[samples, timesteps]` indicating whether\n",
      " |      a given timestep should be masked  (optional, defaults to `None`).\n",
      " |      An individual `True` entry indicates that the corresponding timestep\n",
      " |      should be utilized, while a `False` entry indicates that the\n",
      " |      corresponding timestep should be ignored.\n",
      " |    training: Python boolean indicating whether the layer should behave in\n",
      " |      training mode or in inference mode. This argument is passed to the cell\n",
      " |      when calling it. This is only relevant if `dropout` or\n",
      " |      `recurrent_dropout` is used  (optional, defaults to `None`).\n",
      " |    initial_state: List of initial state tensors to be passed to the first\n",
      " |      call of the cell  (optional, defaults to `None` which causes creation\n",
      " |      of zero-filled initial state tensors).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GRU\n",
      " |      keras.layers.recurrent.DropoutRNNCellMixin\n",
      " |      keras.layers.recurrent.GRU\n",
      " |      keras.layers.recurrent.RNN\n",
      " |      keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, time_major=False, reset_after=True, **kwargs)\n",
      " |  \n",
      " |  call(self, inputs, mask=None, training=None, initial_state=None)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Note here that `call()` method in `tf.keras` is little bit different\n",
      " |      from `keras` API. In `keras` API, you can pass support masking for\n",
      " |      layers as additional arguments. Whereas `tf.keras` has `compute_mask()`\n",
      " |      method to support masking.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor, or dict/list/tuple of input tensors.\n",
      " |          The first positional `inputs` argument is subject to special rules:\n",
      " |          - `inputs` must be explicitly passed. A layer cannot have zero\n",
      " |            arguments, and `inputs` cannot be provided via the default value\n",
      " |            of a keyword argument.\n",
      " |          - NumPy array or Python scalar values in `inputs` get cast as tensors.\n",
      " |          - Keras mask metadata is only collected from `inputs`.\n",
      " |          - Layers are built (`build(input_shape)` method)\n",
      " |            using shape info from `inputs` only.\n",
      " |          - `input_spec` compatibility is only checked against `inputs`.\n",
      " |          - Mixed precision input casting is only applied to `inputs`.\n",
      " |            If a layer has tensor arguments in `*args` or `**kwargs`, their\n",
      " |            casting behavior in mixed precision should be handled manually.\n",
      " |          - The SavedModel input specification is generated using `inputs` only.\n",
      " |          - Integration with various ecosystem packages like TFMOT, TFLite,\n",
      " |            TF.js, etc is only supported for `inputs` and not for tensors in\n",
      " |            positional and keyword arguments.\n",
      " |        *args: Additional positional arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |        **kwargs: Additional keyword arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |          The following optional keyword arguments are reserved:\n",
      " |          - `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          - `mask`: Boolean input mask. If the layer's `call()` method takes a\n",
      " |            `mask` argument, its default value will be set to the mask generated\n",
      " |            for `inputs` by the previous layer (if `input` did come from a layer\n",
      " |            that generated a corresponding mask, i.e. if it came from a Keras\n",
      " |            layer with masking support).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.layers.recurrent.DropoutRNNCellMixin:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_dropout_mask_for_cell(self, inputs, training, count=1)\n",
      " |      Get the dropout mask for RNN cell's input.\n",
      " |      \n",
      " |      It will create mask based on context if there isn't any existing cached\n",
      " |      mask. If a new mask is generated, it will update the cache in the cell.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: The input tensor whose shape will be used to generate dropout\n",
      " |          mask.\n",
      " |        training: Boolean tensor, whether its in training mode, dropout will be\n",
      " |          ignored in non-training mode.\n",
      " |        count: Int, how many dropout mask will be generated. It is useful for cell\n",
      " |          that has internal weights fused together.\n",
      " |      Returns:\n",
      " |        List of mask tensor, generated or cached mask based on context.\n",
      " |  \n",
      " |  get_recurrent_dropout_mask_for_cell(self, inputs, training, count=1)\n",
      " |      Get the recurrent dropout mask for RNN cell.\n",
      " |      \n",
      " |      It will create mask based on context if there isn't any existing cached\n",
      " |      mask. If a new mask is generated, it will update the cache in the cell.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: The input tensor whose shape will be used to generate dropout\n",
      " |          mask.\n",
      " |        training: Boolean tensor, whether its in training mode, dropout will be\n",
      " |          ignored in non-training mode.\n",
      " |        count: Int, how many dropout mask will be generated. It is useful for cell\n",
      " |          that has internal weights fused together.\n",
      " |      Returns:\n",
      " |        List of mask tensor, generated or cached mask based on context.\n",
      " |  \n",
      " |  reset_dropout_mask(self)\n",
      " |      Reset the cached dropout masks if any.\n",
      " |      \n",
      " |      This is important for the RNN layer to invoke this in it `call()` method so\n",
      " |      that the cached mask is cleared before calling the `cell.call()`. The mask\n",
      " |      should be cached across the timestep within the same batch, but shouldn't\n",
      " |      be cached between batches. Otherwise it will introduce unreasonable bias\n",
      " |      against certain index of data within the batch.\n",
      " |  \n",
      " |  reset_recurrent_dropout_mask(self)\n",
      " |      Reset the cached recurrent dropout masks if any.\n",
      " |      \n",
      " |      This is important for the RNN layer to invoke this in it call() method so\n",
      " |      that the cached mask is cleared before calling the cell.call(). The mask\n",
      " |      should be cached across the timestep within the same batch, but shouldn't\n",
      " |      be cached between batches. Otherwise it will introduce unreasonable bias\n",
      " |      against certain index of data within the batch.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.layers.recurrent.DropoutRNNCellMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.layers.recurrent.GRU:\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Note that `get_config()` does not guarantee to return a fresh copy of dict\n",
      " |      every time it is called. The callers should make a copy of the returned dict\n",
      " |      if they want to modify it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.layers.recurrent.GRU:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.layers.recurrent.GRU:\n",
      " |  \n",
      " |  activation\n",
      " |  \n",
      " |  bias_constraint\n",
      " |  \n",
      " |  bias_initializer\n",
      " |  \n",
      " |  bias_regularizer\n",
      " |  \n",
      " |  dropout\n",
      " |  \n",
      " |  implementation\n",
      " |  \n",
      " |  kernel_constraint\n",
      " |  \n",
      " |  kernel_initializer\n",
      " |  \n",
      " |  kernel_regularizer\n",
      " |  \n",
      " |  recurrent_activation\n",
      " |  \n",
      " |  recurrent_constraint\n",
      " |  \n",
      " |  recurrent_dropout\n",
      " |  \n",
      " |  recurrent_initializer\n",
      " |  \n",
      " |  recurrent_regularizer\n",
      " |  \n",
      " |  reset_after\n",
      " |  \n",
      " |  units\n",
      " |  \n",
      " |  use_bias\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.layers.recurrent.RNN:\n",
      " |  \n",
      " |  __call__(self, inputs, initial_state=None, constants=None, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |        - If the layer is not built, the method will call `build`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Args:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_initial_state(self, inputs)\n",
      " |  \n",
      " |  reset_states(self, states=None)\n",
      " |      Reset the recorded states for the stateful RNN layer.\n",
      " |      \n",
      " |      Can only be used when RNN layer is constructed with `stateful` = `True`.\n",
      " |      Args:\n",
      " |        states: Numpy arrays that contains the value for the initial state, which\n",
      " |          will be feed to cell at the first time step. When the value is None,\n",
      " |          zero filled numpy array will be created based on the cell state size.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: When the RNN layer is not stateful.\n",
      " |        ValueError: When the batch size of the RNN layer is unknown.\n",
      " |        ValueError: When the input numpy array is not compatible with the RNN\n",
      " |          layer state, either size wise or dtype wise.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.layers.recurrent.RNN:\n",
      " |  \n",
      " |  states\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |            inputs - Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(inputs))\n",
      " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This is\n",
      " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result of\n",
      " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
      " |          using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Args:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The variable created.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  finalize_state(self)\n",
      " |      Finalizes the layers state after updating layer weights.\n",
      " |      \n",
      " |      This function can be subclassed in a layer and will be called after updating\n",
      " |      a layer weights. It can be overridden to finalize any additional layer state\n",
      " |      after a weight update.\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first input node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first output node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer, as NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of NumPy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of NumPy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function, by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Args:\n",
      " |        weights: a list of NumPy arrays. The number\n",
      " |          of arrays and their shape must match\n",
      " |          number of the dimensions of the weights\n",
      " |          of the layer (i.e. it should match the\n",
      " |          output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the provided weights list does not match the\n",
      " |          layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the layer's computations.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
      " |      the weights.\n",
      " |      \n",
      " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
      " |      computations and the output to be in the compute dtype as well. This is done\n",
      " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
      " |      these casts if implementing your own layer.\n",
      " |      \n",
      " |      Layers often perform certain internal computations in higher precision when\n",
      " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
      " |      will still typically be float16 or bfloat16 in such cases.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The layer's compute dtype.\n",
      " |  \n",
      " |  dtype\n",
      " |      The dtype of the layer weights.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
      " |      dtype of the layer's computations.\n",
      " |  \n",
      " |  dtype_policy\n",
      " |      The dtype policy associated with this layer.\n",
      " |      \n",
      " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> len(model.losses)\n",
      " |      0\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> len(model.losses)\n",
      " |      1\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of `Metric` objects.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
      " |      themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2, 10), dtype=float32, numpy=\n",
       "array([[[-0.04455454,  0.04463908,  0.04506776,  0.02344776,\n",
       "         -0.04919156, -0.04341774, -0.0441567 ,  0.01678076,\n",
       "         -0.01249163,  0.0438653 ],\n",
       "        [-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007]],\n",
       "\n",
       "       [[-0.04455454,  0.04463908,  0.04506776,  0.02344776,\n",
       "         -0.04919156, -0.04341774, -0.0441567 ,  0.01678076,\n",
       "         -0.01249163,  0.0438653 ],\n",
       "        [ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "         -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "         -0.01525488, -0.04575637]],\n",
       "\n",
       "       [[-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "          0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "          0.01806841,  0.03489599],\n",
       "        [-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "          0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "          0.01806841,  0.03489599]],\n",
       "\n",
       "       [[-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007],\n",
       "        [-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007]],\n",
       "\n",
       "       [[ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "         -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "         -0.03513166,  0.03322264],\n",
       "        [ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461]],\n",
       "\n",
       "       [[-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007],\n",
       "        [ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "         -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "         -0.01525488, -0.04575637]],\n",
       "\n",
       "       [[ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803],\n",
       "        [-0.04455454,  0.04463908,  0.04506776,  0.02344776,\n",
       "         -0.04919156, -0.04341774, -0.0441567 ,  0.01678076,\n",
       "         -0.01249163,  0.0438653 ]],\n",
       "\n",
       "       [[-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007],\n",
       "        [-0.02563901, -0.01560853,  0.012044  ,  0.01931993,\n",
       "         -0.03744175,  0.03179225, -0.04118588, -0.01800214,\n",
       "          0.01870343,  0.01538343]],\n",
       "\n",
       "       [[-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[-0.04455454,  0.04463908,  0.04506776,  0.02344776,\n",
       "         -0.04919156, -0.04341774, -0.0441567 ,  0.01678076,\n",
       "         -0.01249163,  0.0438653 ],\n",
       "        [-0.02563901, -0.01560853,  0.012044  ,  0.01931993,\n",
       "         -0.03744175,  0.03179225, -0.04118588, -0.01800214,\n",
       "          0.01870343,  0.01538343]],\n",
       "\n",
       "       [[-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "         -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "          0.01271724, -0.02674476],\n",
       "        [-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "         -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "          0.01271724, -0.02674476]],\n",
       "\n",
       "       [[ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803],\n",
       "        [-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007]],\n",
       "\n",
       "       [[-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "         -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "         -0.01439886, -0.01674361],\n",
       "        [-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "         -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "          0.01271724, -0.02674476]],\n",
       "\n",
       "       [[-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "         -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "          0.01271724, -0.02674476],\n",
       "        [ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461]],\n",
       "\n",
       "       [[-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "         -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "         -0.01439886, -0.01674361],\n",
       "        [-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "         -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "         -0.03513166,  0.03322264]],\n",
       "\n",
       "       [[-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "          0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "          0.01806841,  0.03489599],\n",
       "        [ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "         -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "         -0.01525488, -0.04575637]],\n",
       "\n",
       "       [[-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007],\n",
       "        [-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "         -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "          0.01271724, -0.02674476]],\n",
       "\n",
       "       [[-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "         -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "          0.01271724, -0.02674476],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461],\n",
       "        [-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "          0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "          0.01806841,  0.03489599]],\n",
       "\n",
       "       [[-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "         -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "         -0.01439886, -0.01674361],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[-0.02563901, -0.01560853,  0.012044  ,  0.01931993,\n",
       "         -0.03744175,  0.03179225, -0.04118588, -0.01800214,\n",
       "          0.01870343,  0.01538343],\n",
       "        [ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "         -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "         -0.01525488, -0.04575637]],\n",
       "\n",
       "       [[-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "          0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "          0.01806841,  0.03489599],\n",
       "        [-0.02563901, -0.01560853,  0.012044  ,  0.01931993,\n",
       "         -0.03744175,  0.03179225, -0.04118588, -0.01800214,\n",
       "          0.01870343,  0.01538343]],\n",
       "\n",
       "       [[ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461],\n",
       "        [-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "         -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "          0.01271724, -0.02674476]],\n",
       "\n",
       "       [[ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803],\n",
       "        [-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "          0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "          0.04889529, -0.02774007]],\n",
       "\n",
       "       [[ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "         -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "         -0.01439886, -0.01674361],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[-0.04455454,  0.04463908,  0.04506776,  0.02344776,\n",
       "         -0.04919156, -0.04341774, -0.0441567 ,  0.01678076,\n",
       "         -0.01249163,  0.0438653 ],\n",
       "        [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "          0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "          0.0393439 ,  0.04182803]],\n",
       "\n",
       "       [[ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461],\n",
       "        [-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "         -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "         -0.01439886, -0.01674361]],\n",
       "\n",
       "       [[-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "         -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "         -0.03513166,  0.03322264],\n",
       "        [ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "          0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "         -0.01851915,  0.01992461]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_x =embed(test_x)\n",
    "embed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_hidden = tf.zeros((batch_size, units))\n",
    "init_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_x, state_x=gru(embed_x,init_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2, 32), dtype=float32, numpy=\n",
       "array([[[-8.78948532e-03,  1.13781048e-02,  7.73639930e-03, ...,\n",
       "         -2.38645985e-03,  3.35185975e-03, -8.87926482e-03],\n",
       "        [-8.53142608e-03, -2.90199416e-03,  6.60363911e-03, ...,\n",
       "         -5.99072827e-03,  6.21979198e-05, -1.53577607e-02]],\n",
       "\n",
       "       [[-8.78948532e-03,  1.13781048e-02,  7.73639930e-03, ...,\n",
       "         -2.38645985e-03,  3.35185975e-03, -8.87926482e-03],\n",
       "        [-4.27575875e-03,  1.69663895e-02,  3.32427351e-03, ...,\n",
       "         -5.13378670e-03,  5.40198432e-03,  3.86977615e-03]],\n",
       "\n",
       "       [[-1.13754757e-02, -7.46724149e-03, -4.30806016e-04, ...,\n",
       "         -2.78959423e-03,  4.35484726e-05, -7.74714025e-03],\n",
       "        [-1.71657223e-02, -1.16010411e-02, -7.78712129e-05, ...,\n",
       "         -3.85822030e-03, -1.23892841e-03, -1.20907454e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-8.78948532e-03,  1.13781048e-02,  7.73639930e-03, ...,\n",
       "         -2.38645985e-03,  3.35185975e-03, -8.87926482e-03],\n",
       "        [-4.98111267e-03,  9.80127021e-04,  7.48883816e-04, ...,\n",
       "          1.01228440e-02,  4.80283331e-03, -4.67822142e-03]],\n",
       "\n",
       "       [[ 8.38570239e-04,  3.19815241e-03, -2.38520605e-03, ...,\n",
       "          1.43382838e-03, -3.59463575e-03,  6.11808198e-03],\n",
       "        [ 3.57209658e-03,  7.32741039e-03, -3.10522295e-03, ...,\n",
       "         -7.78462272e-03, -2.90061347e-03,  7.98213482e-03]],\n",
       "\n",
       "       [[-1.13457046e-03,  1.00557962e-02,  3.22373613e-04, ...,\n",
       "         -9.68642067e-04,  2.57615536e-03,  4.49080206e-03],\n",
       "        [ 1.01556834e-04,  8.98117479e-03, -2.51522078e-03, ...,\n",
       "          1.32623105e-03, -2.10925913e-03,  9.06594470e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
       "array([[-8.53142608e-03, -2.90199416e-03,  6.60363911e-03, ...,\n",
       "        -5.99072827e-03,  6.21979198e-05, -1.53577607e-02],\n",
       "       [-4.27575875e-03,  1.69663895e-02,  3.32427351e-03, ...,\n",
       "        -5.13378670e-03,  5.40198432e-03,  3.86977615e-03],\n",
       "       [-1.71657223e-02, -1.16010411e-02, -7.78712129e-05, ...,\n",
       "        -3.85822030e-03, -1.23892841e-03, -1.20907454e-02],\n",
       "       ...,\n",
       "       [-4.98111267e-03,  9.80127021e-04,  7.48883816e-04, ...,\n",
       "         1.01228440e-02,  4.80283331e-03, -4.67822142e-03],\n",
       "       [ 3.57209658e-03,  7.32741039e-03, -3.10522295e-03, ...,\n",
       "        -7.78462272e-03, -2.90061347e-03,  7.98213482e-03],\n",
       "       [ 1.01556834e-04,  8.98117479e-03, -2.51522078e-03, ...,\n",
       "         1.32623105e-03, -2.10925913e-03,  9.06594470e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.expand_dims()函数用于给函数增加维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "tf.expand_dims()函数用于给函数增加维度。\n",
    "tf.expand_dims(\n",
    "    input,\n",
    "    axis=None,\n",
    "    name=None,\n",
    "    dim=None\n",
    ")\n",
    "```\n",
    "参数：\n",
    "```\n",
    "input是输入张量。\n",
    "axis是指定扩大输入张量形状的维度索引值。\n",
    "dim等同于轴，一般不推荐使用。\n",
    "```\n",
    "函数的功能是在给定一个input时，在axis轴处给input增加一个维度。\n",
    "\n",
    "- axis：\n",
    "\n",
    "给定张量输入input，此操作为选择维度索引值，在输入形状的维度索引值的轴处插入1的维度。 维度索引值的轴从零开始; 如果您指定轴是负数，则从最后向后进行计数，也就是倒数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 1), dtype=int32, numpy=\n",
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y=tf.expand_dims([0]*batch_size,1)\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = keras.layers.Dense(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x1bc975c1c50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_y = [tf.zeros((1, units))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 1, 32), dtype=float32, numpy=\n",
       "array([[[-8.53142608e-03, -2.90199416e-03,  6.60363911e-03, ...,\n",
       "         -5.99072827e-03,  6.21979198e-05, -1.53577607e-02]],\n",
       "\n",
       "       [[-4.27575875e-03,  1.69663895e-02,  3.32427351e-03, ...,\n",
       "         -5.13378670e-03,  5.40198432e-03,  3.86977615e-03]],\n",
       "\n",
       "       [[-1.71657223e-02, -1.16010411e-02, -7.78712129e-05, ...,\n",
       "         -3.85822030e-03, -1.23892841e-03, -1.20907454e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-4.98111267e-03,  9.80127021e-04,  7.48883816e-04, ...,\n",
       "          1.01228440e-02,  4.80283331e-03, -4.67822142e-03]],\n",
       "\n",
       "       [[ 3.57209658e-03,  7.32741039e-03, -3.10522295e-03, ...,\n",
       "         -7.78462272e-03, -2.90061347e-03,  7.98213482e-03]],\n",
       "\n",
       "       [[ 1.01556834e-04,  8.98117479e-03, -2.51522078e-03, ...,\n",
       "          1.32623105e-03, -2.10925913e-03,  9.06594470e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(state_x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = keras.layers.Dense(units)\n",
    "W2 = keras.layers.Dense(units)\n",
    "V = keras.layers.Dense(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 1, 2, 10), dtype=float32, numpy=\n",
       "array([[[[-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "          -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "          -0.01439886, -0.01674361],\n",
       "         [-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "          -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "          -0.01439886, -0.01674361]]],\n",
       "\n",
       "\n",
       "       [[[-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "          -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "           0.01271724, -0.02674476],\n",
       "         [-0.02563901, -0.01560853,  0.012044  ,  0.01931993,\n",
       "          -0.03744175,  0.03179225, -0.04118588, -0.01800214,\n",
       "           0.01870343,  0.01538343]]],\n",
       "\n",
       "\n",
       "       [[[-0.04455454,  0.04463908,  0.04506776,  0.02344776,\n",
       "          -0.04919156, -0.04341774, -0.0441567 ,  0.01678076,\n",
       "          -0.01249163,  0.0438653 ],\n",
       "         [ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "           0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "          -0.01851915,  0.01992461]]],\n",
       "\n",
       "\n",
       "       [[[-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "          -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "          -0.03513166,  0.03322264],\n",
       "         [-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "          -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "          -0.01439886, -0.01674361]]],\n",
       "\n",
       "\n",
       "       [[[-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "           0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "           0.04889529, -0.02774007],\n",
       "         [-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "          -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "          -0.03513166,  0.03322264]]],\n",
       "\n",
       "\n",
       "       [[[-0.04455454,  0.04463908,  0.04506776,  0.02344776,\n",
       "          -0.04919156, -0.04341774, -0.0441567 ,  0.01678076,\n",
       "          -0.01249163,  0.0438653 ],\n",
       "         [-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "           0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "           0.04889529, -0.02774007]]],\n",
       "\n",
       "\n",
       "       [[[-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "           0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "           0.01806841,  0.03489599],\n",
       "         [ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "           0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "          -0.01851915,  0.01992461]]],\n",
       "\n",
       "\n",
       "       [[[ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "          -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "          -0.01525488, -0.04575637],\n",
       "         [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "           0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "           0.0393439 ,  0.04182803]]],\n",
       "\n",
       "\n",
       "       [[[-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "          -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "          -0.01439886, -0.01674361],\n",
       "         [-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "           0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "           0.01806841,  0.03489599]]],\n",
       "\n",
       "\n",
       "       [[[ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "          -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "          -0.01525488, -0.04575637],\n",
       "         [-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "          -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "          -0.01439886, -0.01674361]]],\n",
       "\n",
       "\n",
       "       [[[-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "          -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "          -0.03513166,  0.03322264],\n",
       "         [-0.02563901, -0.01560853,  0.012044  ,  0.01931993,\n",
       "          -0.03744175,  0.03179225, -0.04118588, -0.01800214,\n",
       "           0.01870343,  0.01538343]]],\n",
       "\n",
       "\n",
       "       [[[-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "          -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "          -0.01439886, -0.01674361],\n",
       "         [-0.02563901, -0.01560853,  0.012044  ,  0.01931993,\n",
       "          -0.03744175,  0.03179225, -0.04118588, -0.01800214,\n",
       "           0.01870343,  0.01538343]]],\n",
       "\n",
       "\n",
       "       [[[ 0.00118176, -0.00458641, -0.00565982, -0.04272276,\n",
       "           0.03841951, -0.04602567, -0.00741174, -0.01831134,\n",
       "          -0.01851915,  0.01992461],\n",
       "         [-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "          -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "           0.01271724, -0.02674476]]],\n",
       "\n",
       "\n",
       "       [[[-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "           0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "           0.04889529, -0.02774007],\n",
       "         [-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "          -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "          -0.01439886, -0.01674361]]],\n",
       "\n",
       "\n",
       "       [[[-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "          -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "          -0.03513166,  0.03322264],\n",
       "         [-0.02563901, -0.01560853,  0.012044  ,  0.01931993,\n",
       "          -0.03744175,  0.03179225, -0.04118588, -0.01800214,\n",
       "           0.01870343,  0.01538343]]],\n",
       "\n",
       "\n",
       "       [[[ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "           0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "           0.0393439 ,  0.04182803],\n",
       "         [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "           0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "           0.0393439 ,  0.04182803]]],\n",
       "\n",
       "\n",
       "       [[[-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "          -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "           0.01271724, -0.02674476],\n",
       "         [-0.04455454,  0.04463908,  0.04506776,  0.02344776,\n",
       "          -0.04919156, -0.04341774, -0.0441567 ,  0.01678076,\n",
       "          -0.01249163,  0.0438653 ]]],\n",
       "\n",
       "\n",
       "       [[[-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "          -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "          -0.01439886, -0.01674361],\n",
       "         [-0.02563901, -0.01560853,  0.012044  ,  0.01931993,\n",
       "          -0.03744175,  0.03179225, -0.04118588, -0.01800214,\n",
       "           0.01870343,  0.01538343]]],\n",
       "\n",
       "\n",
       "       [[[-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "          -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "          -0.03513166,  0.03322264],\n",
       "         [-0.0176981 , -0.02319922,  0.01458457, -0.02064159,\n",
       "           0.01132836,  0.00938264, -0.04608802, -0.01562606,\n",
       "           0.04889529, -0.02774007]]],\n",
       "\n",
       "\n",
       "       [[[ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "          -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "          -0.01525488, -0.04575637],\n",
       "         [-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "           0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "           0.01806841,  0.03489599]]],\n",
       "\n",
       "\n",
       "       [[[-0.04455454,  0.04463908,  0.04506776,  0.02344776,\n",
       "          -0.04919156, -0.04341774, -0.0441567 ,  0.01678076,\n",
       "          -0.01249163,  0.0438653 ],\n",
       "         [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "           0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "           0.0393439 ,  0.04182803]]],\n",
       "\n",
       "\n",
       "       [[[-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "           0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "           0.01806841,  0.03489599],\n",
       "         [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "           0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "           0.0393439 ,  0.04182803]]],\n",
       "\n",
       "\n",
       "       [[[-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "          -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "          -0.03513166,  0.03322264],\n",
       "         [ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "          -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "          -0.01525488, -0.04575637]]],\n",
       "\n",
       "\n",
       "       [[[-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "          -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "          -0.03513166,  0.03322264],\n",
       "         [-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "          -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "           0.01271724, -0.02674476]]],\n",
       "\n",
       "\n",
       "       [[[-0.02563901, -0.01560853,  0.012044  ,  0.01931993,\n",
       "          -0.03744175,  0.03179225, -0.04118588, -0.01800214,\n",
       "           0.01870343,  0.01538343],\n",
       "         [-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "          -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "          -0.03513166,  0.03322264]]],\n",
       "\n",
       "\n",
       "       [[[-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "          -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "          -0.03513166,  0.03322264],\n",
       "         [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "           0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "           0.0393439 ,  0.04182803]]],\n",
       "\n",
       "\n",
       "       [[[-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "           0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "           0.01806841,  0.03489599],\n",
       "         [-0.00940143,  0.04283244,  0.02311685,  0.03249437,\n",
       "          -0.01870603, -0.03154739,  0.03749646, -0.01486159,\n",
       "          -0.03513166,  0.03322264]]],\n",
       "\n",
       "\n",
       "       [[[-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "          -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "          -0.01439886, -0.01674361],\n",
       "         [ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "          -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "          -0.01525488, -0.04575637]]],\n",
       "\n",
       "\n",
       "       [[[-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "           0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "           0.01806841,  0.03489599],\n",
       "         [-0.00582673,  0.01700796, -0.0066212 , -0.03161768,\n",
       "          -0.01707266,  0.03116467,  0.01597368, -0.01423205,\n",
       "          -0.01439886, -0.01674361]]],\n",
       "\n",
       "\n",
       "       [[[-0.04455454,  0.04463908,  0.04506776,  0.02344776,\n",
       "          -0.04919156, -0.04341774, -0.0441567 ,  0.01678076,\n",
       "          -0.01249163,  0.0438653 ],\n",
       "         [ 0.01003499,  0.0396397 , -0.02368988, -0.0179203 ,\n",
       "          -0.02464662,  0.04157648,  0.00130557,  0.04739784,\n",
       "          -0.01525488, -0.04575637]]],\n",
       "\n",
       "\n",
       "       [[[-0.00401158,  0.02691047, -0.04453013, -0.03804395,\n",
       "          -0.04162438, -0.01433868, -0.04531867,  0.02753348,\n",
       "           0.01271724, -0.02674476],\n",
       "         [-0.02027046,  0.04145514,  0.00930665,  0.03462258,\n",
       "           0.03842961,  0.00826417, -0.04229715, -0.02322576,\n",
       "           0.01806841,  0.03489599]]],\n",
       "\n",
       "\n",
       "       [[[ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "           0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "           0.0393439 ,  0.04182803],\n",
       "         [ 0.04463856,  0.01311718,  0.01403349,  0.04510291,\n",
       "           0.00636585, -0.0237892 , -0.02649209,  0.01983148,\n",
       "           0.0393439 ,  0.04182803]]]], dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2, 32), dtype=float32, numpy=\n",
       "array([[[ 0.01029635,  0.00148423,  0.00549972, ...,  0.00598448,\n",
       "         -0.01493043,  0.00105774],\n",
       "        [ 0.00468471,  0.00159314, -0.00014553, ...,  0.00482081,\n",
       "         -0.01243681, -0.00836829]],\n",
       "\n",
       "       [[ 0.01029635,  0.00148423,  0.00549972, ...,  0.00598448,\n",
       "         -0.01493043,  0.00105774],\n",
       "        [ 0.01216146, -0.00312846,  0.01049282, ...,  0.00014848,\n",
       "         -0.00028515, -0.00459104]],\n",
       "\n",
       "       [[ 0.00205284,  0.00271043, -0.0028394 , ...,  0.00756163,\n",
       "         -0.00707033, -0.00189234],\n",
       "        [ 0.00357856,  0.00420283, -0.00433349, ...,  0.01169347,\n",
       "         -0.01025148, -0.00278315]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.01029635,  0.00148423,  0.00549972, ...,  0.00598448,\n",
       "         -0.01493043,  0.00105774],\n",
       "        [-0.00487379, -0.00513739,  0.0009195 , ...,  0.00571866,\n",
       "         -0.0025768 , -0.00019166]],\n",
       "\n",
       "       [[ 0.00080834,  0.00201335,  0.00551351, ..., -0.01267827,\n",
       "          0.00227052,  0.00325649],\n",
       "        [ 0.00753825,  0.0047963 ,  0.0052021 , ..., -0.0114388 ,\n",
       "          0.00029005, -0.00036635]],\n",
       "\n",
       "       [[ 0.00659507,  0.00364232,  0.00299507, ...,  0.00513964,\n",
       "         -0.00247541,  0.00533355],\n",
       "        [ 0.0052113 ,  0.00365361,  0.00801666, ..., -0.01181648,\n",
       "          0.00252331,  0.00549265]]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1(output_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 1, 32), dtype=float32, numpy=\n",
       "array([[[-0.00513653, -0.01047728, -0.00045647, ..., -0.00154757,\n",
       "         -0.00178081,  0.01662941]],\n",
       "\n",
       "       [[ 0.0046214 ,  0.00318803,  0.00460281, ..., -0.0096824 ,\n",
       "         -0.00314243, -0.01308868]],\n",
       "\n",
       "       [[-0.00207403, -0.01013148,  0.00273078, ...,  0.00146232,\n",
       "         -0.00381819,  0.00718357]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.01347445, -0.00428144,  0.00013697, ..., -0.00117883,\n",
       "         -0.00402178,  0.01163399]],\n",
       "\n",
       "       [[ 0.00279944,  0.00287612, -0.00194468, ..., -0.00280624,\n",
       "          0.00435776, -0.00834067]],\n",
       "\n",
       "       [[-0.00133585,  0.00107002, -0.00105409, ..., -0.00324946,\n",
       "          0.00073775, -0.00378133]]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2(tf.expand_dims(state_x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2, 32), dtype=float32, numpy=\n",
       "array([[[ 5.1598260e-03, -8.9930501e-03,  5.0432440e-03, ...,\n",
       "          4.4369092e-03, -1.6711244e-02,  1.7687155e-02],\n",
       "        [-4.5181904e-04, -8.8841394e-03, -6.0200389e-04, ...,\n",
       "          3.2732380e-03, -1.4217618e-02,  8.2611218e-03]],\n",
       "\n",
       "       [[ 5.1598260e-03, -8.9930501e-03,  5.0432440e-03, ...,\n",
       "          4.4369092e-03, -1.6711244e-02,  1.7687155e-02],\n",
       "        [ 7.0249299e-03, -1.3605742e-02,  1.0036348e-02, ...,\n",
       "         -1.3990933e-03, -2.0659592e-03,  1.2038375e-02]],\n",
       "\n",
       "       [[-3.0836826e-03, -7.7668540e-03, -3.2958761e-03, ...,\n",
       "          6.0140593e-03, -8.8511426e-03,  1.4737076e-02],\n",
       "        [-1.5579653e-03, -6.2744492e-03, -4.7899582e-03, ...,\n",
       "          1.0145903e-02, -1.2032293e-02,  1.3846265e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 5.1598260e-03, -8.9930501e-03,  5.0432440e-03, ...,\n",
       "          4.4369092e-03, -1.6711244e-02,  1.7687155e-02],\n",
       "        [-1.0010317e-02, -1.5614673e-02,  4.6302844e-04, ...,\n",
       "          4.1710925e-03, -4.3576090e-03,  1.6437758e-02]],\n",
       "\n",
       "       [[-4.3281876e-03, -8.4639275e-03,  5.0570350e-03, ...,\n",
       "         -1.4225841e-02,  4.8971281e-04,  1.9885901e-02],\n",
       "        [ 2.4017189e-03, -5.6809797e-03,  4.7456268e-03, ...,\n",
       "         -1.2986377e-02, -1.4907594e-03,  1.6263066e-02]],\n",
       "\n",
       "       [[ 1.4585420e-03, -6.8349610e-03,  2.5386014e-03, ...,\n",
       "          3.5920637e-03, -4.2562187e-03,  2.1962970e-02],\n",
       "        [ 7.4776821e-05, -6.8236664e-03,  7.5601833e-03, ...,\n",
       "         -1.3364047e-02,  7.4250123e-04,  2.2122063e-02]]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1(output_x)+W2(tf.expand_dims(state_x, 1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 32), dtype=float32, numpy=\n",
       "array([[ 0.01029635,  0.00148423,  0.00549972,  0.00289583, -0.01529934,\n",
       "        -0.00110411,  0.00214484, -0.0037445 , -0.01671661, -0.00739318,\n",
       "         0.00232342,  0.01436429,  0.00534583,  0.00994391,  0.01382414,\n",
       "         0.01220044,  0.00159514, -0.00685544,  0.00646889,  0.00529287,\n",
       "         0.01438059,  0.00312169, -0.00390364,  0.00990993,  0.01294299,\n",
       "        -0.01431926, -0.0056556 ,  0.00897159,  0.00214641,  0.00598448,\n",
       "        -0.01493043,  0.00105774],\n",
       "       [ 0.00468471,  0.00159314, -0.00014553,  0.01112385, -0.01260095,\n",
       "        -0.00549098,  0.00533292, -0.00307141,  0.00259329, -0.00415351,\n",
       "        -0.00647227,  0.00984916,  0.00214036,  0.00155237,  0.01105889,\n",
       "        -0.0082521 ,  0.00606149, -0.00575504, -0.00234239,  0.00660378,\n",
       "         0.00276902,  0.00408481,  0.00526104,  0.00867581,  0.02541972,\n",
       "        -0.00263549, -0.01172028,  0.01250698,  0.00290068,  0.00482081,\n",
       "        -0.01243681, -0.00836829]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1(output_x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
       "array([[-0.00513653, -0.01047728, -0.00045647,  0.00573935, -0.00897936,\n",
       "        -0.0152008 , -0.00637342,  0.00083789,  0.01351233,  0.0084723 ,\n",
       "         0.00138286,  0.0017098 , -0.00738968, -0.01253817,  0.00373444,\n",
       "        -0.00298303, -0.00039392, -0.01606884,  0.0044345 , -0.00147224,\n",
       "         0.002454  ,  0.00040417,  0.00048474,  0.01082856,  0.01240887,\n",
       "         0.00326927,  0.00602526,  0.0012772 , -0.00555167, -0.00154757,\n",
       "        -0.00178081,  0.01662941]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2(tf.expand_dims(state_x, 1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2, 32), dtype=float32, numpy=\n",
       "array([[[ 5.15982602e-03, -8.99305008e-03,  5.04324399e-03, ...,\n",
       "          4.43690922e-03, -1.67112444e-02,  1.76871549e-02],\n",
       "        [-4.51819040e-04, -8.88413936e-03, -6.02003885e-04, ...,\n",
       "          3.27323796e-03, -1.42176179e-02,  8.26112181e-03]],\n",
       "\n",
       "       [[ 1.49177574e-02,  4.67226002e-03,  1.01025235e-02, ...,\n",
       "         -3.69791454e-03, -1.80728585e-02, -1.20309349e-02],\n",
       "        [ 1.67828612e-02,  5.95687889e-05,  1.50956279e-02, ...,\n",
       "         -9.53391753e-03, -3.42757348e-03, -1.76797137e-02]],\n",
       "\n",
       "       [[-2.11829320e-05, -7.42104929e-03, -1.08622946e-04, ...,\n",
       "          9.02394671e-03, -1.08885225e-02,  5.29123470e-03],\n",
       "        [ 1.50453439e-03, -5.92864444e-03, -1.60270510e-03, ...,\n",
       "          1.31557891e-02, -1.40696727e-02,  4.40042419e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.17809545e-03, -2.79720570e-03,  5.63668367e-03, ...,\n",
       "          4.80564870e-03, -1.89522132e-02,  1.26917288e-02],\n",
       "        [-1.83482394e-02, -9.41882841e-03,  1.05646811e-03, ...,\n",
       "          4.53983201e-03, -6.59857644e-03,  1.14423325e-02]],\n",
       "\n",
       "       [[ 3.60777718e-03,  4.88947844e-03,  3.56882368e-03, ...,\n",
       "         -1.54845119e-02,  6.62828470e-03, -5.08418679e-03],\n",
       "        [ 1.03376834e-02,  7.67242629e-03,  3.25741549e-03, ...,\n",
       "         -1.42450482e-02,  4.64781234e-03, -8.70702136e-03]],\n",
       "\n",
       "       [[ 5.25921676e-03,  4.71234042e-03,  1.94097916e-03, ...,\n",
       "          1.89017784e-03, -1.73765351e-03,  1.55222765e-03],\n",
       "        [ 3.87545186e-03,  4.72363550e-03,  6.96256105e-03, ...,\n",
       "         -1.50659326e-02,  3.26106627e-03,  1.71132083e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1(output_x)+W2(tf.expand_dims(state_x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2, 32), dtype=float32, numpy=\n",
       "array([[[ 5.15977945e-03, -8.99280701e-03,  5.04320068e-03, ...,\n",
       "          4.43687942e-03, -1.67096872e-02,  1.76853091e-02],\n",
       "        [-4.51818953e-04, -8.88390467e-03, -6.02003711e-04, ...,\n",
       "          3.27322609e-03, -1.42166587e-02,  8.26093275e-03]],\n",
       "\n",
       "       [[ 1.49166491e-02,  4.67222556e-03,  1.01021798e-02, ...,\n",
       "         -3.69789731e-03, -1.80708896e-02, -1.20303538e-02],\n",
       "        [ 1.67812835e-02,  5.95687889e-05,  1.50944795e-02, ...,\n",
       "         -9.53362789e-03, -3.42755998e-03, -1.76778696e-02]],\n",
       "\n",
       "       [[-2.11829320e-05, -7.42091192e-03, -1.08622946e-04, ...,\n",
       "          9.02370084e-03, -1.08880904e-02,  5.29118488e-03],\n",
       "        [ 1.50453311e-03, -5.92857460e-03, -1.60270371e-03, ...,\n",
       "          1.31550292e-02, -1.40687441e-02,  4.40039532e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.17808450e-03, -2.79719825e-03,  5.63662313e-03, ...,\n",
       "          4.80561145e-03, -1.89499427e-02,  1.26910452e-02],\n",
       "        [-1.83461793e-02, -9.41854902e-03,  1.05646765e-03, ...,\n",
       "          4.53980034e-03, -6.59848051e-03,  1.14418315e-02]],\n",
       "\n",
       "       [[ 3.60776135e-03,  4.88943933e-03,  3.56880832e-03, ...,\n",
       "         -1.54832723e-02,  6.62818691e-03, -5.08414209e-03],\n",
       "        [ 1.03373136e-02,  7.67227495e-03,  3.25740385e-03, ...,\n",
       "         -1.42440833e-02,  4.64777881e-03, -8.70680064e-03]],\n",
       "\n",
       "       [[ 5.25916740e-03,  4.71230503e-03,  1.94097648e-03, ...,\n",
       "          1.89017539e-03, -1.73765153e-03,  1.55222625e-03],\n",
       "        [ 3.87543230e-03,  4.72360011e-03,  6.96244743e-03, ...,\n",
       "         -1.50647918e-02,  3.26105440e-03,  1.71131897e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.tanh(W1(output_x)+W2(tf.expand_dims(state_x, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2, 1), dtype=float32, numpy=\n",
       "array([[[ 7.79337436e-03],\n",
       "        [-1.72917242e-03]],\n",
       "\n",
       "       [[-7.97312893e-03],\n",
       "        [-2.02629548e-02]],\n",
       "\n",
       "       [[-8.66569113e-04],\n",
       "        [ 2.18018447e-03]],\n",
       "\n",
       "       [[ 1.05049449e-03],\n",
       "        [-1.20109087e-03]],\n",
       "\n",
       "       [[ 1.00703212e-02],\n",
       "        [ 1.19689591e-02]],\n",
       "\n",
       "       [[ 1.20761711e-02],\n",
       "        [ 8.32181331e-03]],\n",
       "\n",
       "       [[-1.47455223e-02],\n",
       "        [-1.97050255e-02]],\n",
       "\n",
       "       [[ 6.47079805e-03],\n",
       "        [ 8.35466012e-03]],\n",
       "\n",
       "       [[-7.58036331e-04],\n",
       "        [ 4.37178998e-04]],\n",
       "\n",
       "       [[ 9.06376867e-04],\n",
       "        [ 5.69616072e-03]],\n",
       "\n",
       "       [[ 6.05362281e-03],\n",
       "        [-2.64034607e-05]],\n",
       "\n",
       "       [[-1.53386500e-03],\n",
       "        [-3.36065050e-03]],\n",
       "\n",
       "       [[ 5.44138858e-03],\n",
       "        [ 1.10795945e-02]],\n",
       "\n",
       "       [[ 1.01707028e-02],\n",
       "        [ 4.98454971e-03]],\n",
       "\n",
       "       [[-1.27204508e-03],\n",
       "        [-3.09423730e-03]],\n",
       "\n",
       "       [[ 2.74671428e-03],\n",
       "        [ 6.02369988e-03]],\n",
       "\n",
       "       [[-9.19098500e-03],\n",
       "        [-6.95762690e-04]],\n",
       "\n",
       "       [[-7.57751521e-03],\n",
       "        [-1.73729546e-02]],\n",
       "\n",
       "       [[ 4.13776375e-04],\n",
       "        [-4.72555449e-03]],\n",
       "\n",
       "       [[-9.48688365e-04],\n",
       "        [ 7.23344600e-03]],\n",
       "\n",
       "       [[-3.40461265e-03],\n",
       "        [ 5.20835305e-03]],\n",
       "\n",
       "       [[-7.02298479e-04],\n",
       "        [ 7.30200578e-03]],\n",
       "\n",
       "       [[-1.07287467e-02],\n",
       "        [-1.93430912e-02]],\n",
       "\n",
       "       [[ 6.37107715e-03],\n",
       "        [ 2.76489696e-03]],\n",
       "\n",
       "       [[ 1.00703212e-02],\n",
       "        [ 1.19689591e-02]],\n",
       "\n",
       "       [[ 4.91996249e-03],\n",
       "        [ 6.00880943e-04]],\n",
       "\n",
       "       [[ 1.01707028e-02],\n",
       "        [ 4.98454971e-03]],\n",
       "\n",
       "       [[ 5.44138858e-03],\n",
       "        [ 1.10795945e-02]],\n",
       "\n",
       "       [[-7.02298479e-04],\n",
       "        [ 7.30200578e-03]],\n",
       "\n",
       "       [[ 7.74340052e-03],\n",
       "        [ 5.28401230e-03]],\n",
       "\n",
       "       [[-2.42176908e-03],\n",
       "        [-5.59142511e-03]],\n",
       "\n",
       "       [[ 1.20761711e-02],\n",
       "        [ 8.32181331e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V(tf.nn.tanh(W1(output_x)+W2(tf.expand_dims(state_x, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2, 1), dtype=float32, numpy=\n",
       "array([[[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(V(tf.nn.tanh(W1(output_x)+W2(tf.expand_dims(state_x, 1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2, 1), dtype=float32, numpy=\n",
       "array([[[0.5023806 ],\n",
       "        [0.49761936]],\n",
       "\n",
       "       [[0.50307244],\n",
       "        [0.4969276 ]],\n",
       "\n",
       "       [[0.4992383 ],\n",
       "        [0.5007617 ]],\n",
       "\n",
       "       [[0.5005629 ],\n",
       "        [0.49943712]],\n",
       "\n",
       "       [[0.49952537],\n",
       "        [0.5004747 ]],\n",
       "\n",
       "       [[0.5009386 ],\n",
       "        [0.4990614 ]],\n",
       "\n",
       "       [[0.50123984],\n",
       "        [0.49876013]],\n",
       "\n",
       "       [[0.49952903],\n",
       "        [0.50047094]],\n",
       "\n",
       "       [[0.4997012 ],\n",
       "        [0.5002988 ]],\n",
       "\n",
       "       [[0.49880257],\n",
       "        [0.50119746]],\n",
       "\n",
       "       [[0.50152   ],\n",
       "        [0.49847996]],\n",
       "\n",
       "       [[0.5004567 ],\n",
       "        [0.4995433 ]],\n",
       "\n",
       "       [[0.49859047],\n",
       "        [0.50140953]],\n",
       "\n",
       "       [[0.5012965 ],\n",
       "        [0.49870345]],\n",
       "\n",
       "       [[0.50045556],\n",
       "        [0.49954444]],\n",
       "\n",
       "       [[0.49918073],\n",
       "        [0.5008192 ]],\n",
       "\n",
       "       [[0.49787623],\n",
       "        [0.5021238 ]],\n",
       "\n",
       "       [[0.50244886],\n",
       "        [0.49755117]],\n",
       "\n",
       "       [[0.50128484],\n",
       "        [0.4987152 ]],\n",
       "\n",
       "       [[0.49795446],\n",
       "        [0.5020455 ]],\n",
       "\n",
       "       [[0.49784675],\n",
       "        [0.5021532 ]],\n",
       "\n",
       "       [[0.49799892],\n",
       "        [0.50200105]],\n",
       "\n",
       "       [[0.5021536 ],\n",
       "        [0.49784642]],\n",
       "\n",
       "       [[0.5009015 ],\n",
       "        [0.49909845]],\n",
       "\n",
       "       [[0.49952537],\n",
       "        [0.5004747 ]],\n",
       "\n",
       "       [[0.5010798 ],\n",
       "        [0.49892023]],\n",
       "\n",
       "       [[0.5012965 ],\n",
       "        [0.49870345]],\n",
       "\n",
       "       [[0.49859047],\n",
       "        [0.50140953]],\n",
       "\n",
       "       [[0.49799892],\n",
       "        [0.50200105]],\n",
       "\n",
       "       [[0.5006148 ],\n",
       "        [0.49938518]],\n",
       "\n",
       "       [[0.50079244],\n",
       "        [0.4992076 ]],\n",
       "\n",
       "       [[0.5009386 ],\n",
       "        [0.4990614 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(V(tf.nn.tanh(W1(output_x)+W2(tf.expand_dims(state_x, 1)))),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2, 32), dtype=float32, numpy=\n",
       "array([[[-4.4156672e-03,  5.7161395e-03,  3.8866170e-03, ...,\n",
       "         -1.1989111e-03,  1.6839093e-03, -4.4607706e-03],\n",
       "        [-4.2454028e-03, -1.4440885e-03,  3.2860986e-03, ...,\n",
       "         -2.9811023e-03,  3.0950891e-05, -7.6423190e-03]],\n",
       "\n",
       "       [[-4.4217478e-03,  5.7240110e-03,  3.8919693e-03, ...,\n",
       "         -1.2005622e-03,  1.6862282e-03, -4.4669136e-03],\n",
       "        [-2.1247424e-03,  8.4310668e-03,  1.6519232e-03, ...,\n",
       "         -2.5511202e-03,  2.6843951e-03,  1.9229986e-03]],\n",
       "\n",
       "       [[-5.6790733e-03, -3.7279329e-03, -2.1507486e-04, ...,\n",
       "         -1.3926723e-03,  2.1741065e-05, -3.8676693e-03],\n",
       "        [-8.5959360e-03, -5.8093569e-03, -3.8994920e-05, ...,\n",
       "         -1.9320489e-03, -6.2040787e-04, -6.0545821e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-4.4001467e-03,  5.6960480e-03,  3.8729561e-03, ...,\n",
       "         -1.1946972e-03,  1.6779907e-03, -4.4450914e-03],\n",
       "        [-2.4874939e-03,  4.8946089e-04,  3.7398149e-04, ...,\n",
       "          5.0551984e-03,  2.3984637e-03, -2.3362346e-03]],\n",
       "\n",
       "       [[ 4.1994965e-04,  1.6016106e-03, -1.1944931e-03, ...,\n",
       "          7.1805040e-04, -1.8001664e-03,  3.0638892e-03],\n",
       "        [ 1.7832178e-03,  3.6578989e-03, -1.5501508e-03, ...,\n",
       "         -3.8861427e-03, -1.4480082e-03,  3.9847423e-03]],\n",
       "\n",
       "       [[-5.6835014e-04,  5.0373366e-03,  1.6148938e-04, ...,\n",
       "         -4.8523021e-04,  1.2904956e-03,  2.2496160e-03],\n",
       "        [ 5.0683095e-05,  4.4821575e-03, -1.2552496e-03, ...,\n",
       "          6.6187076e-04, -1.0526498e-03,  4.5244633e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(V(tf.nn.tanh(W1(output_x)+W2(tf.expand_dims(state_x, 1)))),axis=1) * output_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
       "array([[-0.00866107,  0.00427205,  0.00717272, ..., -0.00418001,\n",
       "         0.00171486, -0.01210309],\n",
       "       [-0.00654649,  0.01415508,  0.00554389, ..., -0.00375168,\n",
       "         0.00437062, -0.00254392],\n",
       "       [-0.01427501, -0.00953729, -0.00025407, ..., -0.00332472,\n",
       "        -0.00059867, -0.00992225],\n",
       "       ...,\n",
       "       [-0.00688764,  0.00618551,  0.00424694, ...,  0.0038605 ,\n",
       "         0.00407645, -0.00678133],\n",
       "       [ 0.00220317,  0.00525951, -0.00274464, ..., -0.00316809,\n",
       "        -0.00324817,  0.00704863],\n",
       "       [-0.00051767,  0.00951949, -0.00109376, ...,  0.00017664,\n",
       "         0.00023785,  0.00677408]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.nn.softmax(V(tf.nn.tanh(W1(output_x)+W2(tf.expand_dims(state_x, 1)))),axis=1) * output_x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 1, 32), dtype=float32, numpy=\n",
       "array([[[-0.00866107,  0.00427205,  0.00717272, ..., -0.00418001,\n",
       "          0.00171486, -0.01210309]],\n",
       "\n",
       "       [[-0.00654649,  0.01415508,  0.00554389, ..., -0.00375168,\n",
       "          0.00437062, -0.00254392]],\n",
       "\n",
       "       [[-0.01427501, -0.00953729, -0.00025407, ..., -0.00332472,\n",
       "         -0.00059867, -0.00992225]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.00688764,  0.00618551,  0.00424694, ...,  0.0038605 ,\n",
       "          0.00407645, -0.00678133]],\n",
       "\n",
       "       [[ 0.00220317,  0.00525951, -0.00274464, ..., -0.00316809,\n",
       "         -0.00324817,  0.00704863]],\n",
       "\n",
       "       [[-0.00051767,  0.00951949, -0.00109376, ...,  0.00017664,\n",
       "          0.00023785,  0.00677408]]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(tf.reduce_sum(tf.nn.softmax(V(tf.nn.tanh(W1(output_x)+W2(tf.expand_dims(state_x, 1)))),axis=1) * output_x, axis=1) , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 1, 42), dtype=float32, numpy=\n",
       "array([[[-0.00866107,  0.00427205,  0.00717272, ..., -0.02322576,\n",
       "          0.01806841,  0.03489599]],\n",
       "\n",
       "       [[-0.00654649,  0.01415508,  0.00554389, ..., -0.02322576,\n",
       "          0.01806841,  0.03489599]],\n",
       "\n",
       "       [[-0.01427501, -0.00953729, -0.00025407, ..., -0.02322576,\n",
       "          0.01806841,  0.03489599]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.00688764,  0.00618551,  0.00424694, ..., -0.02322576,\n",
       "          0.01806841,  0.03489599]],\n",
       "\n",
       "       [[ 0.00220317,  0.00525951, -0.00274464, ..., -0.02322576,\n",
       "          0.01806841,  0.03489599]],\n",
       "\n",
       "       [[-0.00051767,  0.00951949, -0.00109376, ..., -0.02322576,\n",
       "          0.01806841,  0.03489599]]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([tf.expand_dims(tf.reduce_sum(tf.nn.softmax(V(tf.nn.tanh(W1(output_x)+W2(tf.expand_dims(state_x, 1)))),axis=1) * output_x, axis=1) , 1), embed(test_y)],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru1 = keras.layers.GRU(units, return_sequences=True, return_state=True,\n",
    "                                       recurrent_initializer=\"glorot_uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(32, 1, 32), dtype=float32, numpy=\n",
       " array([[[ 0.00036771,  0.00946481,  0.00336648, ..., -0.00062484,\n",
       "          -0.00501798, -0.00876475]],\n",
       " \n",
       "        [[ 0.00011893,  0.00942861,  0.00669399, ..., -0.00020584,\n",
       "          -0.00579007, -0.00790557]],\n",
       " \n",
       "        [[-0.00124503,  0.00480166,  0.00167328, ..., -0.00205437,\n",
       "          -0.0050885 , -0.00797837]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.00190381,  0.00823201,  0.00451687, ..., -0.00213723,\n",
       "          -0.00405872, -0.00935513]],\n",
       " \n",
       "        [[-0.00132481,  0.0058695 ,  0.0035357 , ...,  0.00239262,\n",
       "          -0.00535032, -0.00917497]],\n",
       " \n",
       "        [[-0.00097965,  0.00704645,  0.00501177, ...,  0.00049192,\n",
       "          -0.00489612, -0.0074265 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
       " array([[ 0.00036771,  0.00946481,  0.00336648, ..., -0.00062484,\n",
       "         -0.00501798, -0.00876475],\n",
       "        [ 0.00011893,  0.00942861,  0.00669399, ..., -0.00020584,\n",
       "         -0.00579007, -0.00790557],\n",
       "        [-0.00124503,  0.00480166,  0.00167328, ..., -0.00205437,\n",
       "         -0.0050885 , -0.00797837],\n",
       "        ...,\n",
       "        [-0.00190381,  0.00823201,  0.00451687, ..., -0.00213723,\n",
       "         -0.00405872, -0.00935513],\n",
       "        [-0.00132481,  0.0058695 ,  0.0035357 , ...,  0.00239262,\n",
       "         -0.00535032, -0.00917497],\n",
       "        [-0.00097965,  0.00704645,  0.00501177, ...,  0.00049192,\n",
       "         -0.00489612, -0.0074265 ]], dtype=float32)>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru1(tf.concat([tf.expand_dims(tf.reduce_sum(tf.nn.softmax(V(tf.nn.tanh(W1(output_x)+W2(tf.expand_dims(state_x, 1)))),axis=1) * output_x, axis=1) , 1), embed(test_y)],axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_y, state_y = gru1(tf.concat([tf.expand_dims(tf.reduce_sum(tf.nn.softmax(V(tf.nn.tanh(W1(output_x)+W2(tf.expand_dims(state_x, 1)))),axis=1) * output_x, axis=1) , 1), embed(test_y)],axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 1, 32), dtype=float32, numpy=\n",
       "array([[[ 0.00036771,  0.00946481,  0.00336648, ..., -0.00062484,\n",
       "         -0.00501798, -0.00876475]],\n",
       "\n",
       "       [[ 0.00011893,  0.00942861,  0.00669399, ..., -0.00020584,\n",
       "         -0.00579007, -0.00790557]],\n",
       "\n",
       "       [[-0.00124503,  0.00480166,  0.00167328, ..., -0.00205437,\n",
       "         -0.0050885 , -0.00797837]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.00190381,  0.00823201,  0.00451687, ..., -0.00213723,\n",
       "         -0.00405872, -0.00935513]],\n",
       "\n",
       "       [[-0.00132481,  0.0058695 ,  0.0035357 , ...,  0.00239262,\n",
       "         -0.00535032, -0.00917497]],\n",
       "\n",
       "       [[-0.00097965,  0.00704645,  0.00501177, ...,  0.00049192,\n",
       "         -0.00489612, -0.0074265 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
       "array([[ 0.00036771,  0.00946481,  0.00336648, ..., -0.00062484,\n",
       "        -0.00501798, -0.00876475],\n",
       "       [ 0.00011893,  0.00942861,  0.00669399, ..., -0.00020584,\n",
       "        -0.00579007, -0.00790557],\n",
       "       [-0.00124503,  0.00480166,  0.00167328, ..., -0.00205437,\n",
       "        -0.0050885 , -0.00797837],\n",
       "       ...,\n",
       "       [-0.00190381,  0.00823201,  0.00451687, ..., -0.00213723,\n",
       "        -0.00405872, -0.00935513],\n",
       "       [-0.00132481,  0.0058695 ,  0.0035357 , ...,  0.00239262,\n",
       "        -0.00535032, -0.00917497],\n",
       "       [-0.00097965,  0.00704645,  0.00501177, ...,  0.00049192,\n",
       "        -0.00489612, -0.0074265 ]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1,output_y.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
       "array([[ 0.00036771,  0.00946481,  0.00336648, ..., -0.00062484,\n",
       "        -0.00501798, -0.00876475],\n",
       "       [ 0.00011893,  0.00942861,  0.00669399, ..., -0.00020584,\n",
       "        -0.00579007, -0.00790557],\n",
       "       [-0.00124503,  0.00480166,  0.00167328, ..., -0.00205437,\n",
       "        -0.0050885 , -0.00797837],\n",
       "       ...,\n",
       "       [-0.00190381,  0.00823201,  0.00451687, ..., -0.00213723,\n",
       "        -0.00405872, -0.00935513],\n",
       "       [-0.00132481,  0.0058695 ,  0.0035357 , ...,  0.00239262,\n",
       "        -0.00535032, -0.00917497],\n",
       "       [-0.00097965,  0.00704645,  0.00501177, ...,  0.00049192,\n",
       "        -0.00489612, -0.0074265 ]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(output_y, (-1,output_y.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = keras.layers.Dense(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 10), dtype=float32, numpy=\n",
       "array([[-5.95972361e-03,  2.77436059e-03, -2.20011547e-03,\n",
       "         7.65925134e-03, -1.45629831e-02,  2.78214971e-03,\n",
       "         2.06288183e-03,  1.90507702e-03,  6.89866347e-03,\n",
       "         6.51487662e-03],\n",
       "       [-1.00120995e-02,  1.84332626e-03, -5.50950971e-03,\n",
       "         7.76329171e-03, -1.39553417e-02,  2.70068343e-03,\n",
       "         6.03591278e-03, -5.99945080e-04,  9.27803107e-03,\n",
       "         4.61826753e-03],\n",
       "       [-6.15117420e-03,  3.11450218e-03,  1.76435639e-03,\n",
       "         8.00396502e-03, -1.19071109e-02,  4.74305637e-03,\n",
       "        -3.70908342e-03,  4.13117814e-04,  6.21915981e-03,\n",
       "         8.54632352e-03],\n",
       "       [-1.33094226e-03,  3.41514708e-03,  7.63795245e-03,\n",
       "         4.21474548e-03, -1.23214470e-02,  3.29726026e-03,\n",
       "        -6.63302280e-03,  4.98001929e-03,  3.07544251e-03,\n",
       "         6.20615017e-03],\n",
       "       [-8.92779045e-03, -1.04309223e-03,  3.75626446e-03,\n",
       "         1.02804909e-02, -1.26910694e-02,  5.61295403e-03,\n",
       "        -6.35087490e-03,  2.72928784e-03,  1.01710018e-02,\n",
       "         1.62577182e-02],\n",
       "       [-9.79627110e-03,  1.02036027e-03, -3.79267940e-03,\n",
       "         2.54085916e-03, -1.07438229e-02,  4.85989638e-03,\n",
       "         6.96214847e-03,  1.49267260e-04,  8.17462709e-03,\n",
       "         8.26129317e-03],\n",
       "       [-5.39238937e-03,  2.46985536e-03,  4.31414694e-03,\n",
       "         4.35773330e-03, -1.17445625e-02,  3.22490558e-03,\n",
       "        -2.69782566e-03,  2.46274751e-03,  5.49016707e-03,\n",
       "         4.32075188e-03],\n",
       "       [-8.93941522e-03,  1.92310952e-04, -1.40865613e-03,\n",
       "         1.03953891e-02, -1.40058976e-02,  4.31103911e-03,\n",
       "        -6.50395639e-04,  1.89184374e-03,  1.02442279e-02,\n",
       "         1.29800104e-02],\n",
       "       [-2.12267973e-03,  3.24284378e-03,  6.72535272e-03,\n",
       "         5.03645279e-03, -1.11132385e-02,  3.68255260e-03,\n",
       "        -5.59947174e-03,  3.37383640e-03,  3.91437020e-03,\n",
       "         6.01364672e-03],\n",
       "       [-4.17156052e-03,  1.69390987e-03,  6.26407797e-03,\n",
       "         6.64319843e-03, -1.24664688e-02,  4.17809188e-03,\n",
       "        -6.54119346e-03,  4.01577307e-03,  5.88256307e-03,\n",
       "         1.01274317e-02],\n",
       "       [-6.75531570e-03,  2.59684958e-03, -3.11732804e-03,\n",
       "         8.47831555e-03, -1.33639798e-02,  3.16112814e-03,\n",
       "         3.10599944e-03,  2.88993819e-04,  7.72698550e-03,\n",
       "         6.31448999e-03],\n",
       "       [-8.67169071e-03, -5.61514171e-04, -1.84230553e-03,\n",
       "         4.81037237e-03, -1.54610584e-02,  1.67973968e-03,\n",
       "         3.70507780e-03,  6.66383002e-03,  6.77973498e-03,\n",
       "         4.15209774e-03],\n",
       "       [-8.43684282e-03, -1.33215683e-04,  1.26363710e-03,\n",
       "         4.72464599e-03, -1.16493274e-02,  5.31972852e-03,\n",
       "        -5.67733077e-04,  2.36345897e-03,  7.50535773e-03,\n",
       "         1.27837900e-02],\n",
       "       [-6.08865917e-03,  6.82105427e-04,  5.13116783e-03,\n",
       "         7.87185319e-03, -1.25562390e-02,  4.73023951e-03,\n",
       "        -6.45783450e-03,  3.66249424e-03,  7.36351777e-03,\n",
       "         1.23324478e-02],\n",
       "       [-8.23049434e-03,  6.56355871e-04,  2.21123919e-05,\n",
       "         2.03517126e-03, -1.08415391e-02,  2.84427684e-03,\n",
       "         4.48447838e-03,  3.05663887e-03,  5.91034163e-03,\n",
       "         3.15202517e-03],\n",
       "       [-8.33529513e-03, -1.04177743e-05, -1.22279837e-03,\n",
       "         3.07347113e-03, -1.37625271e-02,  2.98866211e-03,\n",
       "         3.54020693e-03,  4.86656604e-03,  5.77515550e-03,\n",
       "         6.53585698e-03],\n",
       "       [-8.93997401e-03,  1.48736173e-03, -1.41856936e-03,\n",
       "         1.54775614e-03, -9.14571062e-03,  4.07844223e-03,\n",
       "         6.97003119e-03,  1.24216778e-04,  7.46558607e-03,\n",
       "         4.49063396e-03],\n",
       "       [-8.48798174e-03,  2.28119222e-03,  6.98879128e-04,\n",
       "         6.57198485e-03, -1.13387788e-02,  4.10278002e-03,\n",
       "        -8.56429338e-04, -3.34066222e-04,  7.36492174e-03,\n",
       "         5.72643150e-03],\n",
       "       [-4.22646338e-03,  1.88272877e-03,  3.70956585e-03,\n",
       "         4.59094346e-03, -1.36986114e-02,  2.77401274e-03,\n",
       "        -2.82499567e-03,  5.50661143e-03,  4.63890657e-03,\n",
       "         5.27654868e-03],\n",
       "       [-8.58190842e-03, -7.59603572e-04,  7.46677048e-04,\n",
       "         6.86473027e-03, -1.42245870e-02,  3.09816678e-03,\n",
       "        -3.90969217e-06,  5.18623926e-03,  8.02216679e-03,\n",
       "         9.01350472e-03],\n",
       "       [-7.30772223e-03,  1.47869298e-03,  3.82821192e-04,\n",
       "         3.87050258e-03, -1.15047377e-02,  5.00004832e-03,\n",
       "         4.54169232e-04,  1.58139446e-03,  5.96198346e-03,\n",
       "         9.79332533e-03],\n",
       "       [-8.16733483e-03,  4.61975811e-04,  2.59752478e-03,\n",
       "         4.09644982e-03, -9.60629713e-03,  4.26227041e-03,\n",
       "         7.48135615e-04,  1.55485049e-03,  7.14600319e-03,\n",
       "         8.01188592e-03],\n",
       "       [-6.68089464e-03,  2.31680367e-03,  2.85382289e-03,\n",
       "         5.38582355e-03, -9.81627591e-03,  3.80124338e-03,\n",
       "        -1.22947153e-03, -2.90117227e-04,  6.40331674e-03,\n",
       "         3.70780705e-03],\n",
       "       [-5.23527246e-03,  3.04768188e-03,  3.09594767e-03,\n",
       "         7.26644788e-03, -1.07129673e-02,  4.56516212e-03,\n",
       "        -3.78469774e-03,  5.50136319e-04,  5.80035709e-03,\n",
       "         7.41801970e-03],\n",
       "       [-8.92779045e-03, -1.04309223e-03,  3.75626446e-03,\n",
       "         1.02804909e-02, -1.26910694e-02,  5.61295403e-03,\n",
       "        -6.35087490e-03,  2.72928784e-03,  1.01710018e-02,\n",
       "         1.62577182e-02],\n",
       "       [-8.49293359e-03,  5.91266435e-05, -1.31519907e-03,\n",
       "         2.66456045e-03, -1.28877461e-02,  3.89257492e-03,\n",
       "         3.17407772e-03,  3.87424044e-03,  6.26874203e-03,\n",
       "         7.93231092e-03],\n",
       "       [-6.08865917e-03,  6.82105427e-04,  5.13116783e-03,\n",
       "         7.87185319e-03, -1.25562390e-02,  4.73023951e-03,\n",
       "        -6.45783450e-03,  3.66249424e-03,  7.36351777e-03,\n",
       "         1.23324478e-02],\n",
       "       [-8.43684282e-03, -1.33215683e-04,  1.26363710e-03,\n",
       "         4.72464599e-03, -1.16493274e-02,  5.31972852e-03,\n",
       "        -5.67733077e-04,  2.36345897e-03,  7.50535773e-03,\n",
       "         1.27837900e-02],\n",
       "       [-8.16733483e-03,  4.61975811e-04,  2.59752478e-03,\n",
       "         4.09644982e-03, -9.60629713e-03,  4.26227041e-03,\n",
       "         7.48135615e-04,  1.55485049e-03,  7.14600319e-03,\n",
       "         8.01188592e-03],\n",
       "       [-8.79580714e-03,  1.05002848e-03, -3.56862973e-03,\n",
       "         1.00623984e-02, -1.46990353e-02,  3.65736568e-03,\n",
       "         2.17714789e-03,  9.63195460e-04,  9.68480483e-03,\n",
       "         1.04304217e-02],\n",
       "       [-8.20538960e-03,  1.02568651e-03, -3.79770063e-06,\n",
       "         6.87793130e-04, -1.03348643e-02,  4.79914062e-03,\n",
       "         3.63987102e-03,  1.52442325e-03,  5.56976162e-03,\n",
       "         7.22822919e-03],\n",
       "       [-9.79627110e-03,  1.02036027e-03, -3.79267940e-03,\n",
       "         2.54085916e-03, -1.07438229e-02,  4.85989638e-03,\n",
       "         6.96214847e-03,  1.49267260e-04,  8.17462709e-03,\n",
       "         8.26129317e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1(tf.reshape(output_y, (-1,output_y.shape[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6.5_tf2",
   "language": "python",
   "name": "py3.6.5_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac2faf5899a66e9621a617b941fc8fd5e22d901e8dabc92896275f4b1af12769"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
