{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.keras.layers.Embedding(\n",
    "    input_dim, output_dim, embeddings_initializer='uniform',\n",
    "    embeddings_regularizer=None, activity_regularizer=None,\n",
    "    embeddings_constraint=None, mask_zero=False, input_length=None, **kwargs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=10\n",
    "out_dim=10\n",
    "batch_size =32\n",
    "units = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x7fbf14c69a20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 其中1000表示input_dimension,也就是词汇表的大小，size of vocabulary\n",
    "embed = keras.layers.Embedding(input_dim=vocab_size,output_dim=out_dim)\n",
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 4],\n",
       "       [6, 4],\n",
       "       [7, 8],\n",
       "       [5, 3],\n",
       "       [5, 1],\n",
       "       [7, 0],\n",
       "       [7, 9],\n",
       "       [4, 1],\n",
       "       [3, 6],\n",
       "       [9, 1],\n",
       "       [9, 4],\n",
       "       [1, 7],\n",
       "       [3, 2],\n",
       "       [3, 3],\n",
       "       [3, 3],\n",
       "       [0, 6],\n",
       "       [7, 7],\n",
       "       [4, 1],\n",
       "       [9, 3],\n",
       "       [7, 6],\n",
       "       [8, 0],\n",
       "       [7, 8],\n",
       "       [3, 3],\n",
       "       [7, 5],\n",
       "       [3, 1],\n",
       "       [8, 3],\n",
       "       [7, 8],\n",
       "       [3, 2],\n",
       "       [2, 3],\n",
       "       [1, 1],\n",
       "       [5, 6],\n",
       "       [3, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x=np.random.randint(10,size=(batch_size,2))\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2, 10), dtype=float32, numpy=\n",
       "array([[[-0.01372281, -0.02558281,  0.03040667,  0.01933027,\n",
       "          0.00082835, -0.03225042,  0.02454338, -0.01896745,\n",
       "          0.01792786,  0.01688181],\n",
       "        [ 0.0167869 , -0.0241568 , -0.01391283, -0.03121316,\n",
       "          0.02011046,  0.00024397, -0.02705981, -0.02816738,\n",
       "          0.02115688, -0.03088739]],\n",
       "\n",
       "       [[ 0.04986143, -0.04900012, -0.01908988,  0.02565249,\n",
       "          0.02327783, -0.02456356, -0.02516657,  0.04759378,\n",
       "          0.04027304, -0.0461815 ],\n",
       "        [ 0.0167869 , -0.0241568 , -0.01391283, -0.03121316,\n",
       "          0.02011046,  0.00024397, -0.02705981, -0.02816738,\n",
       "          0.02115688, -0.03088739]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [ 0.017586  ,  0.03954829, -0.0106926 , -0.04188658,\n",
       "          0.04094392, -0.03261347, -0.01800026,  0.02243816,\n",
       "         -0.0490546 ,  0.02102741]],\n",
       "\n",
       "       [[-0.0417516 ,  0.00715182, -0.00751869,  0.00929739,\n",
       "         -0.01326126, -0.03954202, -0.02942857, -0.02379285,\n",
       "          0.04480174, -0.03904296],\n",
       "        [-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746]],\n",
       "\n",
       "       [[-0.0417516 ,  0.00715182, -0.00751869,  0.00929739,\n",
       "         -0.01326126, -0.03954202, -0.02942857, -0.02379285,\n",
       "          0.04480174, -0.03904296],\n",
       "        [ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [ 0.04232648,  0.01709416, -0.00094085, -0.01240324,\n",
       "          0.00967775,  0.00040231,  0.0060554 ,  0.02592048,\n",
       "          0.00393624, -0.02889873]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [ 0.04158828,  0.02619595,  0.03656632,  0.03072089,\n",
       "          0.02597008, -0.01787263, -0.03740325, -0.00571898,\n",
       "          0.02652187,  0.03318973]],\n",
       "\n",
       "       [[ 0.0167869 , -0.0241568 , -0.01391283, -0.03121316,\n",
       "          0.02011046,  0.00024397, -0.02705981, -0.02816738,\n",
       "          0.02115688, -0.03088739],\n",
       "        [ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [ 0.04986143, -0.04900012, -0.01908988,  0.02565249,\n",
       "          0.02327783, -0.02456356, -0.02516657,  0.04759378,\n",
       "          0.04027304, -0.0461815 ]],\n",
       "\n",
       "       [[ 0.04158828,  0.02619595,  0.03656632,  0.03072089,\n",
       "          0.02597008, -0.01787263, -0.03740325, -0.00571898,\n",
       "          0.02652187,  0.03318973],\n",
       "        [ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503]],\n",
       "\n",
       "       [[ 0.04158828,  0.02619595,  0.03656632,  0.03072089,\n",
       "          0.02597008, -0.01787263, -0.03740325, -0.00571898,\n",
       "          0.02652187,  0.03318973],\n",
       "        [ 0.0167869 , -0.0241568 , -0.01391283, -0.03121316,\n",
       "          0.02011046,  0.00024397, -0.02705981, -0.02816738,\n",
       "          0.02115688, -0.03088739]],\n",
       "\n",
       "       [[ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503],\n",
       "        [ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [-0.01372281, -0.02558281,  0.03040667,  0.01933027,\n",
       "          0.00082835, -0.03225042,  0.02454338, -0.01896745,\n",
       "          0.01792786,  0.01688181]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746]],\n",
       "\n",
       "       [[ 0.04232648,  0.01709416, -0.00094085, -0.01240324,\n",
       "          0.00967775,  0.00040231,  0.0060554 ,  0.02592048,\n",
       "          0.00393624, -0.02889873],\n",
       "        [ 0.04986143, -0.04900012, -0.01908988,  0.02565249,\n",
       "          0.02327783, -0.02456356, -0.02516657,  0.04759378,\n",
       "          0.04027304, -0.0461815 ]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901]],\n",
       "\n",
       "       [[ 0.0167869 , -0.0241568 , -0.01391283, -0.03121316,\n",
       "          0.02011046,  0.00024397, -0.02705981, -0.02816738,\n",
       "          0.02115688, -0.03088739],\n",
       "        [ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503]],\n",
       "\n",
       "       [[ 0.04158828,  0.02619595,  0.03656632,  0.03072089,\n",
       "          0.02597008, -0.01787263, -0.03740325, -0.00571898,\n",
       "          0.02652187,  0.03318973],\n",
       "        [-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [ 0.04986143, -0.04900012, -0.01908988,  0.02565249,\n",
       "          0.02327783, -0.02456356, -0.02516657,  0.04759378,\n",
       "          0.04027304, -0.0461815 ]],\n",
       "\n",
       "       [[ 0.017586  ,  0.03954829, -0.0106926 , -0.04188658,\n",
       "          0.04094392, -0.03261347, -0.01800026,  0.02243816,\n",
       "         -0.0490546 ,  0.02102741],\n",
       "        [ 0.04232648,  0.01709416, -0.00094085, -0.01240324,\n",
       "          0.00967775,  0.00040231,  0.0060554 ,  0.02592048,\n",
       "          0.00393624, -0.02889873]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [ 0.017586  ,  0.03954829, -0.0106926 , -0.04188658,\n",
       "          0.04094392, -0.03261347, -0.01800026,  0.02243816,\n",
       "         -0.0490546 ,  0.02102741]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [-0.0417516 ,  0.00715182, -0.00751869,  0.00929739,\n",
       "         -0.01326126, -0.03954202, -0.02942857, -0.02379285,\n",
       "          0.04480174, -0.03904296]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503]],\n",
       "\n",
       "       [[ 0.017586  ,  0.03954829, -0.0106926 , -0.04188658,\n",
       "          0.04094392, -0.03261347, -0.01800026,  0.02243816,\n",
       "         -0.0490546 ,  0.02102741],\n",
       "        [-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [ 0.017586  ,  0.03954829, -0.0106926 , -0.04188658,\n",
       "          0.04094392, -0.03261347, -0.01800026,  0.02243816,\n",
       "         -0.0490546 ,  0.02102741]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [-0.01372281, -0.02558281,  0.03040667,  0.01933027,\n",
       "          0.00082835, -0.03225042,  0.02454338, -0.01896745,\n",
       "          0.01792786,  0.01688181]],\n",
       "\n",
       "       [[-0.01372281, -0.02558281,  0.03040667,  0.01933027,\n",
       "          0.00082835, -0.03225042,  0.02454338, -0.01896745,\n",
       "          0.01792786,  0.01688181],\n",
       "        [-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746]],\n",
       "\n",
       "       [[ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503],\n",
       "        [ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503]],\n",
       "\n",
       "       [[-0.0417516 ,  0.00715182, -0.00751869,  0.00929739,\n",
       "         -0.01326126, -0.03954202, -0.02942857, -0.02379285,\n",
       "          0.04480174, -0.03904296],\n",
       "        [ 0.04986143, -0.04900012, -0.01908988,  0.02565249,\n",
       "          0.02327783, -0.02456356, -0.02516657,  0.04759378,\n",
       "          0.04027304, -0.0461815 ]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [ 0.04232648,  0.01709416, -0.00094085, -0.01240324,\n",
       "          0.00967775,  0.00040231,  0.0060554 ,  0.02592048,\n",
       "          0.00393624, -0.02889873]]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[0,1],[1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 10), dtype=float32, numpy=\n",
       "array([[[ 0.04232648,  0.01709416, -0.00094085, -0.01240324,\n",
       "          0.00967775,  0.00040231,  0.0060554 ,  0.02592048,\n",
       "          0.00393624, -0.02889873],\n",
       "        [ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503]],\n",
       "\n",
       "       [[ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503],\n",
       "        [-0.01372281, -0.02558281,  0.03040667,  0.01933027,\n",
       "          0.00082835, -0.03225042,  0.02454338, -0.01896745,\n",
       "          0.01792786,  0.01688181]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(np.array([[0,1],[1,2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.keras.layers.GRU(\n",
    "    units, activation='tanh', recurrent_activation='sigmoid',\n",
    "    use_bias=True, kernel_initializer='glorot_uniform',\n",
    "    recurrent_initializer='orthogonal',\n",
    "    bias_initializer='zeros', kernel_regularizer=None,\n",
    "    recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "    kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,\n",
    "    dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False,\n",
    "    go_backwards=False, stateful=False, unroll=False, time_major=False,\n",
    "    reset_after=True, **kwargs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数\n",
    "\n",
    "```\n",
    "units 正整数，输出空间的维度。\n",
    "activation 要使用的激活函数。默认值：双曲正切(tanh)。如果您通过 None ，则不会应用激活(即 \"linear\" 激活：a(x) = x )。\n",
    "recurrent_activation 用于循环步骤的激活函数。默认值：sigmoid (sigmoid)。如果您通过 None ，则不会应用激活(即 \"linear\" 激活：a(x) = x )。\n",
    "use_bias 布尔值，(默认 True )，图层是否使用偏置向量。\n",
    "kernel_initializer kernel 权重矩阵的初始化器，用于输入的线性变换。默认值：glorot_uniform。\n",
    "recurrent_initializer recurrent_kernel 权重矩阵的初始化器，用于循环状态的线性变换。默认值：orthogonal。\n",
    "bias_initializer 偏置向量的初始化器。默认值：zeros。\n",
    "kernel_regularizer 应用于kernel 权重矩阵的正则化函数。默认值：None。\n",
    "recurrent_regularizer 应用于recurrent_kernel 权重矩阵的正则化函数。默认值：None。\n",
    "bias_regularizer 应用于偏置向量的正则化函数。默认值：None。\n",
    "activity_regularizer 应用于层输出的正则化函数(\"activation\")。默认值：None。\n",
    "kernel_constraint 应用于kernel 权重矩阵的约束函数。默认值：None。\n",
    "recurrent_constraint 应用于recurrent_kernel 权重矩阵的约束函数。默认值：None。\n",
    "bias_constraint 应用于偏置向量的约束函数。默认值：None。\n",
    "dropout 在 0 和 1 之间浮点数。为输入的线性变换而下降的单位分数。默认值：0。\n",
    "recurrent_dropout 在 0 和 1 之间浮点数。用于循环状态的线性变换的单位的分数。默认值：0。\n",
    "return_sequences 布尔值。是返回输出序列中的最后一个输出，还是返回完整序列。默认值：False。\n",
    "return_state 布尔值。是否返回除了输出之外的最后一个状态。默认值：False。\n",
    "go_backwards 布尔值(默认 False )。如果为 True，则反向处理输入序列并返回反向序列。\n",
    "stateful 布尔值(默认为 False)。如果为 True，则批次中索引 i 处每个样本的最后状态将用作下一批中索引 i 的样本的初始状态。\n",
    "unroll 布尔值(默认为 False)。如果为 True，则网络将展开，否则将使用符号循环。展开可以speed-up一个RNN，虽然它往往更多memory-intensive。展开仅适用于短序列。\n",
    "time_major inputs 和 outputs 张量的形状格式。如果为 True，输入和输出的形状将是 [timesteps, batch, feature] ，而在 False 情况下，它将是 [batch, timesteps, feature] 。使用time_major = True 效率更高一些，因为它避免了 RNN 计算开始和结束时的转置。但是，大多数 TensorFlow 数据是 batch-major，因此默认情况下，此函数接受输入并以 batch-major 形式发出输出。\n",
    "reset_after GRU 约定(是否在矩阵乘法之后或之前应用重置门)。 False = \"before\"，True = \"after\"(默认和 cuDNN 兼容)。\n",
    "```\n",
    "\n",
    "- 属性\n",
    "\n",
    "```\n",
    "activation\n",
    "bias_constraint\n",
    "bias_initializer\n",
    "bias_regularizer\n",
    "dropout\n",
    "implementation\n",
    "kernel_constraint\n",
    "kernel_initializer\n",
    "kernel_regularizer\n",
    "recurrent_activation\n",
    "recurrent_constraint\n",
    "recurrent_dropout\n",
    "recurrent_initializer\n",
    "recurrent_regularizer\n",
    "reset_after\n",
    "states\n",
    "units\n",
    "use_bias\n",
    "```\n",
    "\n",
    "有关 RNN API 使用的详细信息，请参阅 Keras RNN API 指南。\n",
    "\n",
    "根据可用的运行时硬件和约束，该层将选择不同的实现(基于 cuDNN 或 pure-TensorFlow)以最大化性能。如果 GPU 可用并且该层的所有参数都满足 cuDNN 内核的要求(详见下文)，则该层将使用快速 cuDNN 实现。\n",
    "\n",
    "使用 cuDNN 实现的要求是：\n",
    "```\n",
    "activation == tanh\n",
    "recurrent_activation == sigmoid\n",
    "recurrent_dropout == 0\n",
    "unroll 是 False\n",
    "use_bias 是 True\n",
    "reset_after 是 True\n",
    "```\n",
    "输入，如果使用掩码，严格来说是right-padded。\n",
    "在最外层的上下文中启用了即刻执行。\n",
    "GRU 实现有两种变体。默认的基于 v3，并在矩阵乘法之前将重置门应用于隐藏状态。另一种是根据原件，顺序颠倒。\n",
    "\n",
    "第二个变体与 CuDNNGRU (GPU-only) 兼容，并允许在 CPU 上进行推理。因此，它对 kernel 和 recurrent_kernel 有不同的偏见。要使用此变体，请设置 reset_after=True 和 recurrent_activation='sigmoid' 。\n",
    "\n",
    "例如：\n",
    "```\n",
    "inputs = tf.random.normal([32, 10, 8])\n",
    "gru = tf.keras.layers.GRU(4)\n",
    "output = gru(inputs)\n",
    "print(output.shape)\n",
    "(32, 4)\n",
    "gru = tf.keras.layers.GRU(4, return_sequences=True, return_state=True)\n",
    "whole_sequence_output, final_state = gru(inputs)\n",
    "print(whole_sequence_output.shape)\n",
    "(32, 10, 4)\n",
    "print(final_state.shape)\n",
    "(32, 4)\n",
    "```\n",
    "\n",
    "调用参数：\n",
    "inputs:一个 3D 张量，有形状[batch, timesteps, feature].\n",
    "mask: 形状的二进制张量[samples, timesteps]指示是否应屏蔽给定时间步(可选，默认为None)。个人True条目指示应使用相应的时间步长，而Falseentry 表示应该忽略相应的时间步长。\n",
    "training:Python 布尔值，指示层应该在训练模式还是推理模式下运行。此参数在调用时传递给单元格。这仅在以下情况下才相关dropout或者recurrent_dropout使用(可选，默认为None)。\n",
    "initial_state：要传递给单元格第一次调用的初始状态张量列表(可选，默认为None这会导致创建zero-filled 初始状态张量)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = keras.layers.GRU(units, return_sequences=True, return_state=True,\n",
    "                                       recurrent_initializer=\"glorot_uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on GRU in module keras.layers.recurrent_v2 object:\n",
      "\n",
      "class GRU(keras.layers.recurrent.DropoutRNNCellMixin, keras.layers.recurrent.GRU)\n",
      " |  Gated Recurrent Unit - Cho et al. 2014.\n",
      " |  \n",
      " |  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n",
      " |  for details about the usage of RNN API.\n",
      " |  \n",
      " |  Based on available runtime hardware and constraints, this layer\n",
      " |  will choose different implementations (cuDNN-based or pure-TensorFlow)\n",
      " |  to maximize the performance. If a GPU is available and all\n",
      " |  the arguments to the layer meet the requirement of the CuDNN kernel\n",
      " |  (see below for details), the layer will use a fast cuDNN implementation.\n",
      " |  \n",
      " |  The requirements to use the cuDNN implementation are:\n",
      " |  \n",
      " |  1. `activation` == `tanh`\n",
      " |  2. `recurrent_activation` == `sigmoid`\n",
      " |  3. `recurrent_dropout` == 0\n",
      " |  4. `unroll` is `False`\n",
      " |  5. `use_bias` is `True`\n",
      " |  6. `reset_after` is `True`\n",
      " |  7. Inputs, if use masking, are strictly right-padded.\n",
      " |  8. Eager execution is enabled in the outermost context.\n",
      " |  \n",
      " |  There are two variants of the GRU implementation. The default one is based on\n",
      " |  [v3](https://arxiv.org/abs/1406.1078v3) and has reset gate applied to hidden\n",
      " |  state before matrix multiplication. The other one is based on\n",
      " |  [original](https://arxiv.org/abs/1406.1078v1) and has the order reversed.\n",
      " |  \n",
      " |  The second variant is compatible with CuDNNGRU (GPU-only) and allows\n",
      " |  inference on CPU. Thus it has separate biases for `kernel` and\n",
      " |  `recurrent_kernel`. To use this variant, set `'reset_after'=True` and\n",
      " |  `recurrent_activation='sigmoid'`.\n",
      " |  \n",
      " |  For example:\n",
      " |  \n",
      " |  >>> inputs = tf.random.normal([32, 10, 8])\n",
      " |  >>> gru = tf.keras.layers.GRU(4)\n",
      " |  >>> output = gru(inputs)\n",
      " |  >>> print(output.shape)\n",
      " |  (32, 4)\n",
      " |  >>> gru = tf.keras.layers.GRU(4, return_sequences=True, return_state=True)\n",
      " |  >>> whole_sequence_output, final_state = gru(inputs)\n",
      " |  >>> print(whole_sequence_output.shape)\n",
      " |  (32, 10, 4)\n",
      " |  >>> print(final_state.shape)\n",
      " |  (32, 4)\n",
      " |  \n",
      " |  Args:\n",
      " |    units: Positive integer, dimensionality of the output space.\n",
      " |    activation: Activation function to use.\n",
      " |      Default: hyperbolic tangent (`tanh`).\n",
      " |      If you pass `None`, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    recurrent_activation: Activation function to use\n",
      " |      for the recurrent step.\n",
      " |      Default: sigmoid (`sigmoid`).\n",
      " |      If you pass `None`, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix,\n",
      " |      used for the linear transformation of the inputs. Default:\n",
      " |      `glorot_uniform`.\n",
      " |    recurrent_initializer: Initializer for the `recurrent_kernel`\n",
      " |       weights matrix, used for the linear transformation of the recurrent\n",
      " |       state. Default: `orthogonal`.\n",
      " |    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n",
      " |    kernel_regularizer: Regularizer function applied to the `kernel` weights\n",
      " |      matrix. Default: `None`.\n",
      " |    recurrent_regularizer: Regularizer function applied to the\n",
      " |      `recurrent_kernel` weights matrix. Default: `None`.\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector. Default:\n",
      " |      `None`.\n",
      " |    activity_regularizer: Regularizer function applied to the output of the\n",
      " |      layer (its \"activation\"). Default: `None`.\n",
      " |    kernel_constraint: Constraint function applied to the `kernel` weights\n",
      " |      matrix. Default: `None`.\n",
      " |    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n",
      " |      weights matrix. Default: `None`.\n",
      " |    bias_constraint: Constraint function applied to the bias vector. Default:\n",
      " |      `None`.\n",
      " |    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n",
      " |      transformation of the inputs. Default: 0.\n",
      " |    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n",
      " |      the linear transformation of the recurrent state. Default: 0.\n",
      " |    return_sequences: Boolean. Whether to return the last output\n",
      " |      in the output sequence, or the full sequence. Default: `False`.\n",
      " |    return_state: Boolean. Whether to return the last state in addition to the\n",
      " |      output. Default: `False`.\n",
      " |    go_backwards: Boolean (default `False`).\n",
      " |      If True, process the input sequence backwards and return the\n",
      " |      reversed sequence.\n",
      " |    stateful: Boolean (default False). If True, the last state\n",
      " |      for each sample at index i in a batch will be used as initial\n",
      " |      state for the sample of index i in the following batch.\n",
      " |    unroll: Boolean (default False).\n",
      " |      If True, the network will be unrolled,\n",
      " |      else a symbolic loop will be used.\n",
      " |      Unrolling can speed-up a RNN,\n",
      " |      although it tends to be more memory-intensive.\n",
      " |      Unrolling is only suitable for short sequences.\n",
      " |    time_major: The shape format of the `inputs` and `outputs` tensors.\n",
      " |      If True, the inputs and outputs will be in shape\n",
      " |      `[timesteps, batch, feature]`, whereas in the False case, it will be\n",
      " |      `[batch, timesteps, feature]`. Using `time_major = True` is a bit more\n",
      " |      efficient because it avoids transposes at the beginning and end of the\n",
      " |      RNN calculation. However, most TensorFlow data is batch-major, so by\n",
      " |      default this function accepts input and emits output in batch-major\n",
      " |      form.\n",
      " |    reset_after: GRU convention (whether to apply reset gate after or\n",
      " |      before matrix multiplication). False = \"before\",\n",
      " |      True = \"after\" (default and CuDNN compatible).\n",
      " |  \n",
      " |  Call arguments:\n",
      " |    inputs: A 3D tensor, with shape `[batch, timesteps, feature]`.\n",
      " |    mask: Binary tensor of shape `[samples, timesteps]` indicating whether\n",
      " |      a given timestep should be masked  (optional, defaults to `None`).\n",
      " |      An individual `True` entry indicates that the corresponding timestep\n",
      " |      should be utilized, while a `False` entry indicates that the\n",
      " |      corresponding timestep should be ignored.\n",
      " |    training: Python boolean indicating whether the layer should behave in\n",
      " |      training mode or in inference mode. This argument is passed to the cell\n",
      " |      when calling it. This is only relevant if `dropout` or\n",
      " |      `recurrent_dropout` is used  (optional, defaults to `None`).\n",
      " |    initial_state: List of initial state tensors to be passed to the first\n",
      " |      call of the cell  (optional, defaults to `None` which causes creation\n",
      " |      of zero-filled initial state tensors).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GRU\n",
      " |      keras.layers.recurrent.DropoutRNNCellMixin\n",
      " |      keras.layers.recurrent.GRU\n",
      " |      keras.layers.recurrent.RNN\n",
      " |      keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, time_major=False, reset_after=True, **kwargs)\n",
      " |  \n",
      " |  call(self, inputs, mask=None, training=None, initial_state=None)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Note here that `call()` method in `tf.keras` is little bit different\n",
      " |      from `keras` API. In `keras` API, you can pass support masking for\n",
      " |      layers as additional arguments. Whereas `tf.keras` has `compute_mask()`\n",
      " |      method to support masking.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor, or dict/list/tuple of input tensors.\n",
      " |          The first positional `inputs` argument is subject to special rules:\n",
      " |          - `inputs` must be explicitly passed. A layer cannot have zero\n",
      " |            arguments, and `inputs` cannot be provided via the default value\n",
      " |            of a keyword argument.\n",
      " |          - NumPy array or Python scalar values in `inputs` get cast as tensors.\n",
      " |          - Keras mask metadata is only collected from `inputs`.\n",
      " |          - Layers are built (`build(input_shape)` method)\n",
      " |            using shape info from `inputs` only.\n",
      " |          - `input_spec` compatibility is only checked against `inputs`.\n",
      " |          - Mixed precision input casting is only applied to `inputs`.\n",
      " |            If a layer has tensor arguments in `*args` or `**kwargs`, their\n",
      " |            casting behavior in mixed precision should be handled manually.\n",
      " |          - The SavedModel input specification is generated using `inputs` only.\n",
      " |          - Integration with various ecosystem packages like TFMOT, TFLite,\n",
      " |            TF.js, etc is only supported for `inputs` and not for tensors in\n",
      " |            positional and keyword arguments.\n",
      " |        *args: Additional positional arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |        **kwargs: Additional keyword arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |          The following optional keyword arguments are reserved:\n",
      " |          - `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          - `mask`: Boolean input mask. If the layer's `call()` method takes a\n",
      " |            `mask` argument, its default value will be set to the mask generated\n",
      " |            for `inputs` by the previous layer (if `input` did come from a layer\n",
      " |            that generated a corresponding mask, i.e. if it came from a Keras\n",
      " |            layer with masking support).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.layers.recurrent.DropoutRNNCellMixin:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_dropout_mask_for_cell(self, inputs, training, count=1)\n",
      " |      Get the dropout mask for RNN cell's input.\n",
      " |      \n",
      " |      It will create mask based on context if there isn't any existing cached\n",
      " |      mask. If a new mask is generated, it will update the cache in the cell.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: The input tensor whose shape will be used to generate dropout\n",
      " |          mask.\n",
      " |        training: Boolean tensor, whether its in training mode, dropout will be\n",
      " |          ignored in non-training mode.\n",
      " |        count: Int, how many dropout mask will be generated. It is useful for cell\n",
      " |          that has internal weights fused together.\n",
      " |      Returns:\n",
      " |        List of mask tensor, generated or cached mask based on context.\n",
      " |  \n",
      " |  get_recurrent_dropout_mask_for_cell(self, inputs, training, count=1)\n",
      " |      Get the recurrent dropout mask for RNN cell.\n",
      " |      \n",
      " |      It will create mask based on context if there isn't any existing cached\n",
      " |      mask. If a new mask is generated, it will update the cache in the cell.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: The input tensor whose shape will be used to generate dropout\n",
      " |          mask.\n",
      " |        training: Boolean tensor, whether its in training mode, dropout will be\n",
      " |          ignored in non-training mode.\n",
      " |        count: Int, how many dropout mask will be generated. It is useful for cell\n",
      " |          that has internal weights fused together.\n",
      " |      Returns:\n",
      " |        List of mask tensor, generated or cached mask based on context.\n",
      " |  \n",
      " |  reset_dropout_mask(self)\n",
      " |      Reset the cached dropout masks if any.\n",
      " |      \n",
      " |      This is important for the RNN layer to invoke this in it `call()` method so\n",
      " |      that the cached mask is cleared before calling the `cell.call()`. The mask\n",
      " |      should be cached across the timestep within the same batch, but shouldn't\n",
      " |      be cached between batches. Otherwise it will introduce unreasonable bias\n",
      " |      against certain index of data within the batch.\n",
      " |  \n",
      " |  reset_recurrent_dropout_mask(self)\n",
      " |      Reset the cached recurrent dropout masks if any.\n",
      " |      \n",
      " |      This is important for the RNN layer to invoke this in it call() method so\n",
      " |      that the cached mask is cleared before calling the cell.call(). The mask\n",
      " |      should be cached across the timestep within the same batch, but shouldn't\n",
      " |      be cached between batches. Otherwise it will introduce unreasonable bias\n",
      " |      against certain index of data within the batch.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.layers.recurrent.DropoutRNNCellMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.layers.recurrent.GRU:\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Note that `get_config()` does not guarantee to return a fresh copy of dict\n",
      " |      every time it is called. The callers should make a copy of the returned dict\n",
      " |      if they want to modify it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.layers.recurrent.GRU:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.layers.recurrent.GRU:\n",
      " |  \n",
      " |  activation\n",
      " |  \n",
      " |  bias_constraint\n",
      " |  \n",
      " |  bias_initializer\n",
      " |  \n",
      " |  bias_regularizer\n",
      " |  \n",
      " |  dropout\n",
      " |  \n",
      " |  implementation\n",
      " |  \n",
      " |  kernel_constraint\n",
      " |  \n",
      " |  kernel_initializer\n",
      " |  \n",
      " |  kernel_regularizer\n",
      " |  \n",
      " |  recurrent_activation\n",
      " |  \n",
      " |  recurrent_constraint\n",
      " |  \n",
      " |  recurrent_dropout\n",
      " |  \n",
      " |  recurrent_initializer\n",
      " |  \n",
      " |  recurrent_regularizer\n",
      " |  \n",
      " |  reset_after\n",
      " |  \n",
      " |  units\n",
      " |  \n",
      " |  use_bias\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.layers.recurrent.RNN:\n",
      " |  \n",
      " |  __call__(self, inputs, initial_state=None, constants=None, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |        - If the layer is not built, the method will call `build`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Args:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_initial_state(self, inputs)\n",
      " |  \n",
      " |  reset_states(self, states=None)\n",
      " |      Reset the recorded states for the stateful RNN layer.\n",
      " |      \n",
      " |      Can only be used when RNN layer is constructed with `stateful` = `True`.\n",
      " |      Args:\n",
      " |        states: Numpy arrays that contains the value for the initial state, which\n",
      " |          will be feed to cell at the first time step. When the value is None,\n",
      " |          zero filled numpy array will be created based on the cell state size.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: When the RNN layer is not stateful.\n",
      " |        ValueError: When the batch size of the RNN layer is unknown.\n",
      " |        ValueError: When the input numpy array is not compatible with the RNN\n",
      " |          layer state, either size wise or dtype wise.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.layers.recurrent.RNN:\n",
      " |  \n",
      " |  states\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |            inputs - Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(inputs))\n",
      " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This is\n",
      " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result of\n",
      " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
      " |          using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Args:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The variable created.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  finalize_state(self)\n",
      " |      Finalizes the layers state after updating layer weights.\n",
      " |      \n",
      " |      This function can be subclassed in a layer and will be called after updating\n",
      " |      a layer weights. It can be overridden to finalize any additional layer state\n",
      " |      after a weight update.\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first input node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first output node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer, as NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of NumPy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of NumPy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function, by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Args:\n",
      " |        weights: a list of NumPy arrays. The number\n",
      " |          of arrays and their shape must match\n",
      " |          number of the dimensions of the weights\n",
      " |          of the layer (i.e. it should match the\n",
      " |          output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the provided weights list does not match the\n",
      " |          layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the layer's computations.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
      " |      the weights.\n",
      " |      \n",
      " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
      " |      computations and the output to be in the compute dtype as well. This is done\n",
      " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
      " |      these casts if implementing your own layer.\n",
      " |      \n",
      " |      Layers often perform certain internal computations in higher precision when\n",
      " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
      " |      will still typically be float16 or bfloat16 in such cases.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The layer's compute dtype.\n",
      " |  \n",
      " |  dtype\n",
      " |      The dtype of the layer weights.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
      " |      dtype of the layer's computations.\n",
      " |  \n",
      " |  dtype_policy\n",
      " |      The dtype policy associated with this layer.\n",
      " |      \n",
      " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> len(model.losses)\n",
      " |      0\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> len(model.losses)\n",
      " |      1\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of `Metric` objects.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
      " |      themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2, 10), dtype=float32, numpy=\n",
       "array([[[-0.01372281, -0.02558281,  0.03040667,  0.01933027,\n",
       "          0.00082835, -0.03225042,  0.02454338, -0.01896745,\n",
       "          0.01792786,  0.01688181],\n",
       "        [ 0.0167869 , -0.0241568 , -0.01391283, -0.03121316,\n",
       "          0.02011046,  0.00024397, -0.02705981, -0.02816738,\n",
       "          0.02115688, -0.03088739]],\n",
       "\n",
       "       [[ 0.04986143, -0.04900012, -0.01908988,  0.02565249,\n",
       "          0.02327783, -0.02456356, -0.02516657,  0.04759378,\n",
       "          0.04027304, -0.0461815 ],\n",
       "        [ 0.0167869 , -0.0241568 , -0.01391283, -0.03121316,\n",
       "          0.02011046,  0.00024397, -0.02705981, -0.02816738,\n",
       "          0.02115688, -0.03088739]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [ 0.017586  ,  0.03954829, -0.0106926 , -0.04188658,\n",
       "          0.04094392, -0.03261347, -0.01800026,  0.02243816,\n",
       "         -0.0490546 ,  0.02102741]],\n",
       "\n",
       "       [[-0.0417516 ,  0.00715182, -0.00751869,  0.00929739,\n",
       "         -0.01326126, -0.03954202, -0.02942857, -0.02379285,\n",
       "          0.04480174, -0.03904296],\n",
       "        [-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746]],\n",
       "\n",
       "       [[-0.0417516 ,  0.00715182, -0.00751869,  0.00929739,\n",
       "         -0.01326126, -0.03954202, -0.02942857, -0.02379285,\n",
       "          0.04480174, -0.03904296],\n",
       "        [ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [ 0.04232648,  0.01709416, -0.00094085, -0.01240324,\n",
       "          0.00967775,  0.00040231,  0.0060554 ,  0.02592048,\n",
       "          0.00393624, -0.02889873]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [ 0.04158828,  0.02619595,  0.03656632,  0.03072089,\n",
       "          0.02597008, -0.01787263, -0.03740325, -0.00571898,\n",
       "          0.02652187,  0.03318973]],\n",
       "\n",
       "       [[ 0.0167869 , -0.0241568 , -0.01391283, -0.03121316,\n",
       "          0.02011046,  0.00024397, -0.02705981, -0.02816738,\n",
       "          0.02115688, -0.03088739],\n",
       "        [ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [ 0.04986143, -0.04900012, -0.01908988,  0.02565249,\n",
       "          0.02327783, -0.02456356, -0.02516657,  0.04759378,\n",
       "          0.04027304, -0.0461815 ]],\n",
       "\n",
       "       [[ 0.04158828,  0.02619595,  0.03656632,  0.03072089,\n",
       "          0.02597008, -0.01787263, -0.03740325, -0.00571898,\n",
       "          0.02652187,  0.03318973],\n",
       "        [ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503]],\n",
       "\n",
       "       [[ 0.04158828,  0.02619595,  0.03656632,  0.03072089,\n",
       "          0.02597008, -0.01787263, -0.03740325, -0.00571898,\n",
       "          0.02652187,  0.03318973],\n",
       "        [ 0.0167869 , -0.0241568 , -0.01391283, -0.03121316,\n",
       "          0.02011046,  0.00024397, -0.02705981, -0.02816738,\n",
       "          0.02115688, -0.03088739]],\n",
       "\n",
       "       [[ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503],\n",
       "        [ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [-0.01372281, -0.02558281,  0.03040667,  0.01933027,\n",
       "          0.00082835, -0.03225042,  0.02454338, -0.01896745,\n",
       "          0.01792786,  0.01688181]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746]],\n",
       "\n",
       "       [[ 0.04232648,  0.01709416, -0.00094085, -0.01240324,\n",
       "          0.00967775,  0.00040231,  0.0060554 ,  0.02592048,\n",
       "          0.00393624, -0.02889873],\n",
       "        [ 0.04986143, -0.04900012, -0.01908988,  0.02565249,\n",
       "          0.02327783, -0.02456356, -0.02516657,  0.04759378,\n",
       "          0.04027304, -0.0461815 ]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901]],\n",
       "\n",
       "       [[ 0.0167869 , -0.0241568 , -0.01391283, -0.03121316,\n",
       "          0.02011046,  0.00024397, -0.02705981, -0.02816738,\n",
       "          0.02115688, -0.03088739],\n",
       "        [ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503]],\n",
       "\n",
       "       [[ 0.04158828,  0.02619595,  0.03656632,  0.03072089,\n",
       "          0.02597008, -0.01787263, -0.03740325, -0.00571898,\n",
       "          0.02652187,  0.03318973],\n",
       "        [-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [ 0.04986143, -0.04900012, -0.01908988,  0.02565249,\n",
       "          0.02327783, -0.02456356, -0.02516657,  0.04759378,\n",
       "          0.04027304, -0.0461815 ]],\n",
       "\n",
       "       [[ 0.017586  ,  0.03954829, -0.0106926 , -0.04188658,\n",
       "          0.04094392, -0.03261347, -0.01800026,  0.02243816,\n",
       "         -0.0490546 ,  0.02102741],\n",
       "        [ 0.04232648,  0.01709416, -0.00094085, -0.01240324,\n",
       "          0.00967775,  0.00040231,  0.0060554 ,  0.02592048,\n",
       "          0.00393624, -0.02889873]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [ 0.017586  ,  0.03954829, -0.0106926 , -0.04188658,\n",
       "          0.04094392, -0.03261347, -0.01800026,  0.02243816,\n",
       "         -0.0490546 ,  0.02102741]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [-0.0417516 ,  0.00715182, -0.00751869,  0.00929739,\n",
       "         -0.01326126, -0.03954202, -0.02942857, -0.02379285,\n",
       "          0.04480174, -0.03904296]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503]],\n",
       "\n",
       "       [[ 0.017586  ,  0.03954829, -0.0106926 , -0.04188658,\n",
       "          0.04094392, -0.03261347, -0.01800026,  0.02243816,\n",
       "         -0.0490546 ,  0.02102741],\n",
       "        [-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746]],\n",
       "\n",
       "       [[ 0.01573142, -0.02681882, -0.02125452, -0.01403191,\n",
       "          0.03197802,  0.02645608, -0.04975129, -0.03916116,\n",
       "         -0.02402239, -0.03080901],\n",
       "        [ 0.017586  ,  0.03954829, -0.0106926 , -0.04188658,\n",
       "          0.04094392, -0.03261347, -0.01800026,  0.02243816,\n",
       "         -0.0490546 ,  0.02102741]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [-0.01372281, -0.02558281,  0.03040667,  0.01933027,\n",
       "          0.00082835, -0.03225042,  0.02454338, -0.01896745,\n",
       "          0.01792786,  0.01688181]],\n",
       "\n",
       "       [[-0.01372281, -0.02558281,  0.03040667,  0.01933027,\n",
       "          0.00082835, -0.03225042,  0.02454338, -0.01896745,\n",
       "          0.01792786,  0.01688181],\n",
       "        [-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746]],\n",
       "\n",
       "       [[ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503],\n",
       "        [ 0.0430559 , -0.0014899 , -0.04370895,  0.03357038,\n",
       "          0.01690192, -0.02604136, -0.04773562,  0.00408036,\n",
       "          0.01696665,  0.03484503]],\n",
       "\n",
       "       [[-0.0417516 ,  0.00715182, -0.00751869,  0.00929739,\n",
       "         -0.01326126, -0.03954202, -0.02942857, -0.02379285,\n",
       "          0.04480174, -0.03904296],\n",
       "        [ 0.04986143, -0.04900012, -0.01908988,  0.02565249,\n",
       "          0.02327783, -0.02456356, -0.02516657,  0.04759378,\n",
       "          0.04027304, -0.0461815 ]],\n",
       "\n",
       "       [[-0.03456346,  0.04558481, -0.0054585 ,  0.00899393,\n",
       "          0.00796293, -0.00145588, -0.00638177,  0.02523697,\n",
       "         -0.03301877,  0.02456746],\n",
       "        [ 0.04232648,  0.01709416, -0.00094085, -0.01240324,\n",
       "          0.00967775,  0.00040231,  0.0060554 ,  0.02592048,\n",
       "          0.00393624, -0.02889873]]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_x =embed(test_x)\n",
    "embed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_hidden = tf.zeros((batch_size, units))\n",
    "init_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " output_x, state_x=gru(embed_x,init_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2, 32), dtype=float32, numpy=\n",
       "array([[[-4.85169748e-03, -2.57038767e-03, -8.96145124e-03, ...,\n",
       "         -1.83440570e-03, -2.90107727e-03, -6.40665705e-04],\n",
       "        [-1.96462311e-03,  2.77515827e-03, -9.67804808e-03, ...,\n",
       "         -1.22395717e-03,  7.60538969e-05,  1.93269085e-03]],\n",
       "\n",
       "       [[-5.28128818e-03, -2.38672365e-04, -8.03723745e-03, ...,\n",
       "          4.58990689e-03,  6.36845455e-03, -1.08799394e-02],\n",
       "        [-3.79649969e-03,  4.64652292e-03, -1.02896094e-02, ...,\n",
       "          3.69140739e-03,  3.50512727e-03, -2.44601769e-03]],\n",
       "\n",
       "       [[ 2.80382577e-04, -2.53000576e-03, -1.89401791e-03, ...,\n",
       "          9.92462505e-04,  9.77116055e-04,  1.20198831e-03],\n",
       "        [-2.03997735e-03, -3.74573423e-03,  1.02432910e-02, ...,\n",
       "          2.79853889e-03, -1.49048585e-02,  7.12145120e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-5.52272191e-03,  3.88177927e-03,  3.29490402e-03, ...,\n",
       "          1.22958859e-02, -1.11580586e-04, -5.73109370e-03],\n",
       "        [-1.01384930e-02,  6.25167461e-03,  5.41554391e-03, ...,\n",
       "          1.82697158e-02,  7.00670003e-04, -7.30745122e-03]],\n",
       "\n",
       "       [[ 1.26389321e-04,  9.34247021e-03, -8.62040184e-03, ...,\n",
       "          1.56628178e-03, -6.79347373e-04,  7.56325200e-03],\n",
       "        [-5.74722933e-03,  2.66401516e-03, -1.16081061e-02, ...,\n",
       "          5.70303109e-03,  6.40141498e-03, -6.75728917e-03]],\n",
       "\n",
       "       [[ 1.58182567e-03, -4.53691464e-03,  1.19669465e-02, ...,\n",
       "          4.39404231e-03, -4.80876490e-03,  2.14146264e-03],\n",
       "        [-9.25454020e-04,  1.30088464e-03,  5.47531666e-03, ...,\n",
       "          4.80770250e-04, -5.53693250e-03, -5.20914420e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
       "array([[-1.9646231e-03,  2.7751583e-03, -9.6780481e-03, ...,\n",
       "        -1.2239572e-03,  7.6053897e-05,  1.9326909e-03],\n",
       "       [-3.7964997e-03,  4.6465229e-03, -1.0289609e-02, ...,\n",
       "         3.6914074e-03,  3.5051273e-03, -2.4460177e-03],\n",
       "       [-2.0399773e-03, -3.7457342e-03,  1.0243291e-02, ...,\n",
       "         2.7985389e-03, -1.4904859e-02,  7.1214512e-04],\n",
       "       ...,\n",
       "       [-1.0138493e-02,  6.2516746e-03,  5.4155439e-03, ...,\n",
       "         1.8269716e-02,  7.0067000e-04, -7.3074512e-03],\n",
       "       [-5.7472293e-03,  2.6640152e-03, -1.1608106e-02, ...,\n",
       "         5.7030311e-03,  6.4014150e-03, -6.7572892e-03],\n",
       "       [-9.2545402e-04,  1.3008846e-03,  5.4753167e-03, ...,\n",
       "         4.8077025e-04, -5.5369325e-03, -5.2091442e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.expand_dims()函数用于给函数增加维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.expand_dims()函数用于给函数增加维度。\n",
    "tf.expand_dims(\n",
    "    input,\n",
    "    axis=None,\n",
    "    name=None,\n",
    "    dim=None\n",
    ")\n",
    "```\n",
    "参数：\n",
    "```\n",
    "input是输入张量。\n",
    "axis是指定扩大输入张量形状的维度索引值。\n",
    "dim等同于轴，一般不推荐使用。\n",
    "```\n",
    "函数的功能是在给定一个input时，在axis轴处给input增加一个维度。\n",
    "\n",
    "- axis：\n",
    "\n",
    "给定张量输入input，此操作为选择维度索引值，在输入形状的维度索引值的轴处插入1的维度。 维度索引值的轴从零开始; 如果您指定轴是负数，则从最后向后进行计数，也就是倒数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6_tf2.6",
   "language": "python",
   "name": "py3.6_tf2.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
