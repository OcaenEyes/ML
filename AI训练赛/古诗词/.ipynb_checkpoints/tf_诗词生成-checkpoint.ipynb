{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a72595-235b-432b-927a-20af2f85e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "414df86f-f2b9-4945-bbbb-2463b3aff447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    \"\"\"\n",
    "        分词器\n",
    "    \"\"\"\n",
    "    def __init__(self,token_dict):\n",
    "        # 词 -> ID \n",
    "        self.token_dict = token_dict\n",
    "        # ID -> 词\n",
    "        self.token_dict_rev = {value:key for key,value in self.token_dict.items()}\n",
    "        # 词汇表大小\n",
    "        self.vocab_size = len(self.token_dict)\n",
    "        \n",
    "        \n",
    "    def id_to_token(self,token_id):\n",
    "        \"\"\"\n",
    "        给定一个编号，查找词汇表中对应的词\n",
    "        :param token_id: 带查找词的编号\n",
    "        :return: 编号对应的词\n",
    "        \"\"\"\n",
    "        return self.token_dict_rev[token_id]\n",
    "    \n",
    "    def token_to_id(self,token):\n",
    "        \"\"\"\n",
    "        给定一个词，查找它在词汇表中的编号\n",
    "        未找到则返回低频词[UNK]的编号\n",
    "        :param token: 带查找编号的词\n",
    "        :return: 词的编号\n",
    "        \"\"\"\n",
    "        return self.token_dict.get(token,self.token_dict['[UNK]'])\n",
    "        \n",
    "    def encode(self,tokens):\n",
    "        \"\"\"\n",
    "        给定一个字符串s，在头尾分别加上标记开始和结束的特殊字符，并将它转成对应的编号序列\n",
    "        :param tokens: 待编码字符串\n",
    "        :return: 编号序列\n",
    "        \"\"\"\n",
    "        # 加上开始标记\n",
    "        token_ids = [self.token_to_id('[CLS]'), ]\n",
    "        # 加入字符串编号序列\n",
    "        for token in tokens:\n",
    "            token_ids.append(self.token_to_id(token))\n",
    "            \n",
    "        # 加上结束标记\n",
    "        token_ids.append(self.token_to_id('[SEP]'))\n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self,token_ids):\n",
    "        \"\"\"\n",
    "        给定一个编号序列，将它解码成字符串\n",
    "        :param token_ids: 待解码的编号序列\n",
    "        :return: 解码出的字符串\n",
    "        \"\"\"\n",
    "        # 起止标记字符特殊处理\n",
    "        spec_tokens = ['[CLS]','[SEP]']\n",
    "        \n",
    "        # 保存解码出的字符list\n",
    "        tokens = []\n",
    "        for token_id in token_ids:\n",
    "            token = self.id_to_token(token_id)\n",
    "            if token in spec_tokens:\n",
    "                continue\n",
    "                \n",
    "            tokens.append(token)\n",
    "            \n",
    "        #拼接字符串\n",
    "        return ''.join(tokens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1641c62d-d652-43af-a0f3-1b4266706a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 禁用词\n",
    "DISALLOWED_WORDS= ['（', '）', '(', ')', '__', '《', '》', '【', '】', '[', ']']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c41e66f-1c51-4184-899c-f95ee64be328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集路径\n",
    "DATASET_PATH = './data/poetry.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "557412d7-e83b-41b4-bcb5-8e20788e6952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个epoch训练完成后，随机生成SHOW_NUM首古诗作为展示\n",
    "SHOW_NUM = 5\n",
    "# 最佳模型保存路径\n",
    "BEST_MODEL_PATH = './out/best_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "632dcb06-c09c-4e0f-aa8a-9b74e9179d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句子最大长度\n",
    "MAX_LEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56fa68eb-ddf4-49df-9467-e4bb54e583d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最小词频\n",
    "MIN_WORD_FREQUENCY= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8816cf78-3ea8-4e65-a776-4b27459e8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练的batch_size\n",
    "BATCH_SIZE= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72d1a1bf-cb99-467d-b759-9ebab8cadd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共训练多少个epoch\n",
    "TRAIN_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "413aaccf-3397-4052-8ff6-590cb33f7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "disallowed_words = DISALLOWED_WORDS\n",
    "max_len = MAX_LEN\n",
    "min_word_frequency = MIN_WORD_FREQUENCY\n",
    "batch_size = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f8c6bdd-3047-4a75-9583-4d5b9dbb418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "with open(DATASET_PATH,'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [line.replace(\"：\",\":\") for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f0edc35-d49a-498f-8d68-1596a40a45ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'首春:寒随穷律变，春逐鸟声开。初风飘带柳，晚雪间花梅。碧林青旧竹，绿沼翠新苔。芝田初雁去，绮树巧莺来。\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93e313d1-4b58-46b5-85ab-090fbc878c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集列表\n",
    "poetry= []\n",
    "# 逐行处理读取到的数据\n",
    "for line in lines:\n",
    "    if line.count(\":\")!=1:\n",
    "        continue\n",
    "    # 分割后半部分\n",
    "    _,last_part = line.split(\":\")\n",
    "    ignore_flag = False\n",
    "    for dis_word in disallowed_words:\n",
    "        if dis_word in last_part:\n",
    "            ignore_flag =True\n",
    "            break\n",
    "    if ignore_flag:\n",
    "        continue\n",
    "    # 长度不能超过最大长度\n",
    "    if len(last_part) > max_len -2:\n",
    "        continue\n",
    "    poetry.append(last_part.replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6366d3e-c163-40ec-bee1-349103c62df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计词频\n",
    "counter = Counter()\n",
    "for line in poetry:\n",
    "    counter.update(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56ce44be-ee01-460d-9e70-66d762420e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤掉低频的词\n",
    "_tokens = [(token,count) for token,count in counter.items() if count >= min_word_frequency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cb7b95c-9bc0-473b-b570-1f32e3312efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按词频排序，只保留词列表\n",
    "_tokens = sorted(_tokens, key= lambda x :-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb03515d-103d-4d19-8ec5-8abe0fdf805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉词频 只保留词列表\n",
    "_tokens = [token for token,count in _tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbe90e32-9691-46a6-88bc-e6e73be4144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将特殊词和数据集拼接起来\n",
    "_tokens = ['[PAD]','[UNK]','[CLS]','[SEP]']+_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4335a3b-09ec-4120-a5da-95baf65c8ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 3434)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60865f45-9e35-40e2-aa48-37025bbdd8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x20a999383c8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(_tokens,range(len(_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98419f3c-2b8b-4566-a96b-9a691895c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建字典 token -> id 的关系\n",
    "token_id_dict = dict(zip(_tokens,range(len(_tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2c0f8aa-e6a9-45a6-806e-4963566fcdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用词典重建分词器\n",
    "tokenizer = Tokenizer(token_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f16b21d-535f-45a2-9bd5-00445f30366b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Tokenizer at 0x20a99922e10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10de0c47-2eac-4892-92bd-93e8e6a4c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混洗数据\n",
    "np.random.shuffle(poetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4d08e47-935b-445e-9fbc-ad477feb71df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryDataGenerator():\n",
    "    \"\"\"\n",
    "    训练数据集生成\n",
    "    \"\"\"\n",
    "    def __init__(self,data,random=False):\n",
    "        #数据集\n",
    "        self.data = data\n",
    "        # batch_size\n",
    "        self.batch_size= batch_size\n",
    "        # 每个epoch迭代的步数\n",
    "        self.steps = int(math.floor(len(self.data) / self.batch_size))\n",
    "        # 每个epoch开始的时候是否随机混洗\n",
    "        self.random = random\n",
    "        \n",
    "    def sequence_padding(self,data,length=None,padding=None):\n",
    "        \"\"\"\n",
    "        将给定数据填充到相同长度\n",
    "        :param data: 待填充数据\n",
    "        :param length: 填充后的长度，不传递此参数则使用data中的最大长度\n",
    "        :param padding: 用于填充的数据，不传递此参数则使用[PAD]的对应编号\n",
    "        :return: 填充后的数据\n",
    "        \"\"\"\n",
    "        \n",
    "        # 计算填充长度\n",
    "        if length is None:\n",
    "            length = max(map(len,data))\n",
    "            \n",
    "        # 计算填充数据\n",
    "        if padding is None:\n",
    "            padding = tokenizer.token_to_id('[PAD]')\n",
    "            \n",
    "        # 开始填充\n",
    "        outputs = []\n",
    "        \n",
    "        for line in data:\n",
    "            padding_length = length -len(line)\n",
    "            # 不足就填充\n",
    "            if padding_length >0:\n",
    "                outputs.append(np.concatenate([line,[padding] *padding_length]))\n",
    "            # 超过就截断\n",
    "            else:\n",
    "                outputs.append(line[:length])\n",
    "                \n",
    "        return np.array(outputs)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        total = len(self.data)\n",
    "        \n",
    "        # 是否孙吉混洗\n",
    "        if self.random:\n",
    "            np.random.shuffle(self.data)\n",
    "            \n",
    "        # 迭代一个epoch，每次yield 一个batch\n",
    "        for start in range(0,total,self.batch_size):\n",
    "            end = min(start+self.batch_size,total)\n",
    "            batch_data = []\n",
    "            \n",
    "            #逐一对古诗进行编码\n",
    "            for single_data in self.data[start:end]:\n",
    "                batch_data.append(tokenizer.encode(single_data))\n",
    "                \n",
    "            # 填充为相同长度\n",
    "            batch_data = self.sequence_padding(batch_data)\n",
    "            \n",
    "            # yield x,y\n",
    "            yield batch_data[:,:-1],tf.one_hot(batch_data[:,1:], tokenizer.vocab_size)\n",
    "            \n",
    "            del batch_data\n",
    "            \n",
    "    def for_fit(self):\n",
    "        \"\"\"\n",
    "        创建一个生成器，用于训练\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            yield from self.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d04f3894-d06c-4904-a648-9f949637efd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a =['1','2',3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4853169-9b06-4a62-b004-d79fe8b822a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ecd0aaa-ad66-4e79-9cee-0368bc049f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', 3, 4, 5]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51cc484f-e6be-4faf-a029-8d319823e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poetry(tokenizer,model,head):\n",
    "    \"\"\"\n",
    "    随机生成一首藏头诗\n",
    "    :param tokenizer: 分词器\n",
    "    :param model: 用于生成古诗的模型\n",
    "    :param head: 藏头诗的头\n",
    "    :return: 一个字符串，表示一首古诗\n",
    "    \"\"\"\n",
    "    # 使用空串初始化token_ids ,加入[CLS]\n",
    "    token_ids = tokenizer.encode('')\n",
    "    token_ids = token_ids[:-1]\n",
    "    \n",
    "    # 标点符号\n",
    "    punctuations = ['，','。']\n",
    "    punctuations_ids = [tokenizer.token_to_id(token) for token in punctuations]\n",
    "    \n",
    "    # 村换生成诗的list\n",
    "    poetry =[]\n",
    "    \n",
    "    # 对于藏头诗的每一个字。都生成一个短句\n",
    "    for ch in head:\n",
    "        #先记录这个字\n",
    "        poetry.append(ch)\n",
    "        # 将藏头字转为id \n",
    "        token_id = tokenizer.token_to_id(ch)\n",
    "        # 加入进列表\n",
    "        token_ids.append(token_id)\n",
    "        \n",
    "        # 生成短句\n",
    "        while True:\n",
    "            #进行预测,只保留第一个样例（我们输入的样例数只有1）的、最后一个token的预测的、不包含[PAD][UNK][CLS]的概率分布\n",
    "            output = model(np.array([token_ids,], dtype=np.int32))\n",
    "            _probas = output.numpy()[0,-1,3:]\n",
    "            del output\n",
    "            \n",
    "            #按照出现概率，对所有的token倒序排列\n",
    "            p_args = _probas.argsort()[::-1][:100]\n",
    "            #排列后的概率顺序\n",
    "            p = _probas[p_args]\n",
    "            \n",
    "            #对概率归一\n",
    "            p = p /sum(p)\n",
    "            \n",
    "            #再按照预测出的概率，随机选择一个词作为预测结果\n",
    "            target_index = np.random.choice(len(p), p=p)\n",
    "            target = p_args[target_index] +3\n",
    "            \n",
    "            #保存\n",
    "            token_ids.append(target)\n",
    "            # 只有不是特殊字符时，才保存到poetry中\n",
    "            if target >3:\n",
    "                poetry.append(tokenizer.id_to_token(target))\n",
    "                \n",
    "            if target in punctuations_ids:\n",
    "                break\n",
    "    return ''.join(poetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b162bd13-a194-4f23-ae75-15ff2e782cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function Input in module keras.engine.input_layer:\n",
      "\n",
      "Input(shape=None, batch_size=None, name=None, dtype=None, sparse=None, tensor=None, ragged=None, type_spec=None, **kwargs)\n",
      "    `Input()` is used to instantiate a Keras tensor.\n",
      "    \n",
      "    A Keras tensor is a symbolic tensor-like object,\n",
      "    which we augment with certain attributes that allow us to build a Keras model\n",
      "    just by knowing the inputs and outputs of the model.\n",
      "    \n",
      "    For instance, if `a`, `b` and `c` are Keras tensors,\n",
      "    it becomes possible to do:\n",
      "    `model = Model(input=[a, b], output=c)`\n",
      "    \n",
      "    Args:\n",
      "        shape: A shape tuple (integers), not including the batch size.\n",
      "            For instance, `shape=(32,)` indicates that the expected input\n",
      "            will be batches of 32-dimensional vectors. Elements of this tuple\n",
      "            can be None; 'None' elements represent dimensions where the shape is\n",
      "            not known.\n",
      "        batch_size: optional static batch size (integer).\n",
      "        name: An optional name string for the layer.\n",
      "            Should be unique in a model (do not reuse the same name twice).\n",
      "            It will be autogenerated if it isn't provided.\n",
      "        dtype: The data type expected by the input, as a string\n",
      "            (`float32`, `float64`, `int32`...)\n",
      "        sparse: A boolean specifying whether the placeholder to be created is\n",
      "            sparse. Only one of 'ragged' and 'sparse' can be True. Note that,\n",
      "            if `sparse` is False, sparse tensors can still be passed into the\n",
      "            input - they will be densified with a default value of 0.\n",
      "        tensor: Optional existing tensor to wrap into the `Input` layer.\n",
      "            If set, the layer will use the `tf.TypeSpec` of this tensor rather\n",
      "            than creating a new placeholder tensor.\n",
      "        ragged: A boolean specifying whether the placeholder to be created is\n",
      "            ragged. Only one of 'ragged' and 'sparse' can be True. In this case,\n",
      "            values of 'None' in the 'shape' argument represent ragged dimensions.\n",
      "            For more information about RaggedTensors, see\n",
      "            [this guide](https://www.tensorflow.org/guide/ragged_tensors).\n",
      "        type_spec: A `tf.TypeSpec` object to create the input placeholder from.\n",
      "            When provided, all other args except name must be None.\n",
      "        **kwargs: deprecated arguments support. Supports `batch_shape` and\n",
      "            `batch_input_shape`.\n",
      "    \n",
      "    Returns:\n",
      "      A `tensor`.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    ```python\n",
      "    # this is a logistic regression in Keras\n",
      "    x = Input(shape=(32,))\n",
      "    y = Dense(16, activation='softmax')(x)\n",
      "    model = Model(x, y)\n",
      "    ```\n",
      "    \n",
      "    Note that even if eager execution is enabled,\n",
      "    `Input` produces a symbolic tensor-like object (i.e. a placeholder).\n",
      "    This symbolic tensor-like object can be used with lower-level\n",
      "    TensorFlow ops that take tensors as inputs, as such:\n",
      "    \n",
      "    ```python\n",
      "    x = Input(shape=(32,))\n",
      "    y = tf.square(x)  # This op will be treated like a layer\n",
      "    model = Model(x, y)\n",
      "    ```\n",
      "    \n",
      "    (This behavior does not work for higher-order TensorFlow APIs such as\n",
      "    control flow and being directly watched by a `tf.GradientTape`).\n",
      "    \n",
      "    However, the resulting model will not track any variables that were\n",
      "    used as inputs to TensorFlow ops. All variable usages must happen within\n",
      "    Keras layers to make sure they will be tracked by the model's weights.\n",
      "    \n",
      "    The Keras Input can also create a placeholder from an arbitrary `tf.TypeSpec`,\n",
      "    e.g:\n",
      "    \n",
      "    ```python\n",
      "    x = Input(type_spec=tf.RaggedTensorSpec(shape=[None, None],\n",
      "                                            dtype=tf.float32, ragged_rank=1))\n",
      "    y = x.values\n",
      "    model = Model(x, y)\n",
      "    ```\n",
      "    When passing an arbitrary `tf.TypeSpec`, it must represent the signature of an\n",
      "    entire batch instead of just one example.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If both `sparse` and `ragged` are provided.\n",
      "      ValueError: If both `shape` and (`batch_input_shape` or `batch_shape`) are\n",
      "        provided.\n",
      "      ValueError: If `shape`, `tensor` and `type_spec` are None.\n",
      "      ValueError: If arguments besides `type_spec` are non-None while `type_spec`\n",
      "                  is passed.\n",
      "      ValueError: if any unrecognized parameters are provided.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.layers.Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8af886d-e1dc-483f-a362-f120d4186abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "构建lstm模型\n",
    "\"\"\"\n",
    "# model = tf.keras.Sequential()\n",
    "# #不定长度的输入\n",
    "# model.add(tf.keras.layers.Input(None,))\n",
    "# # 词嵌入层\n",
    "# model.add(tf.keras.layers.Embedding(input_dim=tokenizer.vocab_size, output_dim=128))\n",
    "# # 第一个lstm层\n",
    "# model.add(tf.keras.layers.LSTM(128,dropout=0.5,return_sequence=True))\n",
    "# model.add(tf.keras.layers.LSTM(128,dropout=0.5,return_sequence=True))\n",
    "\n",
    "# # 对每一个时间点输出都做softmax, 预测下一个词的概率\n",
    "# model.add(tf.keras.layers.TimeDistributed(\n",
    "#     tf.keras.layers.Dense(tokenizer.vocab_size,activation='softmax')\n",
    "# ))\n",
    "model = tf.keras.Sequential([\n",
    "    # 不定长度的输入\n",
    "    tf.keras.layers.Input((None,)),\n",
    "    # 词嵌入层\n",
    "    tf.keras.layers.Embedding(input_dim=tokenizer.vocab_size, output_dim=128),\n",
    "    # 第一个LSTM层，返回序列作为下一层的输入\n",
    "    tf.keras.layers.LSTM(128, dropout=0.5, return_sequences=True),\n",
    "    # 第二个LSTM层，返回序列作为下一层的输入\n",
    "    tf.keras.layers.LSTM(128, dropout=0.5, return_sequences=True),\n",
    "    # 对每一个时间点的输出都做softmax，预测下一个词的概率\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tokenizer.vocab_size, activation='softmax')),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "604d8dde-3aee-44bb-a4e9-d8390819575d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         439552    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 3434)        442986    \n",
      "=================================================================\n",
      "Total params: 1,145,706\n",
      "Trainable params: 1,145,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a30287f-6fd0-4df8-b866-adba619b5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置优化器和损失函数\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75d37b45-9dfb-4473-acc4-012fc8ee9e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "模型训练\n",
    "\"\"\"\n",
    "class Evaluate(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    训练过程评估，在每个epoch训练完成后，保留最优权重，并随机生成SHOW_NUM首古诗展示\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 给loss赋一个较大的初始值\n",
    "        self.lowest = 1e10\n",
    "        \n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        #在每个epochi训练完成后调用\n",
    "        #如果当前loss更低，则保存当前模型参数\n",
    "        if logs['loss'] <= self.lowest:\n",
    "            self.lowest = logs['loss']\n",
    "            model.save(BEST_MODEL_PATH)\n",
    "            \n",
    "        #随机生成古诗测试，查看效果\n",
    "        print(\"生成测试\")\n",
    "        \n",
    "        for i in range(SHOW_NUM):\n",
    "            print(generate_poetry(tokenizer,model,head=\"春花秋月\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5444ff99-caf3-461b-83fb-c5270dfbbca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集\n",
    "data_generator = PoetryDataGenerator(poetry,random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "797cf65c-dc09-4c2d-8c09-c69be1caa919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1534/1534 [==============================] - 30s 20ms/step - loss: 4.1433\n",
      "生成测试\n",
      "春柳无何到，花年万里流。秋南山阳在，月日柳峰间。\n",
      "春阳楚天去，花里暮山边。秋水夜窗鸟，月天拂野连。\n",
      "春峰春上月来，花柳几日半花来。秋流天夜吹条树，月雨新山似处头。\n",
      "春夜随云远一秦，花城吹处问人看。秋朝唯可见头者，月梦花僧五首朝。\n",
      "春山春风，花前长故尘。秋中未相酒，月夜两湘云。\n",
      "Epoch 2/10\n",
      "1534/1534 [==============================] - 30s 20ms/step - loss: 3.9983\n",
      "生成测试\n",
      "春色三城树，花秋望暮村。秋潮无人事，月叶得何心。\n",
      "春风万里过，花鸟上时惊。秋水深千里，月啼白岸声。\n",
      "春月花亭好，花州夕岁心。秋风不思处，月向梦中时。\n",
      "春草有时路，花江似远乡。秋边闻鸟夕，月水水流头。\n",
      "春上多春野夜人，花西犹到客家知。秋风不见高桥去，月日清天吹更来。\n",
      "Epoch 3/10\n",
      "1534/1534 [==============================] - 30s 20ms/step - loss: 3.9176\n",
      "生成测试\n",
      "春下云沙一客红，花生不到月西烟。秋宵吹曲人寻语，月日人逢落下回。\n",
      "春风出酒远，花发暮飞明。秋影归初晓，月空见汉人。\n",
      "春山临水寺，花落落阳舟。秋霁入秋望，月无空满空。\n",
      "春暖暮梅花，花山绕井连。秋山长雁远，月树隔秋云。\n",
      "春边尽别醉，花岸照中春。秋水秋风水，月光花绿微。\n",
      "Epoch 4/10\n",
      "1534/1534 [==============================] - 30s 20ms/step - loss: 3.8707\n",
      "生成测试\n",
      "春日萧楼天，花晴几渐赊。秋风入竹叶，月静逐年台。\n",
      "春流有竹树，花中出客津。秋风何故景，月有见孤关。\n",
      "春光千阙水，花雨树天关。秋郭山河水，月深照寺深。\n",
      "春光不知时，花上雪溪边。秋吹九云水，月花深夜城。\n",
      "春风正有路，花处复经年。秋气临风树，月开窗早鸣。\n",
      "Epoch 5/10\n",
      "1534/1534 [==============================] - 31s 20ms/step - loss: 3.8251\n",
      "生成测试\n",
      "春欲相无事，花前复不游。秋风起不似，月雨半还思。\n",
      "春城起道时，花阁海城东。秋雨犹寒木，月霜秋月过。\n",
      "春华风上尽，花下独长凉。秋发通云树，月村花上清。\n",
      "春去有来梦，花根已白华。秋山无所见，月雁即吟空。\n",
      "春北深山雪，花深白更飞。秋声不可早，月上万云行。\n",
      "Epoch 6/10\n",
      "1534/1534 [==============================] - 30s 20ms/step - loss: 3.7894\n",
      "生成测试\n",
      "春谷不相家，花香在夕边。秋吹松雨寺，月日暗灯疏。\n",
      "春风萧条月，花白石阴尘。秋去归风雾，月峰清不成。\n",
      "春来不可别，花景在秋微。秋月高山下，月前窗下邻。\n",
      "春日明夜去，花深水色边。秋波空鸟叶，月动叶前秋。\n",
      "春来见人去，花落远荒山。秋色吹岩后，月阳云雨枝。\n",
      "Epoch 7/10\n",
      "1534/1534 [==============================] - 30s 20ms/step - loss: 3.7575 0s - loss: 3.7\n",
      "生成测试\n",
      "春色满残城，花枝映影清。秋风清不落，月落似离州。\n",
      "春风无异日，花雨入人游。秋渡云边月，月高杨叶声。\n",
      "春草秋残寺，花居自不思。秋风有云顶，月夜入湖山。\n",
      "春风满晚分，花雨暗初鸣。秋草风深水，月中空老人。\n",
      "春色春池暮，花城在浦门。秋光照黄叶，月阁水纷纷。\n",
      "Epoch 8/10\n",
      "1534/1534 [==============================] - 30s 20ms/step - loss: 3.7359\n",
      "生成测试\n",
      "春月照寒开，花低独过归。秋风吹绿月，月下向溪城。\n",
      "春霜出砌上，花洒未成心。秋上一苍竹，月重春又流。\n",
      "春夜青城寺，花生半渐鸣。秋风过白古，月水月头明。\n",
      "春风忽相续，花静竹微催。秋月入鱼火，月过云上衣。\n",
      "春霁有风东，花中夕候孤。秋风吹药水，月日带松蝉。\n",
      "Epoch 9/10\n",
      "1534/1534 [==============================] - 30s 20ms/step - loss: 3.7085\n",
      "生成测试\n",
      "春日相思不厌时，花花重雨滴垂苔。秋风水里无人事，月暗芦云带泪啼。\n",
      "春风吹杏落，花翠动成襟。秋日秋风色，月边心有吟。\n",
      "春风偏接一扉明，花满烟苔隔寺舟。秋雪南山多何事，月凉一枝春浪声。\n",
      "春泉落日满天风，花色微山满翠微。秋风不自知无死，月里江山泪钓堂。\n",
      "春风满夜西三城，花叶当凉草锦烟。秋月江边月影满，月边秋水一闻猿。\n",
      "Epoch 10/10\n",
      "1534/1534 [==============================] - 30s 20ms/step - loss: 3.6808\n",
      "生成测试\n",
      "春落花空不觉心，花连自怪望人归。秋风动梦伤来去，月落潮明更旧明。\n",
      "春尽初纷咽，花繁暗不同。秋山远半路，月雪雁来声。\n",
      "春至天辰守，花光是物尘。秋看看夜满，月涨共惊鸦。\n",
      "春风吹满雨庭斜，花被红金白古枝。秋风近月云中日，月上宫舟望月头。\n",
      "春柳生沉起，花流白发期。秋江三尺树，月色满清波。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c3eb38e80>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    data_generator.for_fit(),\n",
    "    steps_per_epoch=data_generator.steps,\n",
    "    workers=-1,\n",
    "    epochs=10,\n",
    "    callbacks=[Evaluate()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "298f645c-3a66-4865-bc5a-5efcc9e4a4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "输入关键字:\n",
      " 高智勇\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高斋有春物，智气喜无情。勇在风前水， \n",
      "\n",
      "高僧何日无人绪，智土从君合了存。勇得还同能有苦， \n",
      "\n",
      "高阶独不极，智树不成开。勇得新兵薄， \n",
      "\n",
      "高步飞晴见雪声，智来不见又空悲。勇书自说堪闲泪， \n",
      "\n",
      "高路江林远，智公已故川。勇章怜旧事， \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "输入关键字，生成藏头诗\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# 加载训练好的模型\n",
    "model = tf.keras.models.load_model(BEST_MODEL_PATH)\n",
    "\n",
    "keywords = input('输入关键字:\\n')\n",
    "\n",
    "\n",
    "# 生成藏头诗\n",
    "for i in range(SHOW_NUM):\n",
    "    print(generate_poetry(tokenizer, model, head=keywords),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935ad59-2528-4ebd-b22a-0510d45b9621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6.5_tf2",
   "language": "python",
   "name": "py3.6.5_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
